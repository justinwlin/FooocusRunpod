[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "fcbh.ops",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.ops",
        "description": "fcbh.ops",
        "detail": "fcbh.ops",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "ceil",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "integrate",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "torchsde",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchsde",
        "description": "torchsde",
        "detail": "torchsde",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageDraw",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageFont",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageFilter",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch.utils",
        "description": "torch.utils",
        "isExtraImport": true,
        "detail": "torch.utils",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "DiagonalGaussianDistribution",
        "importPath": "fcbh.ldm.modules.distributions.distributions",
        "description": "fcbh.ldm.modules.distributions.distributions",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.distributions.distributions",
        "documentation": {}
    },
    {
        "label": "instantiate_from_config",
        "importPath": "fcbh.ldm.util",
        "description": "fcbh.ldm.util",
        "isExtraImport": true,
        "detail": "fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "exists",
        "importPath": "fcbh.ldm.util",
        "description": "fcbh.ldm.util",
        "isExtraImport": true,
        "detail": "fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "default",
        "importPath": "fcbh.ldm.util",
        "description": "fcbh.ldm.util",
        "isExtraImport": true,
        "detail": "fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "instantiate_from_config",
        "importPath": "fcbh.ldm.util",
        "description": "fcbh.ldm.util",
        "isExtraImport": true,
        "detail": "fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "LitEma",
        "importPath": "fcbh.ldm.modules.ema",
        "description": "fcbh.ldm.modules.ema",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.ema",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "einops",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "einops",
        "description": "einops",
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "model_management",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "model_management",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "model_management",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "model_management",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "model_base",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "model_management",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "sd1_clip",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "sd1_clip",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "model_management",
        "importPath": "fcbh",
        "description": "fcbh",
        "isExtraImport": true,
        "detail": "fcbh",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "isfunction",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "isfunction",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "isfunction",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "fcbh.cli_args",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "args",
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "isExtraImport": true,
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "args",
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "isExtraImport": true,
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "args",
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "isExtraImport": true,
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "args",
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "isExtraImport": true,
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "args",
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "isExtraImport": true,
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "LatentPreviewMethod",
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "isExtraImport": true,
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "args",
        "importPath": "fcbh.cli_args",
        "description": "fcbh.cli_args",
        "isExtraImport": true,
        "detail": "fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "torch.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "fcbh.utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.utils",
        "description": "fcbh.utils",
        "detail": "fcbh.utils",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "enum",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "enum",
        "description": "enum",
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "fcbh.options",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.options",
        "description": "fcbh.options",
        "detail": "fcbh.options",
        "documentation": {}
    },
    {
        "label": "enable_args_parsing",
        "importPath": "fcbh.options",
        "description": "fcbh.options",
        "isExtraImport": true,
        "detail": "fcbh.options",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModelWithProjection",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "modeling_utils",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPTextModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPTextConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "modeling_utils",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "fcbh.model_patcher",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.model_patcher",
        "description": "fcbh.model_patcher",
        "detail": "fcbh.model_patcher",
        "documentation": {}
    },
    {
        "label": "ModelPatcher",
        "importPath": "fcbh.model_patcher",
        "description": "fcbh.model_patcher",
        "isExtraImport": true,
        "detail": "fcbh.model_patcher",
        "documentation": {}
    },
    {
        "label": "ModelPatcher",
        "importPath": "fcbh.model_patcher",
        "description": "fcbh.model_patcher",
        "isExtraImport": true,
        "detail": "fcbh.model_patcher",
        "documentation": {}
    },
    {
        "label": "ModelPatcher",
        "importPath": "fcbh.model_patcher",
        "description": "fcbh.model_patcher",
        "isExtraImport": true,
        "detail": "fcbh.model_patcher",
        "documentation": {}
    },
    {
        "label": "fcbh.model_management",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.model_management",
        "description": "fcbh.model_management",
        "detail": "fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "fcbh.model_detection",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.model_detection",
        "description": "fcbh.model_detection",
        "detail": "fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "fcbh.cldm.cldm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.cldm.cldm",
        "description": "fcbh.cldm.cldm",
        "detail": "fcbh.cldm.cldm",
        "documentation": {}
    },
    {
        "label": "fcbh.t2i_adapter.adapter",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.t2i_adapter.adapter",
        "description": "fcbh.t2i_adapter.adapter",
        "detail": "fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "fcbh.sd",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.sd",
        "description": "fcbh.sd",
        "detail": "fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_checkpoint_guess_config",
        "importPath": "fcbh.sd",
        "description": "fcbh.sd",
        "isExtraImport": true,
        "detail": "fcbh.sd",
        "documentation": {}
    },
    {
        "label": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "detail": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "UNetModel",
        "importPath": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "Timestep",
        "importPath": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "forward_timestep_embed",
        "importPath": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "apply_control",
        "importPath": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "CLIPEmbeddingNoiseAugmentation",
        "importPath": "fcbh.ldm.modules.encoders.noise_aug_modules",
        "description": "fcbh.ldm.modules.encoders.noise_aug_modules",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.encoders.noise_aug_modules",
        "documentation": {}
    },
    {
        "label": "fcbh.conds",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.conds",
        "description": "fcbh.conds",
        "detail": "fcbh.conds",
        "documentation": {}
    },
    {
        "label": "CONDRegular",
        "importPath": "fcbh.conds",
        "description": "fcbh.conds",
        "isExtraImport": true,
        "detail": "fcbh.conds",
        "documentation": {}
    },
    {
        "label": "fcbh.model_sampling",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.model_sampling",
        "description": "fcbh.model_sampling",
        "detail": "fcbh.model_sampling",
        "documentation": {}
    },
    {
        "label": "EPS",
        "importPath": "fcbh.model_sampling",
        "description": "fcbh.model_sampling",
        "isExtraImport": true,
        "detail": "fcbh.model_sampling",
        "documentation": {}
    },
    {
        "label": "V_PREDICTION",
        "importPath": "fcbh.model_sampling",
        "description": "fcbh.model_sampling",
        "isExtraImport": true,
        "detail": "fcbh.model_sampling",
        "documentation": {}
    },
    {
        "label": "ModelSamplingDiscrete",
        "importPath": "fcbh.model_sampling",
        "description": "fcbh.model_sampling",
        "isExtraImport": true,
        "detail": "fcbh.model_sampling",
        "documentation": {}
    },
    {
        "label": "fcbh.supported_models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.supported_models",
        "description": "fcbh.supported_models",
        "detail": "fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "fcbh.supported_models_base",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.supported_models_base",
        "description": "fcbh.supported_models_base",
        "detail": "fcbh.supported_models_base",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "make_beta_schedule",
        "importPath": "fcbh.ldm.modules.diffusionmodules.util",
        "description": "fcbh.ldm.modules.diffusionmodules.util",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "make_beta_schedule",
        "importPath": "fcbh.ldm.modules.diffusionmodules.util",
        "description": "fcbh.ldm.modules.diffusionmodules.util",
        "isExtraImport": true,
        "detail": "fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "fcbh.samplers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.samplers",
        "description": "fcbh.samplers",
        "detail": "fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "resolve_areas_and_cond_masks",
        "importPath": "fcbh.samplers",
        "description": "fcbh.samplers",
        "isExtraImport": true,
        "detail": "fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "wrap_model",
        "importPath": "fcbh.samplers",
        "description": "fcbh.samplers",
        "isExtraImport": true,
        "detail": "fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "calculate_start_end_timesteps",
        "importPath": "fcbh.samplers",
        "description": "fcbh.samplers",
        "isExtraImport": true,
        "detail": "fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "fcbh.samplers",
        "description": "fcbh.samplers",
        "isExtraImport": true,
        "detail": "fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "fcbh.lora",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.lora",
        "description": "fcbh.lora",
        "detail": "fcbh.lora",
        "documentation": {}
    },
    {
        "label": "model_lora_keys_unet",
        "importPath": "fcbh.lora",
        "description": "fcbh.lora",
        "isExtraImport": true,
        "detail": "fcbh.lora",
        "documentation": {}
    },
    {
        "label": "model_lora_keys_clip",
        "importPath": "fcbh.lora",
        "description": "fcbh.lora",
        "isExtraImport": true,
        "detail": "fcbh.lora",
        "documentation": {}
    },
    {
        "label": "fcbh.taesd.taesd",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.taesd.taesd",
        "description": "fcbh.taesd.taesd",
        "detail": "fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "TAESD",
        "importPath": "fcbh.taesd.taesd",
        "description": "fcbh.taesd.taesd",
        "isExtraImport": true,
        "detail": "fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "zipfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zipfile",
        "description": "zipfile",
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "fcbh.checkpoint_pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.checkpoint_pickle",
        "description": "fcbh.checkpoint_pickle",
        "detail": "fcbh.checkpoint_pickle",
        "documentation": {}
    },
    {
        "label": "safetensors.torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Reduce",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "torch.nn.modules.batchnorm",
        "description": "torch.nn.modules.batchnorm",
        "isExtraImport": true,
        "detail": "torch.nn.modules.batchnorm",
        "documentation": {}
    },
    {
        "label": "collections.abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections.abc",
        "description": "collections.abc",
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "product",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "_calculate_fan_in_and_fan_out",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "InterpolationMode",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "rotate",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nodes",
        "description": "nodes",
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "MAX_RESOLUTION",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "MAX_RESOLUTION",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "VAEDecode",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "EmptyLatentImage",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "VAEEncode",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "VAEEncodeTiled",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "VAEDecodeTiled",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "nodes",
        "description": "nodes",
        "isExtraImport": true,
        "detail": "nodes",
        "documentation": {}
    },
    {
        "label": "fcbh.sample",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.sample",
        "description": "fcbh.sample",
        "detail": "fcbh.sample",
        "documentation": {}
    },
    {
        "label": "prepare_mask",
        "importPath": "fcbh.sample",
        "description": "fcbh.sample",
        "isExtraImport": true,
        "detail": "fcbh.sample",
        "documentation": {}
    },
    {
        "label": "get_additional_models",
        "importPath": "fcbh.sample",
        "description": "fcbh.sample",
        "isExtraImport": true,
        "detail": "fcbh.sample",
        "documentation": {}
    },
    {
        "label": "get_models_from_cond",
        "importPath": "fcbh.sample",
        "description": "fcbh.sample",
        "isExtraImport": true,
        "detail": "fcbh.sample",
        "documentation": {}
    },
    {
        "label": "cleanup_additional_models",
        "importPath": "fcbh.sample",
        "description": "fcbh.sample",
        "isExtraImport": true,
        "detail": "fcbh.sample",
        "documentation": {}
    },
    {
        "label": "sampling",
        "importPath": "fcbh.k_diffusion",
        "description": "fcbh.k_diffusion",
        "isExtraImport": true,
        "detail": "fcbh.k_diffusion",
        "documentation": {}
    },
    {
        "label": "latent_preview",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "latent_preview",
        "description": "latent_preview",
        "detail": "latent_preview",
        "documentation": {}
    },
    {
        "label": "folder_paths",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "folder_paths",
        "description": "folder_paths",
        "detail": "folder_paths",
        "documentation": {}
    },
    {
        "label": "scipy.ndimage",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.ndimage",
        "description": "scipy.ndimage",
        "detail": "scipy.ndimage",
        "documentation": {}
    },
    {
        "label": "fcbh.model_base",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.model_base",
        "description": "fcbh.model_base",
        "detail": "fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "SDXL",
        "importPath": "fcbh.model_base",
        "description": "fcbh.model_base",
        "isExtraImport": true,
        "detail": "fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "SDXLRefiner",
        "importPath": "fcbh.model_base",
        "description": "fcbh.model_base",
        "isExtraImport": true,
        "detail": "fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "SDXLRefiner",
        "importPath": "fcbh.model_base",
        "description": "fcbh.model_base",
        "isExtraImport": true,
        "detail": "fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "SDXL",
        "importPath": "fcbh.model_base",
        "description": "fcbh.model_base",
        "isExtraImport": true,
        "detail": "fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "model_loading",
        "importPath": "fcbh_extras.chainner_models",
        "description": "fcbh_extras.chainner_models",
        "isExtraImport": true,
        "detail": "fcbh_extras.chainner_models",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "PngInfo",
        "importPath": "PIL.PngImagePlugin",
        "description": "PIL.PngImagePlugin",
        "isExtraImport": true,
        "detail": "PIL.PngImagePlugin",
        "documentation": {}
    },
    {
        "label": "fcbh.diffusers_load",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.diffusers_load",
        "description": "fcbh.diffusers_load",
        "detail": "fcbh.diffusers_load",
        "documentation": {}
    },
    {
        "label": "fcbh.controlnet",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.controlnet",
        "description": "fcbh.controlnet",
        "detail": "fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "fcbh.clip_vision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.clip_vision",
        "description": "fcbh.clip_vision",
        "detail": "fcbh.clip_vision",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "inv",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "lstsq",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "matrix_rank",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "norm",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "IntermediateLayerGetter",
        "importPath": "torchvision.models._utils",
        "description": "torchvision.models._utils",
        "isExtraImport": true,
        "detail": "torchvision.models._utils",
        "documentation": {}
    },
    {
        "label": "get_reference_facial_points",
        "importPath": "fooocus_extras.facexlib.detection.align_trans",
        "description": "fooocus_extras.facexlib.detection.align_trans",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "warp_and_crop_face",
        "importPath": "fooocus_extras.facexlib.detection.align_trans",
        "description": "fooocus_extras.facexlib.detection.align_trans",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "FPN",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_net",
        "description": "fooocus_extras.facexlib.detection.retinaface_net",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "SSH",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_net",
        "description": "fooocus_extras.facexlib.detection.retinaface_net",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "MobileNetV1",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_net",
        "description": "fooocus_extras.facexlib.detection.retinaface_net",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "make_bbox_head",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_net",
        "description": "fooocus_extras.facexlib.detection.retinaface_net",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "make_class_head",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_net",
        "description": "fooocus_extras.facexlib.detection.retinaface_net",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "make_landmark_head",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_net",
        "description": "fooocus_extras.facexlib.detection.retinaface_net",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "PriorBox",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "fooocus_extras.facexlib.detection.retinaface_utils",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "batched_decode",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "fooocus_extras.facexlib.detection.retinaface_utils",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "batched_decode_landm",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "fooocus_extras.facexlib.detection.retinaface_utils",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "decode",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "fooocus_extras.facexlib.detection.retinaface_utils",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "decode_landm",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "fooocus_extras.facexlib.detection.retinaface_utils",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "py_cpu_nms",
        "importPath": "fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "fooocus_extras.facexlib.detection.retinaface_utils",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "init_detection_model",
        "importPath": "fooocus_extras.facexlib.detection",
        "description": "fooocus_extras.facexlib.detection",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.detection",
        "documentation": {}
    },
    {
        "label": "init_parsing_model",
        "importPath": "fooocus_extras.facexlib.parsing",
        "description": "fooocus_extras.facexlib.parsing",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.parsing",
        "documentation": {}
    },
    {
        "label": "img2tensor",
        "importPath": "fooocus_extras.facexlib.utils.misc",
        "description": "fooocus_extras.facexlib.utils.misc",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.utils.misc",
        "documentation": {}
    },
    {
        "label": "imwrite",
        "importPath": "fooocus_extras.facexlib.utils.misc",
        "description": "fooocus_extras.facexlib.utils.misc",
        "isExtraImport": true,
        "detail": "fooocus_extras.facexlib.utils.misc",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "exists",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "download_url_to_file",
        "importPath": "torch.hub",
        "description": "torch.hub",
        "isExtraImport": true,
        "detail": "torch.hub",
        "documentation": {}
    },
    {
        "label": "get_dir",
        "importPath": "torch.hub",
        "description": "torch.hub",
        "isExtraImport": true,
        "detail": "torch.hub",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "modules.config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.config",
        "description": "modules.config",
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_vae_approx",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_embeddings",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_fooocus_expansion",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_upscale_models",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_checkpoints",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_loras",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_vae_approx",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_fooocus_expansion",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_checkpoints",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_loras",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_vae_approx",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "path_fooocus_expansion",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "modules.config",
        "description": "modules.config",
        "isExtraImport": true,
        "detail": "modules.config",
        "documentation": {}
    },
    {
        "label": "fcbh.ldm.modules.attention",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.ldm.modules.attention",
        "description": "fcbh.ldm.modules.attention",
        "detail": "fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "Resampler",
        "importPath": "fooocus_extras.resampler",
        "description": "fooocus_extras.resampler",
        "isExtraImport": true,
        "detail": "fooocus_extras.resampler",
        "documentation": {}
    },
    {
        "label": "modules.core",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.core",
        "description": "modules.core",
        "detail": "modules.core",
        "documentation": {}
    },
    {
        "label": "numpy_to_pytorch",
        "importPath": "modules.core",
        "description": "modules.core",
        "isExtraImport": true,
        "detail": "modules.core",
        "documentation": {}
    },
    {
        "label": "modules.advanced_parameters",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.advanced_parameters",
        "description": "modules.advanced_parameters",
        "detail": "modules.advanced_parameters",
        "documentation": {}
    },
    {
        "label": "modules.constants",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.constants",
        "description": "modules.constants",
        "detail": "modules.constants",
        "documentation": {}
    },
    {
        "label": "numbers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numbers",
        "description": "numbers",
        "detail": "numbers",
        "documentation": {}
    },
    {
        "label": "args_manager",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "args_manager",
        "description": "args_manager",
        "detail": "args_manager",
        "documentation": {}
    },
    {
        "label": "modules.flags",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.flags",
        "description": "modules.flags",
        "detail": "modules.flags",
        "documentation": {}
    },
    {
        "label": "modules.sdxl_styles",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.sdxl_styles",
        "description": "modules.sdxl_styles",
        "detail": "modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "legal_style_names",
        "importPath": "modules.sdxl_styles",
        "description": "modules.sdxl_styles",
        "isExtraImport": true,
        "detail": "modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "load_file_from_url",
        "importPath": "modules.model_loader",
        "description": "modules.model_loader",
        "isExtraImport": true,
        "detail": "modules.model_loader",
        "documentation": {}
    },
    {
        "label": "load_file_from_url",
        "importPath": "modules.model_loader",
        "description": "modules.model_loader",
        "isExtraImport": true,
        "detail": "modules.model_loader",
        "documentation": {}
    },
    {
        "label": "load_file_from_url",
        "importPath": "modules.model_loader",
        "description": "modules.model_loader",
        "isExtraImport": true,
        "detail": "modules.model_loader",
        "documentation": {}
    },
    {
        "label": "get_files_from_folder",
        "importPath": "modules.util",
        "description": "modules.util",
        "isExtraImport": true,
        "detail": "modules.util",
        "documentation": {}
    },
    {
        "label": "resample_image",
        "importPath": "modules.util",
        "description": "modules.util",
        "isExtraImport": true,
        "detail": "modules.util",
        "documentation": {}
    },
    {
        "label": "set_image_shape_ceil",
        "importPath": "modules.util",
        "description": "modules.util",
        "isExtraImport": true,
        "detail": "modules.util",
        "documentation": {}
    },
    {
        "label": "get_image_shape_ceil",
        "importPath": "modules.util",
        "description": "modules.util",
        "isExtraImport": true,
        "detail": "modules.util",
        "documentation": {}
    },
    {
        "label": "generate_temp_filename",
        "importPath": "modules.util",
        "description": "modules.util",
        "isExtraImport": true,
        "detail": "modules.util",
        "documentation": {}
    },
    {
        "label": "get_files_from_folder",
        "importPath": "modules.util",
        "description": "modules.util",
        "isExtraImport": true,
        "detail": "modules.util",
        "documentation": {}
    },
    {
        "label": "modules.patch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.patch",
        "description": "modules.patch",
        "detail": "modules.patch",
        "documentation": {}
    },
    {
        "label": "patch_all",
        "importPath": "modules.patch",
        "description": "modules.patch",
        "isExtraImport": true,
        "detail": "modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_sampler_cfg_function",
        "importPath": "modules.patch",
        "description": "modules.patch",
        "isExtraImport": true,
        "detail": "modules.patch",
        "documentation": {}
    },
    {
        "label": "modules.sample_hijack",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.sample_hijack",
        "description": "modules.sample_hijack",
        "detail": "modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "clip_separate",
        "importPath": "modules.sample_hijack",
        "description": "modules.sample_hijack",
        "isExtraImport": true,
        "detail": "modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "fcbh.latent_formats",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.latent_formats",
        "description": "fcbh.latent_formats",
        "detail": "fcbh.latent_formats",
        "documentation": {}
    },
    {
        "label": "FreeU_V2",
        "importPath": "fcbh_extras.nodes_freelunch",
        "description": "fcbh_extras.nodes_freelunch",
        "isExtraImport": true,
        "detail": "fcbh_extras.nodes_freelunch",
        "documentation": {}
    },
    {
        "label": "match_lora",
        "importPath": "modules.lora",
        "description": "modules.lora",
        "isExtraImport": true,
        "detail": "modules.lora",
        "documentation": {}
    },
    {
        "label": "ModelSamplingDiscrete",
        "importPath": "fcbh_extras.nodes_model_advanced",
        "description": "fcbh_extras.nodes_model_advanced",
        "isExtraImport": true,
        "detail": "fcbh_extras.nodes_model_advanced",
        "documentation": {}
    },
    {
        "label": "modules.inpaint_worker",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.inpaint_worker",
        "description": "modules.inpaint_worker",
        "detail": "modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "fooocus_extras.vae_interpose",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fooocus_extras.vae_interpose",
        "description": "fooocus_extras.vae_interpose",
        "detail": "fooocus_extras.vae_interpose",
        "documentation": {}
    },
    {
        "label": "FooocusExpansion",
        "importPath": "modules.expansion",
        "description": "modules.expansion",
        "isExtraImport": true,
        "detail": "modules.expansion",
        "documentation": {}
    },
    {
        "label": "FooocusExpansion",
        "importPath": "modules.expansion",
        "description": "modules.expansion",
        "isExtraImport": true,
        "detail": "modules.expansion",
        "documentation": {}
    },
    {
        "label": "LogitsProcessorList",
        "importPath": "transformers.generation.logits_process",
        "description": "transformers.generation.logits_process",
        "isExtraImport": true,
        "detail": "transformers.generation.logits_process",
        "documentation": {}
    },
    {
        "label": "PIL.ImageOps",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.ImageOps",
        "description": "PIL.ImageOps",
        "detail": "PIL.ImageOps",
        "documentation": {}
    },
    {
        "label": "gradio.routes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gradio.routes",
        "description": "gradio.routes",
        "detail": "gradio.routes",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "gradio_client",
        "description": "gradio_client",
        "isExtraImport": true,
        "detail": "gradio_client",
        "documentation": {}
    },
    {
        "label": "document",
        "importPath": "gradio_client.documentation",
        "description": "gradio_client.documentation",
        "isExtraImport": true,
        "detail": "gradio_client.documentation",
        "documentation": {}
    },
    {
        "label": "set_documentation_group",
        "importPath": "gradio_client.documentation",
        "description": "gradio_client.documentation",
        "isExtraImport": true,
        "detail": "gradio_client.documentation",
        "documentation": {}
    },
    {
        "label": "ImgSerializable",
        "importPath": "gradio_client.serializing",
        "description": "gradio_client.serializing",
        "isExtraImport": true,
        "detail": "gradio_client.serializing",
        "documentation": {}
    },
    {
        "label": "gradio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gradio",
        "description": "gradio",
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "processing_utils",
        "importPath": "gradio",
        "description": "gradio",
        "isExtraImport": true,
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "gradio",
        "description": "gradio",
        "isExtraImport": true,
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "IOComponent",
        "importPath": "gradio.components.base",
        "description": "gradio.components.base",
        "isExtraImport": true,
        "detail": "gradio.components.base",
        "documentation": {}
    },
    {
        "label": "_Keywords",
        "importPath": "gradio.components.base",
        "description": "gradio.components.base",
        "isExtraImport": true,
        "detail": "gradio.components.base",
        "documentation": {}
    },
    {
        "label": "Block",
        "importPath": "gradio.components.base",
        "description": "gradio.components.base",
        "isExtraImport": true,
        "detail": "gradio.components.base",
        "documentation": {}
    },
    {
        "label": "warn_style_method_deprecation",
        "importPath": "gradio.deprecation",
        "description": "gradio.deprecation",
        "isExtraImport": true,
        "detail": "gradio.deprecation",
        "documentation": {}
    },
    {
        "label": "Changeable",
        "importPath": "gradio.events",
        "description": "gradio.events",
        "isExtraImport": true,
        "detail": "gradio.events",
        "documentation": {}
    },
    {
        "label": "Clearable",
        "importPath": "gradio.events",
        "description": "gradio.events",
        "isExtraImport": true,
        "detail": "gradio.events",
        "documentation": {}
    },
    {
        "label": "Editable",
        "importPath": "gradio.events",
        "description": "gradio.events",
        "isExtraImport": true,
        "detail": "gradio.events",
        "documentation": {}
    },
    {
        "label": "EventListenerMethod",
        "importPath": "gradio.events",
        "description": "gradio.events",
        "isExtraImport": true,
        "detail": "gradio.events",
        "documentation": {}
    },
    {
        "label": "Selectable",
        "importPath": "gradio.events",
        "description": "gradio.events",
        "isExtraImport": true,
        "detail": "gradio.events",
        "documentation": {}
    },
    {
        "label": "Streamable",
        "importPath": "gradio.events",
        "description": "gradio.events",
        "isExtraImport": true,
        "detail": "gradio.events",
        "documentation": {}
    },
    {
        "label": "Uploadable",
        "importPath": "gradio.events",
        "description": "gradio.events",
        "isExtraImport": true,
        "detail": "gradio.events",
        "documentation": {}
    },
    {
        "label": "TokenInterpretable",
        "importPath": "gradio.interpretation",
        "description": "gradio.interpretation",
        "isExtraImport": true,
        "detail": "gradio.interpretation",
        "documentation": {}
    },
    {
        "label": "perform_upscale",
        "importPath": "modules.upscaler",
        "description": "modules.upscaler",
        "isExtraImport": true,
        "detail": "modules.upscaler",
        "documentation": {}
    },
    {
        "label": "importlib.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib.util",
        "description": "importlib.util",
        "detail": "importlib.util",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "modules.anisotropic",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.anisotropic",
        "description": "modules.anisotropic",
        "detail": "modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "fcbh.k_diffusion.sampling",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.k_diffusion.sampling",
        "description": "fcbh.k_diffusion.sampling",
        "detail": "fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "BatchedBrownianTree",
        "importPath": "fcbh.k_diffusion.sampling",
        "description": "fcbh.k_diffusion.sampling",
        "isExtraImport": true,
        "detail": "fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "fcbh.sd1_clip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.sd1_clip",
        "description": "fcbh.sd1_clip",
        "detail": "fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "fcbh.ldm.modules.diffusionmodules.model",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fcbh.ldm.modules.diffusionmodules.model",
        "description": "fcbh.ldm.modules.diffusionmodules.model",
        "detail": "fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "modules.localization",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.localization",
        "description": "modules.localization",
        "detail": "modules.localization",
        "documentation": {}
    },
    {
        "label": "localization_js",
        "importPath": "modules.localization",
        "description": "modules.localization",
        "isExtraImport": true,
        "detail": "modules.localization",
        "documentation": {}
    },
    {
        "label": "RRDBNet",
        "importPath": "fcbh_extras.chainner_models.architecture.RRDB",
        "description": "fcbh_extras.chainner_models.architecture.RRDB",
        "isExtraImport": true,
        "detail": "fcbh_extras.chainner_models.architecture.RRDB",
        "documentation": {}
    },
    {
        "label": "ImageUpscaleWithModel",
        "importPath": "fcbh_extras.nodes_upscale_model",
        "description": "fcbh_extras.nodes_upscale_model",
        "isExtraImport": true,
        "detail": "fcbh_extras.nodes_upscale_model",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "launch",
        "description": "launch",
        "isExtraImport": true,
        "detail": "launch",
        "documentation": {}
    },
    {
        "label": "fooocus_extras.face_crop",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fooocus_extras.face_crop",
        "description": "fooocus_extras.face_crop",
        "detail": "fooocus_extras.face_crop",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "fooocus_version",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fooocus_version",
        "description": "fooocus_version",
        "detail": "fooocus_version",
        "documentation": {}
    },
    {
        "label": "build_launcher",
        "importPath": "build_launcher",
        "description": "build_launcher",
        "isExtraImport": true,
        "detail": "build_launcher",
        "documentation": {}
    },
    {
        "label": "build_launcher",
        "importPath": "build_launcher",
        "description": "build_launcher",
        "isExtraImport": true,
        "detail": "build_launcher",
        "documentation": {}
    },
    {
        "label": "is_installed",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "run",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "python",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "run_pip",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "requirements_met",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "is_installed",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "run",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "python",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "run_pip",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "requirements_met",
        "importPath": "modules.launch_util",
        "description": "modules.launch_util",
        "isExtraImport": true,
        "detail": "modules.launch_util",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "webui",
        "description": "webui",
        "isExtraImport": true,
        "detail": "webui",
        "documentation": {}
    },
    {
        "label": "shared",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shared",
        "description": "shared",
        "detail": "shared",
        "documentation": {}
    },
    {
        "label": "modules.html",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.html",
        "description": "modules.html",
        "detail": "modules.html",
        "documentation": {}
    },
    {
        "label": "modules.async_worker",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.async_worker",
        "description": "modules.async_worker",
        "detail": "modules.async_worker",
        "documentation": {}
    },
    {
        "label": "modules.gradio_hijack",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.gradio_hijack",
        "description": "modules.gradio_hijack",
        "detail": "modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "modules.style_sorter",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modules.style_sorter",
        "description": "modules.style_sorter",
        "detail": "modules.style_sorter",
        "documentation": {}
    },
    {
        "label": "get_current_html_path",
        "importPath": "modules.private_logger",
        "description": "modules.private_logger",
        "isExtraImport": true,
        "detail": "modules.private_logger",
        "documentation": {}
    },
    {
        "label": "reload_javascript",
        "importPath": "modules.ui_gradio_extensions",
        "description": "modules.ui_gradio_extensions",
        "isExtraImport": true,
        "detail": "modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "auth_enabled",
        "importPath": "modules.auth",
        "description": "modules.auth",
        "isExtraImport": true,
        "detail": "modules.auth",
        "documentation": {}
    },
    {
        "label": "check_auth",
        "importPath": "modules.auth",
        "description": "modules.auth",
        "isExtraImport": true,
        "detail": "modules.auth",
        "documentation": {}
    },
    {
        "label": "ControlledUnetModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.cldm.cldm",
        "description": "Fooocus.backend.headless.fcbh.cldm.cldm",
        "peekOfCode": "class ControlledUnetModel(UNetModel):\n    #implemented in the ldm unet\n    pass\nclass ControlNet(nn.Module):\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        hint_channels,",
        "detail": "Fooocus.backend.headless.fcbh.cldm.cldm",
        "documentation": {}
    },
    {
        "label": "ControlNet",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.cldm.cldm",
        "description": "Fooocus.backend.headless.fcbh.cldm.cldm",
        "peekOfCode": "class ControlNet(nn.Module):\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        hint_channels,\n        num_res_blocks,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),",
        "detail": "Fooocus.backend.headless.fcbh.cldm.cldm",
        "documentation": {}
    },
    {
        "label": "NoiseScheduleVP",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "class NoiseScheduleVP:\n    def __init__(\n            self,\n            schedule='discrete',\n            betas=None,\n            alphas_cumprod=None,\n            continuous_beta_0=0.1,\n            continuous_beta_1=20.,\n        ):\n        \"\"\"Create a wrapper class for the forward SDE (VP type).",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "UniPC",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "class UniPC:\n    def __init__(\n        self,\n        model_fn,\n        noise_schedule,\n        predict_x0=True,\n        thresholding=False,\n        max_val=1.,\n        variant='bh1',\n        noise_mask=None,",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "SigmaConvert",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "class SigmaConvert:\n    schedule = \"\"\n    def marginal_log_mean_coeff(self, sigma):\n        return 0.5 * torch.log(1 / ((sigma * sigma) + 1))\n    def marginal_alpha(self, t):\n        return torch.exp(self.marginal_log_mean_coeff(t))\n    def marginal_std(self, t):\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n    def marginal_lambda(self, t):\n        \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "model_wrapper",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "def model_wrapper(\n    model,\n    noise_schedule,\n    model_type=\"noise\",\n    model_kwargs={},\n    guidance_type=\"uncond\",\n    condition=None,\n    unconditional_condition=None,\n    guidance_scale=1.,\n    classifier_fn=None,",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "interpolate_fn",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "def interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "expand_dims",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "predict_eps_sigma",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "def predict_eps_sigma(model, input, sigma_in, **kwargs):\n    sigma = sigma_in.view(sigma_in.shape[:1] + (1,) * (input.ndim - 1))\n    input = input * ((sigma ** 2 + 1.0) ** 0.5)\n    return  (input - model(input, sigma_in, **kwargs)) / sigma\ndef sample_unipc(model, noise, image, sigmas, max_denoise, extra_args=None, callback=None, disable=False, noise_mask=None, variant='bh1'):\n        timesteps = sigmas.clone()\n        if sigmas[-1] == 0:\n            timesteps = sigmas[:]\n            timesteps[-1] = 0.001\n        else:",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "sample_unipc",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "description": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "peekOfCode": "def sample_unipc(model, noise, image, sigmas, max_denoise, extra_args=None, callback=None, disable=False, noise_mask=None, variant='bh1'):\n        timesteps = sigmas.clone()\n        if sigmas[-1] == 0:\n            timesteps = sigmas[:]\n            timesteps[-1] = 0.001\n        else:\n            timesteps = sigmas.clone()\n        ns = SigmaConvert()\n        if image is not None:\n            img = image * ns.marginal_alpha(timesteps[0])",
        "detail": "Fooocus.backend.headless.fcbh.extra_samplers.uni_pc",
        "documentation": {}
    },
    {
        "label": "BatchedBrownianTree",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "class BatchedBrownianTree:\n    \"\"\"A wrapper around torchsde.BrownianTree that enables batches of entropy.\"\"\"\n    def __init__(self, x, t0, t1, seed=None, **kwargs):\n        self.cpu_tree = True\n        if \"cpu\" in kwargs:\n            self.cpu_tree = kwargs.pop(\"cpu\")\n        t0, t1, self.sign = self.sort(t0, t1)\n        w0 = kwargs.get('w0', torch.zeros_like(x))\n        if seed is None:\n            seed = torch.randint(0, 2 ** 63 - 1, []).item()",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "BrownianTreeNoiseSampler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "class BrownianTreeNoiseSampler:\n    \"\"\"A noise sampler backed by a torchsde.BrownianTree.\n    Args:\n        x (Tensor): The tensor whose shape, device and dtype to use to generate\n            random samples.\n        sigma_min (float): The low end of the valid interval.\n        sigma_max (float): The high end of the valid interval.\n        seed (int or List[int]): The random seed. If a list of seeds is\n            supplied instead of a single integer, then the noise sampler will\n            use one BrownianTree per batch item, each with its own seed.",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "PIDStepSizeController",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "class PIDStepSizeController:\n    \"\"\"A PID controller for ODE adaptive step size control.\"\"\"\n    def __init__(self, h, pcoeff, icoeff, dcoeff, order=1, accept_safety=0.81, eps=1e-8):\n        self.h = h\n        self.b1 = (pcoeff + icoeff + dcoeff) / order\n        self.b2 = -(pcoeff + 2 * dcoeff) / order\n        self.b3 = dcoeff / order\n        self.accept_safety = accept_safety\n        self.eps = eps\n        self.errs = []",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "DPMSolver",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "class DPMSolver(nn.Module):\n    \"\"\"DPM-Solver. See https://arxiv.org/abs/2206.00927.\"\"\"\n    def __init__(self, model, extra_args=None, eps_callback=None, info_callback=None):\n        super().__init__()\n        self.model = model\n        self.extra_args = {} if extra_args is None else extra_args\n        self.eps_callback = eps_callback\n        self.info_callback = info_callback\n    def t(self, sigma):\n        return -sigma.log()",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "append_zero",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def append_zero(x):\n    return torch.cat([x, x.new_zeros([1])])\ndef get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\n    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n    ramp = torch.linspace(0, 1, n, device=device)\n    min_inv_rho = sigma_min ** (1 / rho)\n    max_inv_rho = sigma_max ** (1 / rho)\n    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n    return append_zero(sigmas).to(device)\ndef get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "get_sigmas_karras",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\n    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n    ramp = torch.linspace(0, 1, n, device=device)\n    min_inv_rho = sigma_min ** (1 / rho)\n    max_inv_rho = sigma_max ** (1 / rho)\n    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n    return append_zero(sigmas).to(device)\ndef get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\n    \"\"\"Constructs an exponential noise schedule.\"\"\"\n    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "get_sigmas_exponential",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\n    \"\"\"Constructs an exponential noise schedule.\"\"\"\n    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\n    return append_zero(sigmas)\ndef get_sigmas_polyexponential(n, sigma_min, sigma_max, rho=1., device='cpu'):\n    \"\"\"Constructs an polynomial in log sigma noise schedule.\"\"\"\n    ramp = torch.linspace(1, 0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\ndef get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "get_sigmas_polyexponential",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def get_sigmas_polyexponential(n, sigma_min, sigma_max, rho=1., device='cpu'):\n    \"\"\"Constructs an polynomial in log sigma noise schedule.\"\"\"\n    ramp = torch.linspace(1, 0, n, device=device) ** rho\n    sigmas = torch.exp(ramp * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n    return append_zero(sigmas)\ndef get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\n    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\n    t = torch.linspace(1, eps_s, n, device=device)\n    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\n    return append_zero(sigmas)",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "get_sigmas_vp",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\n    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\n    t = torch.linspace(1, eps_s, n, device=device)\n    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\n    return append_zero(sigmas)\ndef to_d(x, sigma, denoised):\n    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n    return (x - denoised) / utils.append_dims(sigma, x.ndim)\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "to_d",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def to_d(x, sigma, denoised):\n    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n    return (x - denoised) / utils.append_dims(sigma, x.ndim)\ndef get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n    if not eta:\n        return sigma_to, 0.\n    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\n    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "get_ancestral_step",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n    if not eta:\n        return sigma_to, 0.\n    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\n    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n    return sigma_down, sigma_up\ndef default_noise_sampler(x):\n    return lambda sigma, sigma_next: torch.randn_like(x)",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "default_noise_sampler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def default_noise_sampler(x):\n    return lambda sigma, sigma_next: torch.randn_like(x)\nclass BatchedBrownianTree:\n    \"\"\"A wrapper around torchsde.BrownianTree that enables batches of entropy.\"\"\"\n    def __init__(self, x, t0, t1, seed=None, **kwargs):\n        self.cpu_tree = True\n        if \"cpu\" in kwargs:\n            self.cpu_tree = kwargs.pop(\"cpu\")\n        t0, t1, self.sign = self.sort(t0, t1)\n        w0 = kwargs.get('w0', torch.zeros_like(x))",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_euler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_euler_ancestral",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with Euler method steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_heun",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_heun(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"Implements Algorithm 2 (Heun steps) from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpm_2",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpm_2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    \"\"\"A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:\n            eps = torch.randn_like(x) * s_noise\n            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpm_2_ancestral",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpm_2_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with DPM-Solver second-order steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "linear_multistep_coeff",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def linear_multistep_coeff(order, t, i, j):\n    if order - 1 > i:\n        raise ValueError(f'Order {order} too high for step {i}')\n    def fn(tau):\n        prod = 1.\n        for k in range(order):\n            if j == k:\n                continue\n            prod *= (tau - t[i - k]) / (t[i - j] - t[i - k])\n        return prod",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_lms",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_lms(model, x, sigmas, extra_args=None, callback=None, disable=None, order=4):\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigmas_cpu = sigmas.detach().cpu().numpy()\n    ds = []\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        d = to_d(x, sigmas[i], denoised)\n        ds.append(d)\n        if len(ds) > order:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpm_fast",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpm_fast(model, x, sigma_min, sigma_max, n, extra_args=None, callback=None, disable=None, eta=0., s_noise=1., noise_sampler=None):\n    \"\"\"DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927.\"\"\"\n    if sigma_min <= 0 or sigma_max <= 0:\n        raise ValueError('sigma_min and sigma_max must not be 0')\n    with tqdm(total=n, disable=disable) as pbar:\n        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)\n        if callback is not None:\n            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})\n        return dpm_solver.dpm_solver_fast(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), n, eta, s_noise, noise_sampler)\n@torch.no_grad()",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpm_adaptive",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpm_adaptive(model, x, sigma_min, sigma_max, extra_args=None, callback=None, disable=None, order=3, rtol=0.05, atol=0.0078, h_init=0.05, pcoeff=0., icoeff=1., dcoeff=0., accept_safety=0.81, eta=0., s_noise=1., noise_sampler=None, return_info=False):\n    \"\"\"DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927.\"\"\"\n    if sigma_min <= 0 or sigma_max <= 0:\n        raise ValueError('sigma_min and sigma_max must not be 0')\n    with tqdm(disable=disable) as pbar:\n        dpm_solver = DPMSolver(model, extra_args, eps_callback=pbar.update)\n        if callback is not None:\n            dpm_solver.info_callback = lambda info: callback({'sigma': dpm_solver.sigma(info['t']), 'sigma_hat': dpm_solver.sigma(info['t_up']), **info})\n        x, info = dpm_solver.dpm_solver_adaptive(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise, noise_sampler)\n    if return_info:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_2s_ancestral",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_2s_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with DPM-Solver++(2S) second-order steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_sde",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, r=1 / 2):\n    \"\"\"DPM-Solver++ (stochastic).\"\"\"\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    seed = extra_args.get(\"seed\", None)\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n    for i in trange(len(sigmas) - 1, disable=disable):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_2m",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_2m(model, x, sigmas, extra_args=None, callback=None, disable=None):\n    \"\"\"DPM-Solver++(2M).\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = lambda t: t.neg().exp()\n    t_fn = lambda sigma: sigma.log().neg()\n    old_denoised = None\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_2m_sde",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_2m_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, solver_type='midpoint'):\n    \"\"\"DPM-Solver++(2M) SDE.\"\"\"\n    if solver_type not in {'heun', 'midpoint'}:\n        raise ValueError('solver_type must be \\'heun\\' or \\'midpoint\\'')\n    seed = extra_args.get(\"seed\", None)\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    old_denoised = None",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_3m_sde",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_3m_sde(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"DPM-Solver++(3M) SDE.\"\"\"\n    seed = extra_args.get(\"seed\", None)\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=seed, cpu=True) if noise_sampler is None else noise_sampler\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    denoised_1, denoised_2 = None, None\n    h, h_1, h_2 = None, None, None\n    for i in trange(len(sigmas) - 1, disable=disable):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_3m_sde_gpu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_3m_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_3m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler)\n@torch.no_grad()\ndef sample_dpmpp_2m_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, solver_type='midpoint'):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_2m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, solver_type=solver_type)\n@torch.no_grad()",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_2m_sde_gpu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_2m_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, solver_type='midpoint'):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_2m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, solver_type=solver_type)\n@torch.no_grad()\ndef sample_dpmpp_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, r=1 / 2):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, r=r)\ndef DDPMSampler_step(x, sigma, sigma_prev, noise, noise_sampler):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_sde_gpu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_dpmpp_sde_gpu(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None, r=1 / 2):\n    sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n    noise_sampler = BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=extra_args.get(\"seed\", None), cpu=False) if noise_sampler is None else noise_sampler\n    return sample_dpmpp_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, r=r)\ndef DDPMSampler_step(x, sigma, sigma_prev, noise, noise_sampler):\n    alpha_cumprod = 1 / ((sigma * sigma) + 1)\n    alpha_cumprod_prev = 1 / ((sigma_prev * sigma_prev) + 1)\n    alpha = (alpha_cumprod / alpha_cumprod_prev)\n    mu = (1.0 / alpha).sqrt() * (x - (1 - alpha) * noise / (1 - alpha_cumprod).sqrt())\n    if sigma_prev > 0:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "DDPMSampler_step",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def DDPMSampler_step(x, sigma, sigma_prev, noise, noise_sampler):\n    alpha_cumprod = 1 / ((sigma * sigma) + 1)\n    alpha_cumprod_prev = 1 / ((sigma_prev * sigma_prev) + 1)\n    alpha = (alpha_cumprod / alpha_cumprod_prev)\n    mu = (1.0 / alpha).sqrt() * (x - (1 - alpha) * noise / (1 - alpha_cumprod).sqrt())\n    if sigma_prev > 0:\n        mu += ((1 - alpha) * (1. - alpha_cumprod_prev) / (1. - alpha_cumprod)).sqrt() * noise_sampler(sigma, sigma_prev)\n    return mu\ndef generic_step_sampler(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None, step_function=None):\n    extra_args = {} if extra_args is None else extra_args",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "generic_step_sampler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def generic_step_sampler(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None, step_function=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = step_function(x / torch.sqrt(1.0 + sigmas[i] ** 2.0), sigmas[i], sigmas[i + 1], (x - denoised) / sigmas[i], noise_sampler)\n        if sigmas[i + 1] != 0:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_ddpm",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_ddpm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    return generic_step_sampler(model, x, sigmas, extra_args, callback, disable, noise_sampler, DDPMSampler_step)\n@torch.no_grad()\ndef sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_lcm",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = denoised\n        if sigmas[i + 1] > 0:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_heunpp2",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "peekOfCode": "def sample_heunpp2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n    # From MIT licensed: https://github.com/Carzit/sd-webui-samplers-scheduler/\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    s_end = sigmas[-1]\n    for i in trange(len(sigmas) - 1, disable=disable):\n        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n        eps = torch.randn_like(x) * s_noise\n        sigma_hat = sigmas[i] * (gamma + 1)\n        if gamma > 0:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "EMAWarmup",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "class EMAWarmup:\n    \"\"\"Implements an EMA warmup using an inverse decay schedule.\n    If inv_gamma=1 and power=1, implements a simple average. inv_gamma=1, power=2/3 are\n    good values for models you plan to train for a million or more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "InverseLR",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "class InverseLR(optim.lr_scheduler._LRScheduler):\n    \"\"\"Implements an inverse decay learning rate schedule with an optional exponential\n    warmup. When last_epoch=-1, sets initial lr as lr.\n    inv_gamma is the number of steps/epochs required for the learning rate to decay to\n    (1 / 2)**power of its original value.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        inv_gamma (float): Inverse multiplicative factor of learning rate decay. Default: 1.\n        power (float): Exponential factor of learning rate decay. Default: 1.\n        warmup (float): Exponential warmup factor (0 <= warmup < 1, 0 to disable)",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "ExponentialLR",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "class ExponentialLR(optim.lr_scheduler._LRScheduler):\n    \"\"\"Implements an exponential learning rate schedule with an optional exponential\n    warmup. When last_epoch=-1, sets initial lr as lr. Decays the learning rate\n    continuously by decay (default 0.5) every num_steps steps.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        num_steps (float): The number of steps to decay the learning rate by decay in.\n        decay (float): The factor by which to decay the learning rate every num_steps\n            steps. Default: 0.5.\n        warmup (float): Exponential warmup factor (0 <= warmup < 1, 0 to disable)",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "FolderOfImages",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "class FolderOfImages(data.Dataset):\n    \"\"\"Recursively finds all images in a directory. It does not support\n    classes/targets.\"\"\"\n    IMG_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'}\n    def __init__(self, root, transform=None):\n        super().__init__()\n        self.root = Path(root)\n        self.transform = nn.Identity() if transform is None else transform\n        self.paths = sorted(path for path in self.root.rglob('*') if path.suffix.lower() in self.IMG_EXTENSIONS)\n    def __repr__(self):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "CSVLogger",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "class CSVLogger:\n    def __init__(self, filename, columns):\n        self.filename = Path(filename)\n        self.columns = columns\n        if self.filename.exists():\n            self.file = open(self.filename, 'a')\n        else:\n            self.file = open(self.filename, 'w')\n            self.write(*self.columns)\n    def write(self, *args):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "hf_datasets_augs_helper",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def hf_datasets_augs_helper(examples, transform, image_key, mode='RGB'):\n    \"\"\"Apply passed in transforms for HuggingFace Datasets.\"\"\"\n    images = [transform(image.convert(mode)) for image in examples[image_key]]\n    return {image_key: images}\ndef append_dims(x, target_dims):\n    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n    expanded = x[(...,) + (None,) * dims_to_append]",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "append_dims",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def append_dims(x, target_dims):\n    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n    expanded = x[(...,) + (None,) * dims_to_append]\n    # MPS will get inf values if it tries to index into the new axes, but detaching fixes this.\n    # https://github.com/pytorch/pytorch/issues/84364\n    return expanded.detach().clone() if expanded.device.type == 'mps' else expanded\ndef n_params(module):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "n_params",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def n_params(module):\n    \"\"\"Returns the number of trainable parameters in a module.\"\"\"\n    return sum(p.numel() for p in module.parameters())\ndef download_file(path, url, digest=None):\n    \"\"\"Downloads a file if it does not exist, optionally checking its SHA-256 hash.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:\n            shutil.copyfileobj(response, f)",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "download_file",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def download_file(path, url, digest=None):\n    \"\"\"Downloads a file if it does not exist, optionally checking its SHA-256 hash.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        file_digest = hashlib.sha256(open(path, 'rb').read()).hexdigest()\n        if digest != file_digest:",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "train_mode",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def train_mode(model, mode=True):\n    \"\"\"A context manager that places a model into training mode and restores\n    the previous mode on exit.\"\"\"\n    modes = [module.training for module in model.modules()]\n    try:\n        yield model.train(mode)\n    finally:\n        for i, module in enumerate(model.modules()):\n            module.training = modes[i]\ndef eval_mode(model):",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "eval_mode",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def eval_mode(model):\n    \"\"\"A context manager that places a model into evaluation mode and restores\n    the previous mode on exit.\"\"\"\n    return train_mode(model, False)\n@torch.no_grad()\ndef ema_update(model, averaged_model, decay):\n    \"\"\"Incorporates updated model parameters into an exponential moving averaged\n    version of a model. It should be called after each optimizer step.\"\"\"\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "ema_update",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def ema_update(model, averaged_model, decay):\n    \"\"\"Incorporates updated model parameters into an exponential moving averaged\n    version of a model. It should be called after each optimizer step.\"\"\"\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    assert model_params.keys() == averaged_params.keys()\n    for name, param in model_params.items():\n        averaged_params[name].mul_(decay).add_(param, alpha=1 - decay)\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "rand_log_normal",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def rand_log_normal(shape, loc=0., scale=1., device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an lognormal distribution.\"\"\"\n    return (torch.randn(shape, device=device, dtype=dtype) * scale + loc).exp()\ndef rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an optionally truncated log-logistic distribution.\"\"\"\n    min_value = torch.as_tensor(min_value, device=device, dtype=torch.float64)\n    max_value = torch.as_tensor(max_value, device=device, dtype=torch.float64)\n    min_cdf = min_value.log().sub(loc).div(scale).sigmoid()\n    max_cdf = max_value.log().sub(loc).div(scale).sigmoid()\n    u = torch.rand(shape, device=device, dtype=torch.float64) * (max_cdf - min_cdf) + min_cdf",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "rand_log_logistic",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an optionally truncated log-logistic distribution.\"\"\"\n    min_value = torch.as_tensor(min_value, device=device, dtype=torch.float64)\n    max_value = torch.as_tensor(max_value, device=device, dtype=torch.float64)\n    min_cdf = min_value.log().sub(loc).div(scale).sigmoid()\n    max_cdf = max_value.log().sub(loc).div(scale).sigmoid()\n    u = torch.rand(shape, device=device, dtype=torch.float64) * (max_cdf - min_cdf) + min_cdf\n    return u.logit().mul(scale).add(loc).exp().to(dtype)\ndef rand_log_uniform(shape, min_value, max_value, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an log-uniform distribution.\"\"\"",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "rand_log_uniform",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def rand_log_uniform(shape, min_value, max_value, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an log-uniform distribution.\"\"\"\n    min_value = math.log(min_value)\n    max_value = math.log(max_value)\n    return (torch.rand(shape, device=device, dtype=dtype) * (max_value - min_value) + min_value).exp()\ndef rand_v_diffusion(shape, sigma_data=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a truncated v-diffusion training timestep distribution.\"\"\"\n    min_cdf = math.atan(min_value / sigma_data) * 2 / math.pi\n    max_cdf = math.atan(max_value / sigma_data) * 2 / math.pi\n    u = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "rand_v_diffusion",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def rand_v_diffusion(shape, sigma_data=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a truncated v-diffusion training timestep distribution.\"\"\"\n    min_cdf = math.atan(min_value / sigma_data) * 2 / math.pi\n    max_cdf = math.atan(max_value / sigma_data) * 2 / math.pi\n    u = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    return torch.tan(u * math.pi / 2) * sigma_data\ndef rand_split_log_normal(shape, loc, scale_1, scale_2, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a split lognormal distribution.\"\"\"\n    n = torch.randn(shape, device=device, dtype=dtype).abs()\n    u = torch.rand(shape, device=device, dtype=dtype)",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "rand_split_log_normal",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def rand_split_log_normal(shape, loc, scale_1, scale_2, device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from a split lognormal distribution.\"\"\"\n    n = torch.randn(shape, device=device, dtype=dtype).abs()\n    u = torch.rand(shape, device=device, dtype=dtype)\n    n_left = n * -scale_1 + loc\n    n_right = n * scale_2 + loc\n    ratio = scale_1 / (scale_1 + scale_2)\n    return torch.where(u < ratio, n_left, n_right).exp()\nclass FolderOfImages(data.Dataset):\n    \"\"\"Recursively finds all images in a directory. It does not support",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "tf32_mode",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "description": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "peekOfCode": "def tf32_mode(cudnn=None, matmul=None):\n    \"\"\"A context manager that sets whether TF32 is allowed on cuDNN or matmul.\"\"\"\n    cudnn_old = torch.backends.cudnn.allow_tf32\n    matmul_old = torch.backends.cuda.matmul.allow_tf32\n    try:\n        if cudnn is not None:\n            torch.backends.cudnn.allow_tf32 = cudnn\n        if matmul is not None:\n            torch.backends.cuda.matmul.allow_tf32 = matmul\n        yield",
        "detail": "Fooocus.backend.headless.fcbh.k_diffusion.utils",
        "documentation": {}
    },
    {
        "label": "DiagonalGaussianRegularizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "description": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "peekOfCode": "class DiagonalGaussianRegularizer(torch.nn.Module):\n    def __init__(self, sample: bool = True):\n        super().__init__()\n        self.sample = sample\n    def get_trainable_parameters(self) -> Any:\n        yield from ()\n    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n        log = dict()\n        posterior = DiagonalGaussianDistribution(z)\n        if self.sample:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "documentation": {}
    },
    {
        "label": "AbstractAutoencoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "description": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "peekOfCode": "class AbstractAutoencoder(torch.nn.Module):\n    \"\"\"\n    This is the base class for all autoencoders, including image autoencoders, image autoencoders with discriminators,\n    unCLIP models, etc. Hence, it is fairly general, and specific features\n    (e.g. discriminator training, encoding, decoding) must be implemented in subclasses.\n    \"\"\"\n    def __init__(\n        self,\n        ema_decay: Union[None, float] = None,\n        monitor: Union[None, str] = None,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "documentation": {}
    },
    {
        "label": "AutoencodingEngine",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "description": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "peekOfCode": "class AutoencodingEngine(AbstractAutoencoder):\n    \"\"\"\n    Base class for all image autoencoders that we train, like VQGAN or AutoencoderKL\n    (we also restore them explicitly as special cases for legacy reasons).\n    Regularizations such as KL or VQ are moved to the regularizer class.\n    \"\"\"\n    def __init__(\n        self,\n        *args,\n        encoder_config: Dict,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "documentation": {}
    },
    {
        "label": "AutoencodingEngineLegacy",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "description": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "peekOfCode": "class AutoencodingEngineLegacy(AutoencodingEngine):\n    def __init__(self, embed_dim: int, **kwargs):\n        self.max_batch_size = kwargs.pop(\"max_batch_size\", None)\n        ddconfig = kwargs.pop(\"ddconfig\")\n        super().__init__(\n            encoder_config={\n                \"target\": \"fcbh.ldm.modules.diffusionmodules.model.Encoder\",\n                \"params\": ddconfig,\n            },\n            decoder_config={",
        "detail": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "documentation": {}
    },
    {
        "label": "AutoencoderKL",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "description": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "peekOfCode": "class AutoencoderKL(AutoencodingEngineLegacy):\n    def __init__(self, **kwargs):\n        if \"lossconfig\" in kwargs:\n            kwargs[\"loss_config\"] = kwargs.pop(\"lossconfig\")\n        super().__init__(\n            regularizer_config={\n                \"target\": (\n                    \"fcbh.ldm.models.autoencoder.DiagonalGaussianRegularizer\"\n                )\n            },",
        "detail": "Fooocus.backend.headless.fcbh.ldm.models.autoencoder",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "class Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = fcbh.ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "class Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = fcbh.ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "ResnetBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "class ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n        self.swish = torch.nn.SiLU(inplace=True)\n        self.norm1 = Normalize(in_channels)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "AttnBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "class AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.norm = Normalize(in_channels)\n        self.q = fcbh.ops.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "class Model(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = self.ch*4\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n                 **ignore_kwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n                 conv_out_op=fcbh.ops.Conv2d,\n                 resnet_op=ResnetBlock,\n                 attn_op=AttnBlock,\n                **ignorekwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "get_timestep_embedding",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "nonlinearity",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\ndef Normalize(in_channels, num_groups=32):\n    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def Normalize(in_channels, num_groups=32):\n    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = fcbh.ops.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "slice_attention",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def slice_attention(q, k, v):\n    r1 = torch.zeros_like(k, device=q.device)\n    scale = (int(q.shape[-1])**(-0.5))\n    mem_free_total = model_management.get_free_memory(q.device)\n    gb = 1024 ** 3\n    tensor_size = q.shape[0] * q.shape[1] * k.shape[2] * q.element_size()\n    modifier = 3 if q.element_size() == 2 else 2.5\n    mem_required = tensor_size * modifier\n    steps = 1\n    if mem_required > mem_free_total:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "normal_attention",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def normal_attention(q, k, v):\n    # compute attention\n    b,c,h,w = q.shape\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n    r1 = slice_attention(q, k, v)\n    h_ = r1.reshape(b,c,h,w)\n    del r1",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "xformers_attention",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def xformers_attention(q, k, v):\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, C, -1).transpose(1, 2).contiguous(),\n        (q, k, v),\n    )\n    try:\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None)\n        out = out.transpose(1, 2).reshape(B, C, H, W)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "pytorch_attention",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def pytorch_attention(q, k, v):\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, 1, C, -1).transpose(2, 3).contiguous(),\n        (q, k, v),\n    )\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n        out = out.transpose(2, 3).reshape(B, C, H, W)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "make_attn",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "peekOfCode": "def make_attn(in_channels, attn_type=\"vanilla\", attn_kwargs=None):\n    return AttnBlock(in_channels)\nclass Model(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = self.ch*4",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.model",
        "documentation": {}
    },
    {
        "label": "TimestepBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "class TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n#This is needed because accelerate makes a copy of transformer_options which breaks \"current_index\"",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "TimestepEmbedSequential",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n    def forward(self, *args, **kwargs):\n        return forward_timestep_embed(self, *args, **kwargs)\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "class Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1, dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "class Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1, dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "class ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "Timestep",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "class Timestep(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    def forward(self, t):\n        return timestep_embedding(t, self.dim)\ndef apply_control(h, control, name):\n    if control is not None and name in control and len(control[name]) > 0:\n        ctrl = control[name].pop()\n        if ctrl is not None:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "UNetModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "class UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "forward_timestep_embed",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "def forward_timestep_embed(ts, x, emb, context=None, transformer_options={}, output_shape=None):\n    for layer in ts:\n        if isinstance(layer, TimestepBlock):\n            x = layer(x, emb)\n        elif isinstance(layer, SpatialTransformer):\n            x = layer(x, context, transformer_options)\n            if \"current_index\" in transformer_options:\n                transformer_options[\"current_index\"] += 1\n        elif isinstance(layer, Upsample):\n            x = layer(x, output_shape=output_shape)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "apply_control",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "peekOfCode": "def apply_control(h, control, name):\n    if control is not None and name in control and len(control[name]) > 0:\n        ctrl = control[name].pop()\n        if ctrl is not None:\n            try:\n                h += ctrl\n            except:\n                print(\"warning control could not be applied\", h.shape, ctrl.shape)\n    return h\nclass UNetModel(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.openaimodel",
        "documentation": {}
    },
    {
        "label": "AbstractLowScaleModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "peekOfCode": "class AbstractLowScaleModel(nn.Module):\n    # for concatenating a downsampled image to the latent representation\n    def __init__(self, noise_schedule_config=None):\n        super(AbstractLowScaleModel, self).__init__()\n        if noise_schedule_config is not None:\n            self.register_schedule(**noise_schedule_config)\n    def register_schedule(self, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                   cosine_s=cosine_s)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "documentation": {}
    },
    {
        "label": "SimpleImageConcat",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "peekOfCode": "class SimpleImageConcat(AbstractLowScaleModel):\n    # no noise level conditioning\n    def __init__(self):\n        super(SimpleImageConcat, self).__init__(noise_schedule_config=None)\n        self.max_noise_level = 0\n    def forward(self, x):\n        # fix to constant noise level\n        return x, torch.zeros(x.shape[0], device=x.device).long()\nclass ImageConcatWithNoiseAugmentation(AbstractLowScaleModel):\n    def __init__(self, noise_schedule_config, max_noise_level=1000, to_cuda=False):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "documentation": {}
    },
    {
        "label": "ImageConcatWithNoiseAugmentation",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "peekOfCode": "class ImageConcatWithNoiseAugmentation(AbstractLowScaleModel):\n    def __init__(self, noise_schedule_config, max_noise_level=1000, to_cuda=False):\n        super().__init__(noise_schedule_config=noise_schedule_config)\n        self.max_noise_level = max_noise_level\n    def forward(self, x, noise_level=None):\n        if noise_level is None:\n            noise_level = torch.randint(0, self.max_noise_level, (x.shape[0],), device=x.device).long()\n        else:\n            assert isinstance(noise_level, torch.Tensor)\n        z = self.q_sample(x, noise_level)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.upscaling",
        "documentation": {}
    },
    {
        "label": "CheckpointFunction",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "class CheckpointFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        ctx.gpu_autocast_kwargs = {\"enabled\": torch.is_autocast_enabled(),\n                                   \"dtype\": torch.get_autocast_gpu_dtype(),\n                                   \"cache_enabled\": torch.is_autocast_cache_enabled()}\n        with torch.no_grad():",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "SiLU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "class SiLU(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "GroupNorm32",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "class GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "HybridConditioner",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "class HybridConditioner(nn.Module):\n    def __init__(self, c_concat_config, c_crossattn_config):\n        super().__init__()\n        self.concat_conditioner = instantiate_from_config(c_concat_config)\n        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n    def forward(self, c_concat, c_crossattn):\n        c_concat = self.concat_conditioner(c_concat)\n        c_crossattn = self.crossattn_conditioner(c_crossattn)\n        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}\ndef noise_like(shape, device, repeat=False):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "make_beta_schedule",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n    if schedule == \"linear\":\n        betas = (\n                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n        )\n    elif schedule == \"cosine\":\n        timesteps = (\n                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n        )\n        alphas = timesteps / (1 + cosine_s) * np.pi / 2",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "make_ddim_timesteps",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n    if ddim_discr_method == 'uniform':\n        c = num_ddpm_timesteps // num_ddim_timesteps\n        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n    elif ddim_discr_method == 'quad':\n        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n    else:\n        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n    # add one to get the final alpha values right (the ones from first scale to data during sampling)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "make_ddim_sampling_parameters",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n    # select alphas for computing the variance schedule\n    alphas = alphacums[ddim_timesteps]\n    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n    # according the the formula provided in https://arxiv.org/abs/2010.02502\n    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n    if verbose:\n        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n        print(f'For the chosen value of eta, which is {eta}, '\n              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "betas_for_alpha_bar",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "extract_into_tensor",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def extract_into_tensor(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "timestep_embedding",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    if not repeat_only:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "zero_module",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "scale_module",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "mean_flat",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\ndef normalization(channels, dtype=None):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "normalization",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def normalization(channels, dtype=None):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels, dtype=dtype)\n# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\nclass SiLU(nn.Module):\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "conv_nd",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return fcbh.ops.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "linear",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return fcbh.ops.Linear(*args, **kwargs)\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "avg_pool_nd",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "noise_like",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "peekOfCode": "def noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.diffusionmodules.util",
        "documentation": {}
    },
    {
        "label": "AbstractDistribution",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "peekOfCode": "class AbstractDistribution:\n    def sample(self):\n        raise NotImplementedError()\n    def mode(self):\n        raise NotImplementedError()\nclass DiracDistribution(AbstractDistribution):\n    def __init__(self, value):\n        self.value = value\n    def sample(self):\n        return self.value",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "documentation": {}
    },
    {
        "label": "DiracDistribution",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "peekOfCode": "class DiracDistribution(AbstractDistribution):\n    def __init__(self, value):\n        self.value = value\n    def sample(self):\n        return self.value\n    def mode(self):\n        return self.value\nclass DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "documentation": {}
    },
    {
        "label": "DiagonalGaussianDistribution",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "peekOfCode": "class DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "documentation": {}
    },
    {
        "label": "normal_kl",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "peekOfCode": "def normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.distributions.distributions",
        "documentation": {}
    },
    {
        "label": "CLIPEmbeddingNoiseAugmentation",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.encoders.noise_aug_modules",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.encoders.noise_aug_modules",
        "peekOfCode": "class CLIPEmbeddingNoiseAugmentation(ImageConcatWithNoiseAugmentation):\n    def __init__(self, *args, clip_stats_path=None, timestep_dim=256, **kwargs):\n        super().__init__(*args, **kwargs)\n        if clip_stats_path is None:\n            clip_mean, clip_std = torch.zeros(timestep_dim), torch.ones(timestep_dim)\n        else:\n            clip_mean, clip_std = torch.load(clip_stats_path, map_location=\"cpu\")\n        self.register_buffer(\"data_mean\", clip_mean[None, :], persistent=False)\n        self.register_buffer(\"data_std\", clip_std[None, :], persistent=False)\n        self.time_embed = Timestep(timestep_dim)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.encoders.noise_aug_modules",
        "documentation": {}
    },
    {
        "label": "GEGLU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "class GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out, dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()\n        self.proj = operations.Linear(dim_in, dim_out * 2, dtype=dtype, device=device)\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0., dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0., dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            operations.Linear(dim, inner_dim, dtype=dtype, device=device),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim, dtype=dtype, device=device, operations=operations)\n        self.net = nn.Sequential(",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "CrossAttention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "class CrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n        self.heads = heads\n        self.dim_head = dim_head\n        self.to_q = operations.Linear(query_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.to_k = operations.Linear(context_dim, inner_dim, bias=False, dtype=dtype, device=device)\n        self.to_v = operations.Linear(context_dim, inner_dim, bias=False, dtype=dtype, device=device)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "BasicTransformerBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "class BasicTransformerBlock(nn.Module):\n    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True,\n                 disable_self_attn=False, dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()\n        self.disable_self_attn = disable_self_attn\n        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout,\n                              context_dim=context_dim if self.disable_self_attn else None, dtype=dtype, device=device, operations=operations)  # is a self-attention if not self.disable_self_attn\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff, dtype=dtype, device=device, operations=operations)\n        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,\n                              heads=n_heads, dim_head=d_head, dropout=dropout, dtype=dtype, device=device, operations=operations)  # is self-attn if context is none",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "SpatialTransformer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "class SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    NEW: use_linear for more efficiency instead of the 1x1 convs\n    \"\"\"\n    def __init__(self, in_channels, n_heads, d_head,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\ndef default(val, d):\n    if exists(val):\n        return val\n    return d\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "uniq",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def uniq(arr):\n    return{el: True for el in arr}.keys()\ndef default(val, d):\n    if exists(val):\n        return val\n    return d\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\ndef init_(tensor):\n    dim = tensor.shape[-1]",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def default(val, d):\n    if exists(val):\n        return val\n    return d\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "max_neg_value",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def max_neg_value(t):\n    return -torch.finfo(t.dtype).max\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out, dtype=None, device=None, operations=fcbh.ops):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "init_",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out, dtype=None, device=None, operations=fcbh.ops):\n        super().__init__()\n        self.proj = operations.Linear(dim_in, dim_out * 2, dtype=dtype, device=device)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "zero_module",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\ndef Normalize(in_channels, dtype=None, device=None):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True, dtype=dtype, device=device)\ndef attention_basic(q, k, v, heads, mask=None):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def Normalize(in_channels, dtype=None, device=None):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True, dtype=dtype, device=device)\ndef attention_basic(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n    h = heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "attention_basic",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def attention_basic(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n    h = heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "attention_sub_quad",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def attention_sub_quad(query, key, value, heads, mask=None):\n    b, _, dim_head = query.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n    query = query.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 1, 3).reshape(b * heads, -1, dim_head)\n    value = value.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 1, 3).reshape(b * heads, -1, dim_head)\n    key = key.unsqueeze(3).reshape(b, -1, heads, dim_head).permute(0, 2, 3, 1).reshape(b * heads, dim_head, -1)\n    dtype = query.dtype\n    upcast_attention = _ATTN_PRECISION ==\"fp32\" and query.dtype != torch.float32\n    if upcast_attention:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "attention_split",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def attention_split(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    scale = dim_head ** -0.5\n    h = heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "attention_xformers",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def attention_xformers(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    q, k, v = map(\n        lambda t: t.unsqueeze(3)\n        .reshape(b, -1, heads, dim_head)\n        .permute(0, 2, 1, 3)\n        .reshape(b * heads, -1, dim_head)\n        .contiguous(),\n        (q, k, v),",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "attention_pytorch",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "def attention_pytorch(q, k, v, heads, mask=None):\n    b, _, dim_head = q.shape\n    dim_head //= heads\n    q, k, v = map(\n        lambda t: t.view(b, -1, heads, dim_head).transpose(1, 2),\n        (q, k, v),\n    )\n    out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)\n    out = (\n        out.transpose(1, 2).reshape(b, -1, heads * dim_head)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "optimized_attention",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "optimized_attention = attention_basic\noptimized_attention_masked = attention_basic\nif model_management.xformers_enabled():\n    print(\"Using xformers cross attention\")\n    optimized_attention = attention_xformers\nelif model_management.pytorch_attention_enabled():\n    print(\"Using pytorch cross attention\")\n    optimized_attention = attention_pytorch\nelse:\n    if args.use_split_cross_attention:",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "optimized_attention_masked",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "peekOfCode": "optimized_attention_masked = attention_basic\nif model_management.xformers_enabled():\n    print(\"Using xformers cross attention\")\n    optimized_attention = attention_xformers\nelif model_management.pytorch_attention_enabled():\n    print(\"Using pytorch cross attention\")\n    optimized_attention = attention_pytorch\nelse:\n    if args.use_split_cross_attention:\n        print(\"Using split optimization for cross attention\")",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.attention",
        "documentation": {}
    },
    {
        "label": "LitEma",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.ema",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.ema",
        "peekOfCode": "class LitEma(nn.Module):\n    def __init__(self, model, decay=0.9999, use_num_upates=True):\n        super().__init__()\n        if decay < 0.0 or decay > 1.0:\n            raise ValueError('Decay must be between 0 and 1')\n        self.m_name2s_name = {}\n        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))\n        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int) if use_num_upates\n        else torch.tensor(-1, dtype=torch.int))\n        for name, p in model.named_parameters():",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.ema",
        "documentation": {}
    },
    {
        "label": "AttnChunk",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "peekOfCode": "class AttnChunk(NamedTuple):\n    exp_values: Tensor\n    exp_weights_sum: Tensor\n    max_score: Tensor\nclass SummarizeChunk(Protocol):\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key_t: Tensor,\n        value: Tensor,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "documentation": {}
    },
    {
        "label": "SummarizeChunk",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "peekOfCode": "class SummarizeChunk(Protocol):\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key_t: Tensor,\n        value: Tensor,\n    ) -> AttnChunk: ...\nclass ComputeQueryChunkAttn(Protocol):\n    @staticmethod\n    def __call__(",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "documentation": {}
    },
    {
        "label": "ComputeQueryChunkAttn",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "peekOfCode": "class ComputeQueryChunkAttn(Protocol):\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key_t: Tensor,\n        value: Tensor,\n    ) -> Tensor: ...\ndef _summarize_chunk(\n    query: Tensor,\n    key_t: Tensor,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "documentation": {}
    },
    {
        "label": "ScannedChunk",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "peekOfCode": "class ScannedChunk(NamedTuple):\n    chunk_idx: int\n    attn_chunk: AttnChunk\ndef efficient_dot_product_attention(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    query_chunk_size=1024,\n    kv_chunk_size: Optional[int] = None,\n    kv_chunk_size_min: Optional[int] = None,",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "documentation": {}
    },
    {
        "label": "dynamic_slice",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "peekOfCode": "def dynamic_slice(\n    x: Tensor,\n    starts: List[int],\n    sizes: List[int],\n) -> Tensor:\n    slicing = [slice(start, start + size) for start, size in zip(starts, sizes)]\n    return x[slicing]\nclass AttnChunk(NamedTuple):\n    exp_values: Tensor\n    exp_weights_sum: Tensor",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "documentation": {}
    },
    {
        "label": "efficient_dot_product_attention",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "description": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "peekOfCode": "def efficient_dot_product_attention(\n    query: Tensor,\n    key_t: Tensor,\n    value: Tensor,\n    query_chunk_size=1024,\n    kv_chunk_size: Optional[int] = None,\n    kv_chunk_size_min: Optional[int] = None,\n    use_checkpoint=True,\n    upcast_attention=False,\n):",
        "detail": "Fooocus.backend.headless.fcbh.ldm.modules.sub_quadratic_attention",
        "documentation": {}
    },
    {
        "label": "AdamWwithEMAandWings",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "class AdamWwithEMAandWings(optim.Optimizer):\n    # credit to https://gist.github.com/crowsonkb/65f7265353f403714fce3b2595e0b298\n    def __init__(self, params, lr=1.e-3, betas=(0.9, 0.999), eps=1.e-8,  # TODO: check hyperparameters before using\n                 weight_decay=1.e-2, amsgrad=False, ema_decay=0.9999,   # ema decay to match previous code\n                 ema_power=1., param_names=()):\n        \"\"\"AdamW that saves EMA versions of the parameters.\"\"\"\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "log_txt_as_img",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def log_txt_as_img(wh, xc, size=10):\n    # wh a tuple of (width, height)\n    # xc a list of captions to plot\n    b = len(xc)\n    txts = list()\n    for bi in range(b):\n        txt = Image.new(\"RGB\", wh, color=\"white\")\n        draw = ImageDraw.Draw(txt)\n        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n        nc = int(40 * (wh[0] / 256))",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "ismap",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def ismap(x):\n    if not isinstance(x, torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] > 3)\ndef isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\ndef exists(x):\n    return x is not None",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "isimage",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\ndef exists(x):\n    return x is not None\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def exists(x):\n    return x is not None\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\ndef mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\ndef mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "mean_flat",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\ndef count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "count_params",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n    return total_params\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "instantiate_from_config",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":\n            return None\n        raise KeyError(\"Expected key `target` to instantiate.\")\n    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "get_obj_from_str",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ldm.util",
        "description": "Fooocus.backend.headless.fcbh.ldm.util",
        "peekOfCode": "def get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    if reload:\n        module_imp = importlib.import_module(module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)\nclass AdamWwithEMAandWings(optim.Optimizer):\n    # credit to https://gist.github.com/crowsonkb/65f7265353f403714fce3b2595e0b298\n    def __init__(self, params, lr=1.e-3, betas=(0.9, 0.999), eps=1.e-8,  # TODO: check hyperparameters before using\n                 weight_decay=1.e-2, amsgrad=False, ema_decay=0.9999,   # ema decay to match previous code",
        "detail": "Fooocus.backend.headless.fcbh.ldm.util",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "ResnetBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class ResnetBlock(nn.Module):\n    def __init__(self, in_c, out_c, down, ksize=3, sk=False, use_conv=True):\n        super().__init__()\n        ps = ksize // 2\n        if in_c != out_c or sk == False:\n            self.in_conv = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            # print('n_in')\n            self.in_conv = None\n        self.block1 = nn.Conv2d(out_c, out_c, 3, 1, 1)",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "Adapter",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class Adapter(nn.Module):\n    def __init__(self, channels=[320, 640, 1280, 1280], nums_rb=3, cin=64, ksize=3, sk=False, use_conv=True, xl=True):\n        super(Adapter, self).__init__()\n        self.unshuffle_amount = 8\n        resblock_no_downsample = []\n        resblock_downsample = [3, 2, 1]\n        self.xl = xl\n        if self.xl:\n            self.unshuffle_amount = 16\n            resblock_no_downsample = [1]",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "QuickGELU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            OrderedDict([(\"c_fc\", nn.Linear(d_model, d_model * 4)), (\"gelu\", QuickGELU()),",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            OrderedDict([(\"c_fc\", nn.Linear(d_model, d_model * 4)), (\"gelu\", QuickGELU()),\n                         (\"c_proj\", nn.Linear(d_model * 4, d_model))]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "StyleAdapter",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class StyleAdapter(nn.Module):\n    def __init__(self, width=1024, context_dim=768, num_head=8, n_layes=3, num_token=4):\n        super().__init__()\n        scale = width ** -0.5\n        self.transformer_layes = nn.Sequential(*[ResidualAttentionBlock(width, num_head) for _ in range(n_layes)])\n        self.num_token = num_token\n        self.style_embedding = nn.Parameter(torch.randn(1, num_token, width) * scale)\n        self.ln_post = LayerNorm(width)\n        self.ln_pre = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, context_dim))",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "ResnetBlock_light",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class ResnetBlock_light(nn.Module):\n    def __init__(self, in_c):\n        super().__init__()\n        self.block1 = nn.Conv2d(in_c, in_c, 3, 1, 1)\n        self.act = nn.ReLU()\n        self.block2 = nn.Conv2d(in_c, in_c, 3, 1, 1)\n    def forward(self, x):\n        h = self.block1(x)\n        h = self.act(h)\n        h = self.block2(h)",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "extractor",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class extractor(nn.Module):\n    def __init__(self, in_c, inter_c, out_c, nums_rb, down=False):\n        super().__init__()\n        self.in_conv = nn.Conv2d(in_c, inter_c, 1, 1, 0)\n        self.body = []\n        for _ in range(nums_rb):\n            self.body.append(ResnetBlock_light(inter_c))\n        self.body = nn.Sequential(*self.body)\n        self.out_conv = nn.Conv2d(inter_c, out_c, 1, 1, 0)\n        self.down = down",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "Adapter_light",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "class Adapter_light(nn.Module):\n    def __init__(self, channels=[320, 640, 1280, 1280], nums_rb=3, cin=64):\n        super(Adapter_light, self).__init__()\n        self.unshuffle_amount = 8\n        self.unshuffle = nn.PixelUnshuffle(self.unshuffle_amount)\n        self.input_channels = cin // (self.unshuffle_amount * self.unshuffle_amount)\n        self.channels = channels\n        self.nums_rb = nums_rb\n        self.body = []\n        self.xl = False",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "conv_nd",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "def conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "avg_pool_nd",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "description": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "peekOfCode": "def avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)",
        "detail": "Fooocus.backend.headless.fcbh.t2i_adapter.adapter",
        "documentation": {}
    },
    {
        "label": "Clamp",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "description": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "peekOfCode": "class Clamp(nn.Module):\n    def forward(self, x):\n        return torch.tanh(x / 3) * 3\nclass Block(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.conv = nn.Sequential(conv(n_in, n_out), nn.ReLU(), conv(n_out, n_out), nn.ReLU(), conv(n_out, n_out))\n        self.skip = nn.Conv2d(n_in, n_out, 1, bias=False) if n_in != n_out else nn.Identity()\n        self.fuse = nn.ReLU()\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "description": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.conv = nn.Sequential(conv(n_in, n_out), nn.ReLU(), conv(n_out, n_out), nn.ReLU(), conv(n_out, n_out))\n        self.skip = nn.Conv2d(n_in, n_out, 1, bias=False) if n_in != n_out else nn.Identity()\n        self.fuse = nn.ReLU()\n    def forward(self, x):\n        return self.fuse(self.conv(x) + self.skip(x))\ndef Encoder():\n    return nn.Sequential(",
        "detail": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "TAESD",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "description": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "peekOfCode": "class TAESD(nn.Module):\n    latent_magnitude = 3\n    latent_shift = 0.5\n    def __init__(self, encoder_path=None, decoder_path=None):\n        \"\"\"Initialize pretrained TAESD on the given device from the given checkpoints.\"\"\"\n        super().__init__()\n        self.taesd_encoder = Encoder()\n        self.taesd_decoder = Decoder()\n        self.vae_scale = torch.nn.Parameter(torch.tensor(1.0))\n        if encoder_path is not None:",
        "detail": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "conv",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "description": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "peekOfCode": "def conv(n_in, n_out, **kwargs):\n    return nn.Conv2d(n_in, n_out, 3, padding=1, **kwargs)\nclass Clamp(nn.Module):\n    def forward(self, x):\n        return torch.tanh(x / 3) * 3\nclass Block(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.conv = nn.Sequential(conv(n_in, n_out), nn.ReLU(), conv(n_out, n_out), nn.ReLU(), conv(n_out, n_out))\n        self.skip = nn.Conv2d(n_in, n_out, 1, bias=False) if n_in != n_out else nn.Identity()",
        "detail": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "description": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "peekOfCode": "def Encoder():\n    return nn.Sequential(\n        conv(3, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 4),\n    )\ndef Decoder():\n    return nn.Sequential(",
        "detail": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "description": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "peekOfCode": "def Decoder():\n    return nn.Sequential(\n        Clamp(), conv(4, 64), nn.ReLU(),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), conv(64, 3),\n    )\nclass TAESD(nn.Module):\n    latent_magnitude = 3",
        "detail": "Fooocus.backend.headless.fcbh.taesd.taesd",
        "documentation": {}
    },
    {
        "label": "Empty",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "description": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "peekOfCode": "class Empty:\n    pass\nclass Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        #TODO: safe unpickle\n        if module.startswith(\"pytorch_lightning\"):\n            return Empty\n        return super().find_class(module, name)",
        "detail": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "documentation": {}
    },
    {
        "label": "Unpickler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "description": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "peekOfCode": "class Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        #TODO: safe unpickle\n        if module.startswith(\"pytorch_lightning\"):\n            return Empty\n        return super().find_class(module, name)",
        "detail": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "description": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "peekOfCode": "load = pickle.load\nclass Empty:\n    pass\nclass Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        #TODO: safe unpickle\n        if module.startswith(\"pytorch_lightning\"):\n            return Empty\n        return super().find_class(module, name)",
        "detail": "Fooocus.backend.headless.fcbh.checkpoint_pickle",
        "documentation": {}
    },
    {
        "label": "EnumAction",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "class EnumAction(argparse.Action):\n    \"\"\"\n    Argparse action for handling Enums\n    \"\"\"\n    def __init__(self, **kwargs):\n        # Pop off the type value\n        enum_type = kwargs.pop(\"type\", None)\n        # Ensure an Enum subclass is provided\n        if enum_type is None:\n            raise ValueError(\"type must be assigned an Enum when using EnumAction\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "LatentPreviewMethod",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "class LatentPreviewMethod(enum.Enum):\n    NoPreviews = \"none\"\n    Auto = \"auto\"\n    Latent2RGB = \"latent2rgb\"\n    TAESD = \"taesd\"\nparser.add_argument(\"--preview-method\", type=LatentPreviewMethod, default=LatentPreviewMethod.NoPreviews, help=\"Default preview method for sampler nodes.\", action=EnumAction)\nattn_group = parser.add_mutually_exclusive_group()\nattn_group.add_argument(\"--use-split-cross-attention\", action=\"store_true\", help=\"Use the split cross attention optimization. Ignored when xformers is used.\")\nattn_group.add_argument(\"--use-quad-cross-attention\", action=\"store_true\", help=\"Use the sub-quadratic cross attention optimization . Ignored when xformers is used.\")\nattn_group.add_argument(\"--use-pytorch-cross-attention\", action=\"store_true\", help=\"Use the new pytorch 2.0 cross attention function.\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"--listen\", type=str, default=\"127.0.0.1\", metavar=\"IP\", nargs=\"?\", const=\"0.0.0.0\", help=\"Specify the IP address to listen on (default: 127.0.0.1). If --listen is provided without an argument, it defaults to 0.0.0.0. (listens on all)\")\nparser.add_argument(\"--port\", type=int, default=8188, help=\"Set the listen port.\")\nparser.add_argument(\"--enable-cors-header\", type=str, default=None, metavar=\"ORIGIN\", nargs=\"?\", const=\"*\", help=\"Enable CORS (Cross-Origin Resource Sharing) with optional origin or allow all with default '*'.\")\nparser.add_argument(\"--max-upload-size\", type=float, default=100, help=\"Set the maximum upload size in MB.\")\nparser.add_argument(\"--extra-model-paths-config\", type=str, default=None, metavar=\"PATH\", nargs='+', action='append', help=\"Load one or more extra_model_paths.yaml files.\")\nparser.add_argument(\"--output-directory\", type=str, default=None, help=\"Set the fcbh_backend output directory.\")\nparser.add_argument(\"--temp-directory\", type=str, default=None, help=\"Set the fcbh_backend temp directory (default is in the fcbh_backend directory).\")\nparser.add_argument(\"--input-directory\", type=str, default=None, help=\"Set the fcbh_backend input directory.\")\nparser.add_argument(\"--auto-launch\", action=\"store_true\", help=\"Automatically launch fcbh_backend in the default browser.\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "cm_group",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "cm_group = parser.add_mutually_exclusive_group()\ncm_group.add_argument(\"--cuda-malloc\", action=\"store_true\", help=\"Enable cudaMallocAsync (enabled by default for torch 2.0 and up).\")\ncm_group.add_argument(\"--disable-cuda-malloc\", action=\"store_true\", help=\"Disable cudaMallocAsync.\")\nparser.add_argument(\"--dont-upcast-attention\", action=\"store_true\", help=\"Disable upcasting of attention. Can boost speed but increase the chances of black images.\")\nfp_group = parser.add_mutually_exclusive_group()\nfp_group.add_argument(\"--force-fp32\", action=\"store_true\", help=\"Force fp32 (If this makes your GPU work better please report it).\")\nfp_group.add_argument(\"--force-fp16\", action=\"store_true\", help=\"Force fp16.\")\nparser.add_argument(\"--bf16-unet\", action=\"store_true\", help=\"Run the UNET in bf16. This should only be used for testing stuff.\")\nfpvae_group = parser.add_mutually_exclusive_group()\nfpvae_group.add_argument(\"--fp16-vae\", action=\"store_true\", help=\"Run the VAE in fp16, might cause black images.\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "fp_group",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "fp_group = parser.add_mutually_exclusive_group()\nfp_group.add_argument(\"--force-fp32\", action=\"store_true\", help=\"Force fp32 (If this makes your GPU work better please report it).\")\nfp_group.add_argument(\"--force-fp16\", action=\"store_true\", help=\"Force fp16.\")\nparser.add_argument(\"--bf16-unet\", action=\"store_true\", help=\"Run the UNET in bf16. This should only be used for testing stuff.\")\nfpvae_group = parser.add_mutually_exclusive_group()\nfpvae_group.add_argument(\"--fp16-vae\", action=\"store_true\", help=\"Run the VAE in fp16, might cause black images.\")\nfpvae_group.add_argument(\"--fp32-vae\", action=\"store_true\", help=\"Run the VAE in full precision fp32.\")\nfpvae_group.add_argument(\"--bf16-vae\", action=\"store_true\", help=\"Run the VAE in bf16.\")\nfpte_group = parser.add_mutually_exclusive_group()\nfpte_group.add_argument(\"--fp8_e4m3fn-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp8 (e4m3fn variant).\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "fpvae_group",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "fpvae_group = parser.add_mutually_exclusive_group()\nfpvae_group.add_argument(\"--fp16-vae\", action=\"store_true\", help=\"Run the VAE in fp16, might cause black images.\")\nfpvae_group.add_argument(\"--fp32-vae\", action=\"store_true\", help=\"Run the VAE in full precision fp32.\")\nfpvae_group.add_argument(\"--bf16-vae\", action=\"store_true\", help=\"Run the VAE in bf16.\")\nfpte_group = parser.add_mutually_exclusive_group()\nfpte_group.add_argument(\"--fp8_e4m3fn-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp8 (e4m3fn variant).\")\nfpte_group.add_argument(\"--fp8_e5m2-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp8 (e5m2 variant).\")\nfpte_group.add_argument(\"--fp16-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp16.\")\nfpte_group.add_argument(\"--fp32-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp32.\")\nparser.add_argument(\"--directml\", type=int, nargs=\"?\", metavar=\"DIRECTML_DEVICE\", const=-1, help=\"Use torch-directml.\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "fpte_group",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "fpte_group = parser.add_mutually_exclusive_group()\nfpte_group.add_argument(\"--fp8_e4m3fn-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp8 (e4m3fn variant).\")\nfpte_group.add_argument(\"--fp8_e5m2-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp8 (e5m2 variant).\")\nfpte_group.add_argument(\"--fp16-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp16.\")\nfpte_group.add_argument(\"--fp32-text-enc\", action=\"store_true\", help=\"Store text encoder weights in fp32.\")\nparser.add_argument(\"--directml\", type=int, nargs=\"?\", metavar=\"DIRECTML_DEVICE\", const=-1, help=\"Use torch-directml.\")\nparser.add_argument(\"--disable-ipex-optimize\", action=\"store_true\", help=\"Disables ipex.optimize when loading models with Intel GPUs.\")\nclass LatentPreviewMethod(enum.Enum):\n    NoPreviews = \"none\"\n    Auto = \"auto\"",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "attn_group",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "attn_group = parser.add_mutually_exclusive_group()\nattn_group.add_argument(\"--use-split-cross-attention\", action=\"store_true\", help=\"Use the split cross attention optimization. Ignored when xformers is used.\")\nattn_group.add_argument(\"--use-quad-cross-attention\", action=\"store_true\", help=\"Use the sub-quadratic cross attention optimization . Ignored when xformers is used.\")\nattn_group.add_argument(\"--use-pytorch-cross-attention\", action=\"store_true\", help=\"Use the new pytorch 2.0 cross attention function.\")\nparser.add_argument(\"--disable-xformers\", action=\"store_true\", help=\"Disable xformers.\")\nvram_group = parser.add_mutually_exclusive_group()\nvram_group.add_argument(\"--gpu-only\", action=\"store_true\", help=\"Store and run everything (text encoders/CLIP models, etc... on the GPU).\")\nvram_group.add_argument(\"--highvram\", action=\"store_true\", help=\"By default models will be unloaded to CPU memory after being used. This option keeps them in GPU memory.\")\nvram_group.add_argument(\"--normalvram\", action=\"store_true\", help=\"Used to force normal vram use if lowvram gets automatically enabled.\")\nvram_group.add_argument(\"--lowvram\", action=\"store_true\", help=\"Split the unet in parts to use less vram.\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "vram_group",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.cli_args",
        "description": "Fooocus.backend.headless.fcbh.cli_args",
        "peekOfCode": "vram_group = parser.add_mutually_exclusive_group()\nvram_group.add_argument(\"--gpu-only\", action=\"store_true\", help=\"Store and run everything (text encoders/CLIP models, etc... on the GPU).\")\nvram_group.add_argument(\"--highvram\", action=\"store_true\", help=\"By default models will be unloaded to CPU memory after being used. This option keeps them in GPU memory.\")\nvram_group.add_argument(\"--normalvram\", action=\"store_true\", help=\"Used to force normal vram use if lowvram gets automatically enabled.\")\nvram_group.add_argument(\"--lowvram\", action=\"store_true\", help=\"Split the unet in parts to use less vram.\")\nvram_group.add_argument(\"--novram\", action=\"store_true\", help=\"When lowvram isn't enough.\")\nvram_group.add_argument(\"--cpu\", action=\"store_true\", help=\"To use the CPU for everything (slow).\")\nparser.add_argument(\"--disable-smart-memory\", action=\"store_true\", help=\"Force fcbh_backend to agressively offload to regular ram instead of keeping models in vram when it can.\")\nparser.add_argument(\"--dont-print-server\", action=\"store_true\", help=\"Don't print server output.\")\nparser.add_argument(\"--quick-test-for-ci\", action=\"store_true\", help=\"Quick test for CI.\")",
        "detail": "Fooocus.backend.headless.fcbh.cli_args",
        "documentation": {}
    },
    {
        "label": "ClipVisionModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.clip_vision",
        "description": "Fooocus.backend.headless.fcbh.clip_vision",
        "peekOfCode": "class ClipVisionModel():\n    def __init__(self, json_config):\n        config = CLIPVisionConfig.from_json_file(json_config)\n        self.load_device = fcbh.model_management.text_encoder_device()\n        offload_device = fcbh.model_management.text_encoder_offload_device()\n        self.dtype = torch.float32\n        if fcbh.model_management.should_use_fp16(self.load_device, prioritize_performance=False):\n            self.dtype = torch.float16\n        with fcbh.ops.use_fcbh_ops(offload_device, self.dtype):\n            with modeling_utils.no_init_weights():",
        "detail": "Fooocus.backend.headless.fcbh.clip_vision",
        "documentation": {}
    },
    {
        "label": "clip_preprocess",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.clip_vision",
        "description": "Fooocus.backend.headless.fcbh.clip_vision",
        "peekOfCode": "def clip_preprocess(image, size=224):\n    mean = torch.tensor([ 0.48145466,0.4578275,0.40821073], device=image.device, dtype=image.dtype)\n    std = torch.tensor([0.26862954,0.26130258,0.27577711], device=image.device, dtype=image.dtype)\n    scale = (size / min(image.shape[1], image.shape[2]))\n    image = torch.nn.functional.interpolate(image.movedim(-1, 1), size=(round(scale * image.shape[1]), round(scale * image.shape[2])), mode=\"bicubic\", antialias=True)\n    h = (image.shape[2] - size)//2\n    w = (image.shape[3] - size)//2\n    image = image[:,:,h:h+size,w:w+size]\n    image = torch.clip((255. * image), 0, 255).round() / 255.0\n    return (image - mean.view([3,1,1])) / std.view([3,1,1])",
        "detail": "Fooocus.backend.headless.fcbh.clip_vision",
        "documentation": {}
    },
    {
        "label": "convert_to_transformers",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.clip_vision",
        "description": "Fooocus.backend.headless.fcbh.clip_vision",
        "peekOfCode": "def convert_to_transformers(sd, prefix):\n    sd_k = sd.keys()\n    if \"{}transformer.resblocks.0.attn.in_proj_weight\".format(prefix) in sd_k:\n        keys_to_replace = {\n            \"{}class_embedding\".format(prefix): \"vision_model.embeddings.class_embedding\",\n            \"{}conv1.weight\".format(prefix): \"vision_model.embeddings.patch_embedding.weight\",\n            \"{}positional_embedding\".format(prefix): \"vision_model.embeddings.position_embedding.weight\",\n            \"{}ln_post.bias\".format(prefix): \"vision_model.post_layernorm.bias\",\n            \"{}ln_post.weight\".format(prefix): \"vision_model.post_layernorm.weight\",\n            \"{}ln_pre.bias\".format(prefix): \"vision_model.pre_layrnorm.bias\",",
        "detail": "Fooocus.backend.headless.fcbh.clip_vision",
        "documentation": {}
    },
    {
        "label": "load_clipvision_from_sd",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.clip_vision",
        "description": "Fooocus.backend.headless.fcbh.clip_vision",
        "peekOfCode": "def load_clipvision_from_sd(sd, prefix=\"\", convert_keys=False):\n    if convert_keys:\n        sd = convert_to_transformers(sd, prefix)\n    if \"vision_model.encoder.layers.47.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_g.json\")\n    elif \"vision_model.encoder.layers.30.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_h.json\")\n    elif \"vision_model.encoder.layers.22.layer_norm1.weight\" in sd:\n        json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_vision_config_vitl.json\")\n    else:",
        "detail": "Fooocus.backend.headless.fcbh.clip_vision",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.clip_vision",
        "description": "Fooocus.backend.headless.fcbh.clip_vision",
        "peekOfCode": "def load(ckpt_path):\n    sd = load_torch_file(ckpt_path)\n    if \"visual.transformer.resblocks.0.attn.in_proj_weight\" in sd:\n        return load_clipvision_from_sd(sd, prefix=\"visual.\", convert_keys=True)\n    else:\n        return load_clipvision_from_sd(sd)",
        "detail": "Fooocus.backend.headless.fcbh.clip_vision",
        "documentation": {}
    },
    {
        "label": "CONDRegular",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.conds",
        "description": "Fooocus.backend.headless.fcbh.conds",
        "peekOfCode": "class CONDRegular:\n    def __init__(self, cond):\n        self.cond = cond\n    def _copy_with(self, cond):\n        return self.__class__(cond)\n    def process_cond(self, batch_size, device, **kwargs):\n        return self._copy_with(fcbh.utils.repeat_to_batch_size(self.cond, batch_size).to(device))\n    def can_concat(self, other):\n        if self.cond.shape != other.cond.shape:\n            return False",
        "detail": "Fooocus.backend.headless.fcbh.conds",
        "documentation": {}
    },
    {
        "label": "CONDNoiseShape",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.conds",
        "description": "Fooocus.backend.headless.fcbh.conds",
        "peekOfCode": "class CONDNoiseShape(CONDRegular):\n    def process_cond(self, batch_size, device, area, **kwargs):\n        data = self.cond[:,:,area[2]:area[0] + area[2],area[3]:area[1] + area[3]]\n        return self._copy_with(fcbh.utils.repeat_to_batch_size(data, batch_size).to(device))\nclass CONDCrossAttn(CONDRegular):\n    def can_concat(self, other):\n        s1 = self.cond.shape\n        s2 = other.cond.shape\n        if s1 != s2:\n            if s1[0] != s2[0] or s1[2] != s2[2]: #these 2 cases should not happen",
        "detail": "Fooocus.backend.headless.fcbh.conds",
        "documentation": {}
    },
    {
        "label": "CONDCrossAttn",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.conds",
        "description": "Fooocus.backend.headless.fcbh.conds",
        "peekOfCode": "class CONDCrossAttn(CONDRegular):\n    def can_concat(self, other):\n        s1 = self.cond.shape\n        s2 = other.cond.shape\n        if s1 != s2:\n            if s1[0] != s2[0] or s1[2] != s2[2]: #these 2 cases should not happen\n                return False\n            mult_min = lcm(s1[1], s2[1])\n            diff = mult_min // min(s1[1], s2[1])\n            if diff > 4: #arbitrary limit on the padding because it's probably going to impact performance negatively if it's too much",
        "detail": "Fooocus.backend.headless.fcbh.conds",
        "documentation": {}
    },
    {
        "label": "CONDConstant",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.conds",
        "description": "Fooocus.backend.headless.fcbh.conds",
        "peekOfCode": "class CONDConstant(CONDRegular):\n    def __init__(self, cond):\n        self.cond = cond\n    def process_cond(self, batch_size, device, **kwargs):\n        return self._copy_with(self.cond)\n    def can_concat(self, other):\n        if self.cond != other.cond:\n            return False\n        return True\n    def concat(self, others):",
        "detail": "Fooocus.backend.headless.fcbh.conds",
        "documentation": {}
    },
    {
        "label": "lcm",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.conds",
        "description": "Fooocus.backend.headless.fcbh.conds",
        "peekOfCode": "def lcm(a, b): #TODO: eventually replace by math.lcm (added in python3.9)\n    return abs(a*b) // math.gcd(a, b)\nclass CONDRegular:\n    def __init__(self, cond):\n        self.cond = cond\n    def _copy_with(self, cond):\n        return self.__class__(cond)\n    def process_cond(self, batch_size, device, **kwargs):\n        return self._copy_with(fcbh.utils.repeat_to_batch_size(self.cond, batch_size).to(device))\n    def can_concat(self, other):",
        "detail": "Fooocus.backend.headless.fcbh.conds",
        "documentation": {}
    },
    {
        "label": "ControlBase",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "class ControlBase:\n    def __init__(self, device=None):\n        self.cond_hint_original = None\n        self.cond_hint = None\n        self.strength = 1.0\n        self.timestep_percent_range = (0.0, 1.0)\n        self.timestep_range = None\n        if device is None:\n            device = fcbh.model_management.get_torch_device()\n        self.device = device",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "ControlNet",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "class ControlNet(ControlBase):\n    def __init__(self, control_model, global_average_pooling=False, device=None):\n        super().__init__(device)\n        self.control_model = control_model\n        self.control_model_wrapped = fcbh.model_patcher.ModelPatcher(self.control_model, load_device=fcbh.model_management.get_torch_device(), offload_device=fcbh.model_management.unet_offload_device())\n        self.global_average_pooling = global_average_pooling\n        self.model_sampling_current = None\n    def get_control(self, x_noisy, t, cond, batched_number):\n        control_prev = None\n        if self.previous_controlnet is not None:",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "ControlLoraOps",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "class ControlLoraOps:\n    class Linear(torch.nn.Module):\n        def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                    device=None, dtype=None) -> None:\n            factory_kwargs = {'device': device, 'dtype': dtype}\n            super().__init__()\n            self.in_features = in_features\n            self.out_features = out_features\n            self.weight = None\n            self.up = None",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "ControlLora",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "class ControlLora(ControlNet):\n    def __init__(self, control_weights, global_average_pooling=False, device=None):\n        ControlBase.__init__(self, device)\n        self.control_weights = control_weights\n        self.global_average_pooling = global_average_pooling\n    def pre_run(self, model, percent_to_timestep_function):\n        super().pre_run(model, percent_to_timestep_function)\n        controlnet_config = model.model_config.unet_config.copy()\n        controlnet_config.pop(\"out_channels\")\n        controlnet_config[\"hint_channels\"] = self.control_weights[\"input_hint_block.0.weight\"].shape[1]",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "T2IAdapter",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "class T2IAdapter(ControlBase):\n    def __init__(self, t2i_model, channels_in, device=None):\n        super().__init__(device)\n        self.t2i_model = t2i_model\n        self.channels_in = channels_in\n        self.control_input = None\n    def scale_image_to(self, width, height):\n        unshuffle_amount = self.t2i_model.unshuffle_amount\n        width = math.ceil(width / unshuffle_amount) * unshuffle_amount\n        height = math.ceil(height / unshuffle_amount) * unshuffle_amount",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "broadcast_image_to",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "def broadcast_image_to(tensor, target_batch_size, batched_number):\n    current_batch_size = tensor.shape[0]\n    #print(current_batch_size, target_batch_size)\n    if current_batch_size == 1:\n        return tensor\n    per_batch = target_batch_size // batched_number\n    tensor = tensor[:per_batch]\n    if per_batch > tensor.shape[0]:\n        tensor = torch.cat([tensor] * (per_batch // tensor.shape[0]) + [tensor[:(per_batch % tensor.shape[0])]], dim=0)\n    current_batch_size = tensor.shape[0]",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "load_controlnet",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "def load_controlnet(ckpt_path, model=None):\n    controlnet_data = fcbh.utils.load_torch_file(ckpt_path, safe_load=True)\n    if \"lora_controlnet\" in controlnet_data:\n        return ControlLora(controlnet_data)\n    controlnet_config = None\n    if \"controlnet_cond_embedding.conv_in.weight\" in controlnet_data: #diffusers format\n        unet_dtype = fcbh.model_management.unet_dtype()\n        controlnet_config = fcbh.model_detection.unet_config_from_diffusers_unet(controlnet_data, unet_dtype)\n        diffusers_keys = fcbh.utils.unet_to_diffusers(controlnet_config)\n        diffusers_keys[\"controlnet_mid_block.weight\"] = \"middle_block_out.0.weight\"",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "load_t2i_adapter",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.controlnet",
        "description": "Fooocus.backend.headless.fcbh.controlnet",
        "peekOfCode": "def load_t2i_adapter(t2i_data):\n    if 'adapter' in t2i_data:\n        t2i_data = t2i_data['adapter']\n    if 'adapter.body.0.resnets.0.block1.weight' in t2i_data: #diffusers format\n        prefix_replace = {}\n        for i in range(4):\n            for j in range(2):\n                prefix_replace[\"adapter.body.{}.resnets.{}.\".format(i, j)] = \"body.{}.\".format(i * 2 + j)\n            prefix_replace[\"adapter.body.{}.\".format(i, j)] = \"body.{}.\".format(i * 2)\n        prefix_replace[\"adapter.\"] = \"\"",
        "detail": "Fooocus.backend.headless.fcbh.controlnet",
        "documentation": {}
    },
    {
        "label": "convert_unet_state_dict",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "def convert_unet_state_dict(unet_state_dict):\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in\n    # the exact order in which I have arranged them.\n    mapping = {k: k for k in unet_state_dict.keys()}\n    for sd_name, hf_name in unet_conversion_map:\n        mapping[hf_name] = sd_name\n    for k, v in mapping.items():\n        if \"resnets\" in k:\n            for sd_part, hf_part in unet_conversion_map_resnet:",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "reshape_weight_for_sd",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "def reshape_weight_for_sd(w):\n    # convert HF linear weights to SD conv2d weights\n    return w.reshape(*w.shape, 1, 1)\ndef convert_vae_state_dict(vae_state_dict):\n    mapping = {k: k for k in vae_state_dict.keys()}\n    for k, v in mapping.items():\n        for sd_part, hf_part in vae_conversion_map:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    for k, v in mapping.items():",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "convert_vae_state_dict",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "def convert_vae_state_dict(vae_state_dict):\n    mapping = {k: k for k in vae_state_dict.keys()}\n    for k, v in mapping.items():\n        for sd_part, hf_part in vae_conversion_map:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    for k, v in mapping.items():\n        if \"attentions\" in k:\n            for sd_part, hf_part in vae_conversion_map_attn:\n                v = v.replace(hf_part, sd_part)",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "convert_text_enc_state_dict_v20",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "def convert_text_enc_state_dict_v20(text_enc_dict, prefix=\"\"):\n    new_state_dict = {}\n    capture_qkv_weight = {}\n    capture_qkv_bias = {}\n    for k, v in text_enc_dict.items():\n        if not k.startswith(prefix):\n            continue\n        if (\n                k.endswith(\".self_attn.q_proj.weight\")\n                or k.endswith(\".self_attn.k_proj.weight\")",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "convert_text_enc_state_dict",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "def convert_text_enc_state_dict(text_enc_dict):\n    return text_enc_dict",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "unet_conversion_map",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "unet_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n    (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n    (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n    (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\"),\n    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n    (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n    (\"out.0.weight\", \"conv_norm_out.weight\"),\n    (\"out.0.bias\", \"conv_norm_out.bias\"),",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "unet_conversion_map_resnet",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "unet_conversion_map_resnet = [\n    # (stable-diffusion, HF Diffusers)\n    (\"in_layers.0\", \"norm1\"),\n    (\"in_layers.2\", \"conv1\"),\n    (\"out_layers.0\", \"norm2\"),\n    (\"out_layers.3\", \"conv2\"),\n    (\"emb_layers.1\", \"time_emb_proj\"),\n    (\"skip_connection\", \"conv_shortcut\"),\n]\nunet_conversion_map_layer = []",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "unet_conversion_map_layer",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "unet_conversion_map_layer = []\n# hardcoded number of downblocks and resnets/attentions...\n# would need smarter logic for other networks.\nfor i in range(4):\n    # loop over downblocks/upblocks\n    for j in range(2):\n        # loop over resnets/attentions for downblocks\n        hf_down_res_prefix = f\"down_blocks.{i}.resnets.{j}.\"\n        sd_down_res_prefix = f\"input_blocks.{3 * i + j + 1}.0.\"\n        unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "hf_mid_atn_prefix",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "hf_mid_atn_prefix = \"mid_block.attentions.0.\"\nsd_mid_atn_prefix = \"middle_block.1.\"\nunet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))\nfor j in range(2):\n    hf_mid_res_prefix = f\"mid_block.resnets.{j}.\"\n    sd_mid_res_prefix = f\"middle_block.{2 * j}.\"\n    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))\ndef convert_unet_state_dict(unet_state_dict):\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "sd_mid_atn_prefix",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "sd_mid_atn_prefix = \"middle_block.1.\"\nunet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))\nfor j in range(2):\n    hf_mid_res_prefix = f\"mid_block.resnets.{j}.\"\n    sd_mid_res_prefix = f\"middle_block.{2 * j}.\"\n    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))\ndef convert_unet_state_dict(unet_state_dict):\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in\n    # the exact order in which I have arranged them.",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "vae_conversion_map",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "vae_conversion_map = [\n    # (stable-diffusion, HF Diffusers)\n    (\"nin_shortcut\", \"conv_shortcut\"),\n    (\"norm_out\", \"conv_norm_out\"),\n    (\"mid.attn_1.\", \"mid_block.attentions.0.\"),\n]\nfor i in range(4):\n    # down_blocks have two resnets\n    for j in range(2):\n        hf_down_prefix = f\"encoder.down_blocks.{i}.resnets.{j}.\"",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "vae_conversion_map_attn",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "vae_conversion_map_attn = [\n    # (stable-diffusion, HF Diffusers)\n    (\"norm.\", \"group_norm.\"),\n    (\"q.\", \"query.\"),\n    (\"k.\", \"key.\"),\n    (\"v.\", \"value.\"),\n    (\"q.\", \"to_q.\"),\n    (\"k.\", \"to_k.\"),\n    (\"v.\", \"to_v.\"),\n    (\"proj_out.\", \"to_out.0.\"),",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "textenc_conversion_lst",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "textenc_conversion_lst = [\n    # (stable-diffusion, HF Diffusers)\n    (\"resblocks.\", \"text_model.encoder.layers.\"),\n    (\"ln_1\", \"layer_norm1\"),\n    (\"ln_2\", \"layer_norm2\"),\n    (\".c_fc.\", \".fc1.\"),\n    (\".c_proj.\", \".fc2.\"),\n    (\".attn\", \".self_attn\"),\n    (\"ln_final.\", \"transformer.text_model.final_layer_norm.\"),\n    (\"token_embedding.weight\", \"transformer.text_model.embeddings.token_embedding.weight\"),",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "protected",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "protected = {re.escape(x[1]): x[0] for x in textenc_conversion_lst}\ntextenc_pattern = re.compile(\"|\".join(protected.keys()))\n# Ordering is from https://github.com/pytorch/pytorch/blob/master/test/cpp/api/modules.cpp\ncode2idx = {\"q\": 0, \"k\": 1, \"v\": 2}\ndef convert_text_enc_state_dict_v20(text_enc_dict, prefix=\"\"):\n    new_state_dict = {}\n    capture_qkv_weight = {}\n    capture_qkv_bias = {}\n    for k, v in text_enc_dict.items():\n        if not k.startswith(prefix):",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "textenc_pattern",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "textenc_pattern = re.compile(\"|\".join(protected.keys()))\n# Ordering is from https://github.com/pytorch/pytorch/blob/master/test/cpp/api/modules.cpp\ncode2idx = {\"q\": 0, \"k\": 1, \"v\": 2}\ndef convert_text_enc_state_dict_v20(text_enc_dict, prefix=\"\"):\n    new_state_dict = {}\n    capture_qkv_weight = {}\n    capture_qkv_bias = {}\n    for k, v in text_enc_dict.items():\n        if not k.startswith(prefix):\n            continue",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "code2idx",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "description": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "peekOfCode": "code2idx = {\"q\": 0, \"k\": 1, \"v\": 2}\ndef convert_text_enc_state_dict_v20(text_enc_dict, prefix=\"\"):\n    new_state_dict = {}\n    capture_qkv_weight = {}\n    capture_qkv_bias = {}\n    for k, v in text_enc_dict.items():\n        if not k.startswith(prefix):\n            continue\n        if (\n                k.endswith(\".self_attn.q_proj.weight\")",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_convert",
        "documentation": {}
    },
    {
        "label": "first_file",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_load",
        "description": "Fooocus.backend.headless.fcbh.diffusers_load",
        "peekOfCode": "def first_file(path, filenames):\n    for f in filenames:\n        p = os.path.join(path, f)\n        if os.path.exists(p):\n            return p\n    return None\ndef load_diffusers(model_path, output_vae=True, output_clip=True, embedding_directory=None):\n    diffusion_model_names = [\"diffusion_pytorch_model.fp16.safetensors\", \"diffusion_pytorch_model.safetensors\", \"diffusion_pytorch_model.fp16.bin\", \"diffusion_pytorch_model.bin\"]\n    unet_path = first_file(os.path.join(model_path, \"unet\"), diffusion_model_names)\n    vae_path = first_file(os.path.join(model_path, \"vae\"), diffusion_model_names)",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_load",
        "documentation": {}
    },
    {
        "label": "load_diffusers",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.diffusers_load",
        "description": "Fooocus.backend.headless.fcbh.diffusers_load",
        "peekOfCode": "def load_diffusers(model_path, output_vae=True, output_clip=True, embedding_directory=None):\n    diffusion_model_names = [\"diffusion_pytorch_model.fp16.safetensors\", \"diffusion_pytorch_model.safetensors\", \"diffusion_pytorch_model.fp16.bin\", \"diffusion_pytorch_model.bin\"]\n    unet_path = first_file(os.path.join(model_path, \"unet\"), diffusion_model_names)\n    vae_path = first_file(os.path.join(model_path, \"vae\"), diffusion_model_names)\n    text_encoder_model_names = [\"model.fp16.safetensors\", \"model.safetensors\", \"pytorch_model.fp16.bin\", \"pytorch_model.bin\"]\n    text_encoder1_path = first_file(os.path.join(model_path, \"text_encoder\"), text_encoder_model_names)\n    text_encoder2_path = first_file(os.path.join(model_path, \"text_encoder_2\"), text_encoder_model_names)\n    text_encoder_paths = [text_encoder1_path]\n    if text_encoder2_path is not None:\n        text_encoder_paths.append(text_encoder2_path)",
        "detail": "Fooocus.backend.headless.fcbh.diffusers_load",
        "documentation": {}
    },
    {
        "label": "GEGLU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * torch.nn.functional.gelu(gate)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim)\n        self.net = nn.Sequential(",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "GatedCrossAttentionDense",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class GatedCrossAttentionDense(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n        self.attn = CrossAttention(\n            query_dim=query_dim,\n            context_dim=context_dim,\n            heads=n_heads,\n            dim_head=d_head)\n        self.ff = FeedForward(query_dim, glu=True)\n        self.norm1 = nn.LayerNorm(query_dim)",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "GatedSelfAttentionDense",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class GatedSelfAttentionDense(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n        # we need a linear projection since we need cat visual feature and obj\n        # feature\n        self.linear = nn.Linear(context_dim, query_dim)\n        self.attn = CrossAttention(\n            query_dim=query_dim,\n            context_dim=query_dim,\n            heads=n_heads,",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "GatedSelfAttentionDense2",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class GatedSelfAttentionDense2(nn.Module):\n    def __init__(self, query_dim, context_dim, n_heads, d_head):\n        super().__init__()\n        # we need a linear projection since we need cat visual feature and obj\n        # feature\n        self.linear = nn.Linear(context_dim, query_dim)\n        self.attn = CrossAttention(\n            query_dim=query_dim, context_dim=query_dim, dim_head=d_head)\n        self.ff = FeedForward(query_dim, glu=True)\n        self.norm1 = nn.LayerNorm(query_dim)",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "FourierEmbedder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class FourierEmbedder():\n    def __init__(self, num_freqs=64, temperature=100):\n        self.num_freqs = num_freqs\n        self.temperature = temperature\n        self.freq_bands = temperature ** (torch.arange(num_freqs) / num_freqs)\n    @torch.no_grad()\n    def __call__(self, x, cat_dim=-1):\n        \"x: arbitrary shape of tensor. dim: cat dim\"\n        out = []\n        for freq in self.freq_bands:",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "PositionNet",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class PositionNet(nn.Module):\n    def __init__(self, in_dim, out_dim, fourier_freqs=8):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.fourier_embedder = FourierEmbedder(num_freqs=fourier_freqs)\n        self.position_dim = fourier_freqs * 2 * 4  # 2 is sin&cos, 4 is xyxy\n        self.linears = nn.Sequential(\n            nn.Linear(self.in_dim + self.position_dim, 512),\n            nn.SiLU(),",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "Gligen",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "class Gligen(nn.Module):\n    def __init__(self, modules, position_net, key_dim):\n        super().__init__()\n        self.module_list = nn.ModuleList(modules)\n        self.position_net = position_net\n        self.key_dim = key_dim\n        self.max_objs = 30\n        self.current_device = torch.device(\"cpu\")\n    def _set_position(self, boxes, masks, positive_embeddings):\n        objs = self.position_net(boxes, masks, positive_embeddings)",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n# feedforward\nclass GEGLU(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "uniq",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "def uniq(arr):\n    return{el: True for el in arr}.keys()\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "def default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "load_gligen",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.gligen",
        "description": "Fooocus.backend.headless.fcbh.gligen",
        "peekOfCode": "def load_gligen(sd):\n    sd_k = sd.keys()\n    output_list = []\n    key_dim = 768\n    for a in [\"input_blocks\", \"middle_block\", \"output_blocks\"]:\n        for b in range(20):\n            k_temp = filter(lambda k: \"{}.{}.\".format(a, b)\n                            in k and \".fuser.\" in k, sd_k)\n            k_temp = map(lambda k: (k, k.split(\".fuser.\")[-1]), k_temp)\n            n_sd = {}",
        "detail": "Fooocus.backend.headless.fcbh.gligen",
        "documentation": {}
    },
    {
        "label": "LatentFormat",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.latent_formats",
        "description": "Fooocus.backend.headless.fcbh.latent_formats",
        "peekOfCode": "class LatentFormat:\n    scale_factor = 1.0\n    latent_rgb_factors = None\n    taesd_decoder_name = None\n    def process_in(self, latent):\n        return latent * self.scale_factor\n    def process_out(self, latent):\n        return latent / self.scale_factor\nclass SD15(LatentFormat):\n    def __init__(self, scale_factor=0.18215):",
        "detail": "Fooocus.backend.headless.fcbh.latent_formats",
        "documentation": {}
    },
    {
        "label": "SD15",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.latent_formats",
        "description": "Fooocus.backend.headless.fcbh.latent_formats",
        "peekOfCode": "class SD15(LatentFormat):\n    def __init__(self, scale_factor=0.18215):\n        self.scale_factor = scale_factor\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3512,  0.2297,  0.3227],\n                    [ 0.3250,  0.4974,  0.2350],\n                    [-0.2829,  0.1762,  0.2721],\n                    [-0.2120, -0.2616, -0.7177]\n                ]",
        "detail": "Fooocus.backend.headless.fcbh.latent_formats",
        "documentation": {}
    },
    {
        "label": "SDXL",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.latent_formats",
        "description": "Fooocus.backend.headless.fcbh.latent_formats",
        "peekOfCode": "class SDXL(LatentFormat):\n    def __init__(self):\n        self.scale_factor = 0.13025\n        self.latent_rgb_factors = [\n                    #   R        G        B\n                    [ 0.3920,  0.4054,  0.4549],\n                    [-0.2634, -0.0196,  0.0653],\n                    [ 0.0568,  0.1687, -0.0755],\n                    [-0.3112, -0.2359, -0.2076]\n                ]",
        "detail": "Fooocus.backend.headless.fcbh.latent_formats",
        "documentation": {}
    },
    {
        "label": "load_lora",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.lora",
        "description": "Fooocus.backend.headless.fcbh.lora",
        "peekOfCode": "def load_lora(lora, to_load):\n    patch_dict = {}\n    loaded_keys = set()\n    for x in to_load:\n        alpha_name = \"{}.alpha\".format(x)\n        alpha = None\n        if alpha_name in lora.keys():\n            alpha = lora[alpha_name].item()\n            loaded_keys.add(alpha_name)\n        regular_lora = \"{}.lora_up.weight\".format(x)",
        "detail": "Fooocus.backend.headless.fcbh.lora",
        "documentation": {}
    },
    {
        "label": "model_lora_keys_clip",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.lora",
        "description": "Fooocus.backend.headless.fcbh.lora",
        "peekOfCode": "def model_lora_keys_clip(model, key_map={}):\n    sdk = model.state_dict().keys()\n    text_model_lora_key = \"lora_te_text_model_encoder_layers_{}_{}\"\n    clip_l_present = False\n    for b in range(32): #TODO: clean up\n        for c in LORA_CLIP_MAP:\n            k = \"clip_h.transformer.text_model.encoder.layers.{}.{}.weight\".format(b, c)\n            if k in sdk:\n                lora_key = text_model_lora_key.format(b, LORA_CLIP_MAP[c])\n                key_map[lora_key] = k",
        "detail": "Fooocus.backend.headless.fcbh.lora",
        "documentation": {}
    },
    {
        "label": "model_lora_keys_unet",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.lora",
        "description": "Fooocus.backend.headless.fcbh.lora",
        "peekOfCode": "def model_lora_keys_unet(model, key_map={}):\n    sdk = model.state_dict().keys()\n    for k in sdk:\n        if k.startswith(\"diffusion_model.\") and k.endswith(\".weight\"):\n            key_lora = k[len(\"diffusion_model.\"):-len(\".weight\")].replace(\".\", \"_\")\n            key_map[\"lora_unet_{}\".format(key_lora)] = k\n    diffusers_keys = fcbh.utils.unet_to_diffusers(model.model_config.unet_config)\n    for k in diffusers_keys:\n        if k.endswith(\".weight\"):\n            unet_key = \"diffusion_model.{}\".format(diffusers_keys[k])",
        "detail": "Fooocus.backend.headless.fcbh.lora",
        "documentation": {}
    },
    {
        "label": "LORA_CLIP_MAP",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.lora",
        "description": "Fooocus.backend.headless.fcbh.lora",
        "peekOfCode": "LORA_CLIP_MAP = {\n    \"mlp.fc1\": \"mlp_fc1\",\n    \"mlp.fc2\": \"mlp_fc2\",\n    \"self_attn.k_proj\": \"self_attn_k_proj\",\n    \"self_attn.q_proj\": \"self_attn_q_proj\",\n    \"self_attn.v_proj\": \"self_attn_v_proj\",\n    \"self_attn.out_proj\": \"self_attn_out_proj\",\n}\ndef load_lora(lora, to_load):\n    patch_dict = {}",
        "detail": "Fooocus.backend.headless.fcbh.lora",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "class ModelType(Enum):\n    EPS = 1\n    V_PREDICTION = 2\nfrom fcbh.model_sampling import EPS, V_PREDICTION, ModelSamplingDiscrete\ndef model_sampling(model_config, model_type):\n    if model_type == ModelType.EPS:\n        c = EPS\n    elif model_type == ModelType.V_PREDICTION:\n        c = V_PREDICTION\n    s = ModelSamplingDiscrete",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "class BaseModel(torch.nn.Module):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__()\n        unet_config = model_config.unet_config\n        self.latent_format = model_config.latent_format\n        self.model_config = model_config\n        if not unet_config.get(\"disable_unet_model_creation\", False):\n            self.diffusion_model = UNetModel(**unet_config, device=device)\n        self.model_type = model_type\n        self.model_sampling = model_sampling(model_config, model_type)",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "SD21UNCLIP",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "class SD21UNCLIP(BaseModel):\n    def __init__(self, model_config, noise_aug_config, model_type=ModelType.V_PREDICTION, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**noise_aug_config)\n    def encode_adm(self, **kwargs):\n        unclip_conditioning = kwargs.get(\"unclip_conditioning\", None)\n        device = kwargs[\"device\"]\n        if unclip_conditioning is None:\n            return torch.zeros((1, self.adm_channels))\n        else:",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "SDXLRefiner",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "class SDXLRefiner(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**{\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1280})\n    def encode_adm(self, **kwargs):\n        clip_pooled = sdxl_pooled(kwargs, self.noise_augmentor)\n        width = kwargs.get(\"width\", 768)\n        height = kwargs.get(\"height\", 768)\n        crop_w = kwargs.get(\"crop_w\", 0)",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "SDXL",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "class SDXL(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**{\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1280})\n    def encode_adm(self, **kwargs):\n        clip_pooled = sdxl_pooled(kwargs, self.noise_augmentor)\n        width = kwargs.get(\"width\", 768)\n        height = kwargs.get(\"height\", 768)\n        crop_w = kwargs.get(\"crop_w\", 0)",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "model_sampling",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "def model_sampling(model_config, model_type):\n    if model_type == ModelType.EPS:\n        c = EPS\n    elif model_type == ModelType.V_PREDICTION:\n        c = V_PREDICTION\n    s = ModelSamplingDiscrete\n    class ModelSampling(s, c):\n        pass\n    return ModelSampling(model_config)\nclass BaseModel(torch.nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "unclip_adm",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "def unclip_adm(unclip_conditioning, device, noise_augmentor, noise_augment_merge=0.0):\n    adm_inputs = []\n    weights = []\n    noise_aug = []\n    for unclip_cond in unclip_conditioning:\n        for adm_cond in unclip_cond[\"clip_vision_output\"].image_embeds:\n            weight = unclip_cond[\"strength\"]\n            noise_augment = unclip_cond[\"noise_augmentation\"]\n            noise_level = round((noise_augmentor.max_noise_level - 1) * noise_augment)\n            c_adm, noise_level_emb = noise_augmentor(adm_cond.to(device), noise_level=torch.tensor([noise_level], device=device))",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "sdxl_pooled",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_base",
        "description": "Fooocus.backend.headless.fcbh.model_base",
        "peekOfCode": "def sdxl_pooled(args, noise_augmentor):\n    if \"unclip_conditioning\" in args:\n        return unclip_adm(args.get(\"unclip_conditioning\", None), args[\"device\"], noise_augmentor)[:,:1280]\n    else:\n        return args[\"pooled_output\"]\nclass SDXLRefiner(BaseModel):\n    def __init__(self, model_config, model_type=ModelType.EPS, device=None):\n        super().__init__(model_config, model_type, device=device)\n        self.embedder = Timestep(256)\n        self.noise_augmentor = CLIPEmbeddingNoiseAugmentation(**{\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1280})",
        "detail": "Fooocus.backend.headless.fcbh.model_base",
        "documentation": {}
    },
    {
        "label": "count_blocks",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def count_blocks(state_dict_keys, prefix_string):\n    count = 0\n    while True:\n        c = False\n        for k in state_dict_keys:\n            if k.startswith(prefix_string.format(count)):\n                c = True\n                break\n        if c == False:\n            break",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "calculate_transformer_depth",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def calculate_transformer_depth(prefix, state_dict_keys, state_dict):\n    context_dim = None\n    use_linear_in_transformer = False\n    transformer_prefix = prefix + \"1.transformer_blocks.\"\n    transformer_keys = sorted(list(filter(lambda a: a.startswith(transformer_prefix), state_dict_keys)))\n    if len(transformer_keys) > 0:\n        last_transformer_depth = count_blocks(state_dict_keys, transformer_prefix + '{}')\n        context_dim = state_dict['{}0.attn2.to_k.weight'.format(transformer_prefix)].shape[1]\n        use_linear_in_transformer = len(state_dict['{}1.proj_in.weight'.format(prefix)].shape) == 2\n        return last_transformer_depth, context_dim, use_linear_in_transformer",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "detect_unet_config",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def detect_unet_config(state_dict, key_prefix, dtype):\n    state_dict_keys = list(state_dict.keys())\n    unet_config = {\n        \"use_checkpoint\": False,\n        \"image_size\": 32,\n        \"out_channels\": 4,\n        \"use_spatial_transformer\": True,\n        \"legacy\": False\n    }\n    y_input = '{}label_emb.0.0.weight'.format(key_prefix)",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "model_config_from_unet_config",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def model_config_from_unet_config(unet_config):\n    for model_config in fcbh.supported_models.models:\n        if model_config.matches(unet_config):\n            return model_config(unet_config)\n    print(\"no match\", unet_config)\n    return None\ndef model_config_from_unet(state_dict, unet_key_prefix, dtype, use_base_if_no_match=False):\n    unet_config = detect_unet_config(state_dict, unet_key_prefix, dtype)\n    model_config = model_config_from_unet_config(unet_config)\n    if model_config is None and use_base_if_no_match:",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "model_config_from_unet",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def model_config_from_unet(state_dict, unet_key_prefix, dtype, use_base_if_no_match=False):\n    unet_config = detect_unet_config(state_dict, unet_key_prefix, dtype)\n    model_config = model_config_from_unet_config(unet_config)\n    if model_config is None and use_base_if_no_match:\n        return fcbh.supported_models_base.BASE(unet_config)\n    else:\n        return model_config\ndef convert_config(unet_config):\n    new_config = unet_config.copy()\n    num_res_blocks = new_config.get(\"num_res_blocks\", None)",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "convert_config",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def convert_config(unet_config):\n    new_config = unet_config.copy()\n    num_res_blocks = new_config.get(\"num_res_blocks\", None)\n    channel_mult = new_config.get(\"channel_mult\", None)\n    if isinstance(num_res_blocks, int):\n        num_res_blocks = len(channel_mult) * [num_res_blocks]\n    if \"attention_resolutions\" in new_config:\n        attention_resolutions = new_config.pop(\"attention_resolutions\")\n        transformer_depth = new_config.get(\"transformer_depth\", None)\n        transformer_depth_middle = new_config.get(\"transformer_depth_middle\", None)",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "unet_config_from_diffusers_unet",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def unet_config_from_diffusers_unet(state_dict, dtype):\n    match = {}\n    transformer_depth = []\n    attn_res = 1\n    down_blocks = count_blocks(state_dict, \"down_blocks.{}\")\n    for i in range(down_blocks):\n        attn_blocks = count_blocks(state_dict, \"down_blocks.{}.attentions.\".format(i) + '{}')\n        for ab in range(attn_blocks):\n            transformer_count = count_blocks(state_dict, \"down_blocks.{}.attentions.{}.transformer_blocks.\".format(i, ab) + '{}')\n            transformer_depth.append(transformer_count)",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "model_config_from_diffusers_unet",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_detection",
        "description": "Fooocus.backend.headless.fcbh.model_detection",
        "peekOfCode": "def model_config_from_diffusers_unet(state_dict, dtype):\n    unet_config = unet_config_from_diffusers_unet(state_dict, dtype)\n    if unet_config is not None:\n        return model_config_from_unet_config(unet_config)\n    return None",
        "detail": "Fooocus.backend.headless.fcbh.model_detection",
        "documentation": {}
    },
    {
        "label": "VRAMState",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "class VRAMState(Enum):\n    DISABLED = 0    #No vram present: no need to move models to vram\n    NO_VRAM = 1     #Very low vram: enable all the options to save vram\n    LOW_VRAM = 2\n    NORMAL_VRAM = 3\n    HIGH_VRAM = 4\n    SHARED = 5      #No dedicated vram: memory shared between CPU and GPU but models still need to be moved between both.\nclass CPUState(Enum):\n    GPU = 0\n    CPU = 1",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "CPUState",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "class CPUState(Enum):\n    GPU = 0\n    CPU = 1\n    MPS = 2\n# Determine VRAM State\nvram_state = VRAMState.NORMAL_VRAM\nset_vram_to = VRAMState.NORMAL_VRAM\ncpu_state = CPUState.GPU\ntotal_vram = 0\nlowvram_available = True",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "LoadedModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "class LoadedModel:\n    def __init__(self, model):\n        self.model = model\n        self.model_accelerated = False\n        self.device = model.load_device\n    def model_memory(self):\n        return self.model.model_size()\n    def model_memory_required(self, device):\n        if device == self.model.current_device:\n            return 0",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "InterruptProcessingException",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "class InterruptProcessingException(Exception):\n    pass\ninterrupt_processing_mutex = threading.RLock()\ninterrupt_processing = False\ndef interrupt_current_processing(value=True):\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        interrupt_processing = value\ndef processing_interrupted():",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "is_intel_xpu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def is_intel_xpu():\n    global cpu_state\n    global xpu_available\n    if cpu_state == CPUState.GPU:\n        if xpu_available:\n            return True\n    return False\ndef get_torch_device():\n    global directml_enabled\n    global cpu_state",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "get_torch_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def get_torch_device():\n    global directml_enabled\n    global cpu_state\n    if directml_enabled:\n        global directml_device\n        return directml_device\n    if cpu_state == CPUState.MPS:\n        return torch.device(\"mps\")\n    if cpu_state == CPUState.CPU:\n        return torch.device(\"cpu\")",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "get_total_memory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def get_total_memory(dev=None, torch_total_too=False):\n    global directml_enabled\n    if dev is None:\n        dev = get_torch_device()\n    if hasattr(dev, 'type') and (dev.type == 'cpu' or dev.type == 'mps'):\n        mem_total = psutil.virtual_memory().total\n        mem_total_torch = mem_total\n    else:\n        if directml_enabled:\n            mem_total = 1024 * 1024 * 1024 #TODO",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "is_nvidia",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def is_nvidia():\n    global cpu_state\n    if cpu_state == CPUState.GPU:\n        if torch.version.cuda:\n            return True\n    return False\nENABLE_PYTORCH_ATTENTION = False\nif args.use_pytorch_cross_attention:\n    ENABLE_PYTORCH_ATTENTION = True\n    XFORMERS_IS_AVAILABLE = False",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "get_torch_device_name",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def get_torch_device_name(device):\n    if hasattr(device, 'type'):\n        if device.type == \"cuda\":\n            try:\n                allocator_backend = torch.cuda.get_allocator_backend()\n            except:\n                allocator_backend = \"\"\n            return \"{} {} : {}\".format(device, torch.cuda.get_device_name(device), allocator_backend)\n        else:\n            return \"{}\".format(device.type)",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "minimum_inference_memory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def minimum_inference_memory():\n    return (1024 * 1024 * 1024)\ndef unload_model_clones(model):\n    to_unload = []\n    for i in range(len(current_loaded_models)):\n        if model.is_clone(current_loaded_models[i].model):\n            to_unload = [i] + to_unload\n    for i in to_unload:\n        print(\"unload clone\", i)\n        current_loaded_models.pop(i).model_unload()",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "unload_model_clones",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def unload_model_clones(model):\n    to_unload = []\n    for i in range(len(current_loaded_models)):\n        if model.is_clone(current_loaded_models[i].model):\n            to_unload = [i] + to_unload\n    for i in to_unload:\n        print(\"unload clone\", i)\n        current_loaded_models.pop(i).model_unload()\ndef free_memory(memory_required, device, keep_loaded=[]):\n    unloaded_model = False",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "free_memory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def free_memory(memory_required, device, keep_loaded=[]):\n    unloaded_model = False\n    for i in range(len(current_loaded_models) -1, -1, -1):\n        if not DISABLE_SMART_MEMORY:\n            if get_free_memory(device) > memory_required:\n                break\n        shift_model = current_loaded_models[i]\n        if shift_model.device == device:\n            if shift_model not in keep_loaded:\n                m = current_loaded_models.pop(i)",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "load_models_gpu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def load_models_gpu(models, memory_required=0):\n    global vram_state\n    inference_memory = minimum_inference_memory()\n    extra_mem = max(inference_memory, memory_required)\n    models_to_load = []\n    models_already_loaded = []\n    for x in models:\n        loaded_model = LoadedModel(x)\n        if loaded_model in current_loaded_models:\n            index = current_loaded_models.index(loaded_model)",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "load_model_gpu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def load_model_gpu(model):\n    return load_models_gpu([model])\ndef cleanup_models():\n    to_delete = []\n    for i in range(len(current_loaded_models)):\n        if sys.getrefcount(current_loaded_models[i].model) <= 2:\n            to_delete = [i] + to_delete\n    for i in to_delete:\n        x = current_loaded_models.pop(i)\n        x.model_unload()",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "cleanup_models",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def cleanup_models():\n    to_delete = []\n    for i in range(len(current_loaded_models)):\n        if sys.getrefcount(current_loaded_models[i].model) <= 2:\n            to_delete = [i] + to_delete\n    for i in to_delete:\n        x = current_loaded_models.pop(i)\n        x.model_unload()\n        del x\ndef dtype_size(dtype):",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "dtype_size",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def dtype_size(dtype):\n    dtype_size = 4\n    if dtype == torch.float16 or dtype == torch.bfloat16:\n        dtype_size = 2\n    return dtype_size\ndef unet_offload_device():\n    if vram_state == VRAMState.HIGH_VRAM:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "unet_offload_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def unet_offload_device():\n    if vram_state == VRAMState.HIGH_VRAM:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\ndef unet_inital_load_device(parameters, dtype):\n    torch_dev = get_torch_device()\n    if vram_state == VRAMState.HIGH_VRAM:\n        return torch_dev\n    cpu_dev = torch.device(\"cpu\")",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "unet_inital_load_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def unet_inital_load_device(parameters, dtype):\n    torch_dev = get_torch_device()\n    if vram_state == VRAMState.HIGH_VRAM:\n        return torch_dev\n    cpu_dev = torch.device(\"cpu\")\n    if DISABLE_SMART_MEMORY:\n        return cpu_dev\n    model_size = dtype_size(dtype) * parameters\n    mem_dev = get_free_memory(torch_dev)\n    mem_cpu = get_free_memory(cpu_dev)",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "unet_dtype",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def unet_dtype(device=None, model_params=0):\n    if args.bf16_unet:\n        return torch.bfloat16\n    if should_use_fp16(device=device, model_params=model_params):\n        return torch.float16\n    return torch.float32\ndef text_encoder_offload_device():\n    if args.gpu_only:\n        return get_torch_device()\n    else:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "text_encoder_offload_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def text_encoder_offload_device():\n    if args.gpu_only:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\ndef text_encoder_device():\n    if args.gpu_only:\n        return get_torch_device()\n    elif vram_state == VRAMState.HIGH_VRAM or vram_state == VRAMState.NORMAL_VRAM:\n        if is_intel_xpu():",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "text_encoder_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def text_encoder_device():\n    if args.gpu_only:\n        return get_torch_device()\n    elif vram_state == VRAMState.HIGH_VRAM or vram_state == VRAMState.NORMAL_VRAM:\n        if is_intel_xpu():\n            return torch.device(\"cpu\")\n        if should_use_fp16(prioritize_performance=False):\n            return get_torch_device()\n        else:\n            return torch.device(\"cpu\")",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "text_encoder_dtype",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def text_encoder_dtype(device=None):\n    if args.fp8_e4m3fn_text_enc:\n        return torch.float8_e4m3fn\n    elif args.fp8_e5m2_text_enc:\n        return torch.float8_e5m2\n    elif args.fp16_text_enc:\n        return torch.float16\n    elif args.fp32_text_enc:\n        return torch.float32\n    if should_use_fp16(device, prioritize_performance=False):",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "vae_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def vae_device():\n    return get_torch_device()\ndef vae_offload_device():\n    if args.gpu_only:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\ndef vae_dtype():\n    global VAE_DTYPE\n    return VAE_DTYPE",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "vae_offload_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def vae_offload_device():\n    if args.gpu_only:\n        return get_torch_device()\n    else:\n        return torch.device(\"cpu\")\ndef vae_dtype():\n    global VAE_DTYPE\n    return VAE_DTYPE\ndef get_autocast_device(dev):\n    if hasattr(dev, 'type'):",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "vae_dtype",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def vae_dtype():\n    global VAE_DTYPE\n    return VAE_DTYPE\ndef get_autocast_device(dev):\n    if hasattr(dev, 'type'):\n        return dev.type\n    return \"cuda\"\ndef cast_to_device(tensor, device, dtype, copy=False):\n    device_supports_cast = False\n    if tensor.dtype == torch.float32 or tensor.dtype == torch.float16:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "get_autocast_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def get_autocast_device(dev):\n    if hasattr(dev, 'type'):\n        return dev.type\n    return \"cuda\"\ndef cast_to_device(tensor, device, dtype, copy=False):\n    device_supports_cast = False\n    if tensor.dtype == torch.float32 or tensor.dtype == torch.float16:\n        device_supports_cast = True\n    elif tensor.dtype == torch.bfloat16:\n        if hasattr(device, 'type') and device.type.startswith(\"cuda\"):",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "cast_to_device",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def cast_to_device(tensor, device, dtype, copy=False):\n    device_supports_cast = False\n    if tensor.dtype == torch.float32 or tensor.dtype == torch.float16:\n        device_supports_cast = True\n    elif tensor.dtype == torch.bfloat16:\n        if hasattr(device, 'type') and device.type.startswith(\"cuda\"):\n            device_supports_cast = True\n        elif is_intel_xpu():\n            device_supports_cast = True\n    if device_supports_cast:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "xformers_enabled",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def xformers_enabled():\n    global directml_enabled\n    global cpu_state\n    if cpu_state != CPUState.GPU:\n        return False\n    if is_intel_xpu():\n        return False\n    if directml_enabled:\n        return False\n    return XFORMERS_IS_AVAILABLE",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "xformers_enabled_vae",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def xformers_enabled_vae():\n    enabled = xformers_enabled()\n    if not enabled:\n        return False\n    return XFORMERS_ENABLED_VAE\ndef pytorch_attention_enabled():\n    global ENABLE_PYTORCH_ATTENTION\n    return ENABLE_PYTORCH_ATTENTION\ndef pytorch_attention_flash_attention():\n    global ENABLE_PYTORCH_ATTENTION",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "pytorch_attention_enabled",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def pytorch_attention_enabled():\n    global ENABLE_PYTORCH_ATTENTION\n    return ENABLE_PYTORCH_ATTENTION\ndef pytorch_attention_flash_attention():\n    global ENABLE_PYTORCH_ATTENTION\n    if ENABLE_PYTORCH_ATTENTION:\n        #TODO: more reliable way of checking for flash attention?\n        if is_nvidia(): #pytorch flash attention only works on Nvidia\n            return True\n    return False",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "pytorch_attention_flash_attention",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def pytorch_attention_flash_attention():\n    global ENABLE_PYTORCH_ATTENTION\n    if ENABLE_PYTORCH_ATTENTION:\n        #TODO: more reliable way of checking for flash attention?\n        if is_nvidia(): #pytorch flash attention only works on Nvidia\n            return True\n    return False\ndef get_free_memory(dev=None, torch_free_too=False):\n    global directml_enabled\n    if dev is None:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "get_free_memory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def get_free_memory(dev=None, torch_free_too=False):\n    global directml_enabled\n    if dev is None:\n        dev = get_torch_device()\n    if hasattr(dev, 'type') and (dev.type == 'cpu' or dev.type == 'mps'):\n        mem_free_total = psutil.virtual_memory().available\n        mem_free_torch = mem_free_total\n    else:\n        if directml_enabled:\n            mem_free_total = 1024 * 1024 * 1024 #TODO",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "cpu_mode",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def cpu_mode():\n    global cpu_state\n    return cpu_state == CPUState.CPU\ndef mps_mode():\n    global cpu_state\n    return cpu_state == CPUState.MPS\ndef is_device_cpu(device):\n    if hasattr(device, 'type'):\n        if (device.type == 'cpu'):\n            return True",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "mps_mode",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def mps_mode():\n    global cpu_state\n    return cpu_state == CPUState.MPS\ndef is_device_cpu(device):\n    if hasattr(device, 'type'):\n        if (device.type == 'cpu'):\n            return True\n    return False\ndef is_device_mps(device):\n    if hasattr(device, 'type'):",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "is_device_cpu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def is_device_cpu(device):\n    if hasattr(device, 'type'):\n        if (device.type == 'cpu'):\n            return True\n    return False\ndef is_device_mps(device):\n    if hasattr(device, 'type'):\n        if (device.type == 'mps'):\n            return True\n    return False",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "is_device_mps",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def is_device_mps(device):\n    if hasattr(device, 'type'):\n        if (device.type == 'mps'):\n            return True\n    return False\ndef should_use_fp16(device=None, model_params=0, prioritize_performance=True):\n    global directml_enabled\n    if device is not None:\n        if is_device_cpu(device):\n            return False",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "should_use_fp16",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def should_use_fp16(device=None, model_params=0, prioritize_performance=True):\n    global directml_enabled\n    if device is not None:\n        if is_device_cpu(device):\n            return False\n    if FORCE_FP16:\n        return True\n    if device is not None: #TODO\n        if is_device_mps(device):\n            return False",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "soft_empty_cache",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def soft_empty_cache(force=False):\n    global cpu_state\n    if cpu_state == CPUState.MPS:\n        torch.mps.empty_cache()\n    elif is_intel_xpu():\n        torch.xpu.empty_cache()\n    elif torch.cuda.is_available():\n        if force or is_nvidia(): #This seems to make things worse on ROCm so I only do it for cuda\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "resolve_lowvram_weight",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def resolve_lowvram_weight(weight, model, key):\n    if weight.device == torch.device(\"meta\"): #lowvram NOTE: this depends on the inner working of the accelerate library so it might break.\n        key_split = key.split('.')              # I have no idea why they don't just leave the weight there instead of using the meta device.\n        op = fcbh.utils.get_attr(model, '.'.join(key_split[:-1]))\n        weight = op._hf_hook.weights_map[key_split[-1]]\n    return weight\n#TODO: might be cleaner to put this somewhere else\nimport threading\nclass InterruptProcessingException(Exception):\n    pass",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "interrupt_current_processing",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def interrupt_current_processing(value=True):\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        interrupt_processing = value\ndef processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        return interrupt_processing",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "processing_interrupted",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        return interrupt_processing\ndef throw_exception_if_processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        if interrupt_processing:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "throw_exception_if_processing_interrupted",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "def throw_exception_if_processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        if interrupt_processing:\n            interrupt_processing = False\n            raise InterruptProcessingException()",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "vram_state",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "vram_state = VRAMState.NORMAL_VRAM\nset_vram_to = VRAMState.NORMAL_VRAM\ncpu_state = CPUState.GPU\ntotal_vram = 0\nlowvram_available = True\nxpu_available = False\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "set_vram_to",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "set_vram_to = VRAMState.NORMAL_VRAM\ncpu_state = CPUState.GPU\ntotal_vram = 0\nlowvram_available = True\nxpu_available = False\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "cpu_state",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "cpu_state = CPUState.GPU\ntotal_vram = 0\nlowvram_available = True\nxpu_available = False\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml\n    if device_index < 0:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "total_vram",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "total_vram = 0\nlowvram_available = True\nxpu_available = False\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml\n    if device_index < 0:\n        directml_device = torch_directml.device()",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "lowvram_available",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "lowvram_available = True\nxpu_available = False\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml\n    if device_index < 0:\n        directml_device = torch_directml.device()\n    else:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "xpu_available",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "xpu_available = False\ndirectml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml\n    if device_index < 0:\n        directml_device = torch_directml.device()\n    else:\n        directml_device = torch_directml.device(device_index)",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "directml_enabled",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "directml_enabled = False\nif args.directml is not None:\n    import torch_directml\n    directml_enabled = True\n    device_index = args.directml\n    if device_index < 0:\n        directml_device = torch_directml.device()\n    else:\n        directml_device = torch_directml.device(device_index)\n    print(\"Using directml with device:\", torch_directml.device_name(device_index))",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "total_vram",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "total_vram = get_total_memory(get_torch_device()) / (1024 * 1024)\ntotal_ram = psutil.virtual_memory().total / (1024 * 1024)\nprint(\"Total VRAM {:0.0f} MB, total RAM {:0.0f} MB\".format(total_vram, total_ram))\nif not args.normalvram and not args.cpu:\n    if lowvram_available and total_vram <= 4096:\n        print(\"Trying to enable lowvram mode because your GPU seems to have 4GB or less. If you don't want this use: --normalvram\")\n        set_vram_to = VRAMState.LOW_VRAM\ntry:\n    OOM_EXCEPTION = torch.cuda.OutOfMemoryError\nexcept:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "total_ram",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "total_ram = psutil.virtual_memory().total / (1024 * 1024)\nprint(\"Total VRAM {:0.0f} MB, total RAM {:0.0f} MB\".format(total_vram, total_ram))\nif not args.normalvram and not args.cpu:\n    if lowvram_available and total_vram <= 4096:\n        print(\"Trying to enable lowvram mode because your GPU seems to have 4GB or less. If you don't want this use: --normalvram\")\n        set_vram_to = VRAMState.LOW_VRAM\ntry:\n    OOM_EXCEPTION = torch.cuda.OutOfMemoryError\nexcept:\n    OOM_EXCEPTION = Exception",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "XFORMERS_VERSION",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "XFORMERS_VERSION = \"\"\nXFORMERS_ENABLED_VAE = True\nif args.disable_xformers:\n    XFORMERS_IS_AVAILABLE = False\nelse:\n    try:\n        import xformers\n        import xformers.ops\n        XFORMERS_IS_AVAILABLE = True\n        try:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "XFORMERS_ENABLED_VAE",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "XFORMERS_ENABLED_VAE = True\nif args.disable_xformers:\n    XFORMERS_IS_AVAILABLE = False\nelse:\n    try:\n        import xformers\n        import xformers.ops\n        XFORMERS_IS_AVAILABLE = True\n        try:\n            XFORMERS_IS_AVAILABLE = xformers._has_cpp_library",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "ENABLE_PYTORCH_ATTENTION",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "ENABLE_PYTORCH_ATTENTION = False\nif args.use_pytorch_cross_attention:\n    ENABLE_PYTORCH_ATTENTION = True\n    XFORMERS_IS_AVAILABLE = False\nVAE_DTYPE = torch.float32\ntry:\n    if is_nvidia():\n        torch_version = torch.version.__version__\n        if int(torch_version[0]) >= 2:\n            if ENABLE_PYTORCH_ATTENTION == False and args.use_split_cross_attention == False and args.use_quad_cross_attention == False:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "VAE_DTYPE",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "VAE_DTYPE = torch.float32\ntry:\n    if is_nvidia():\n        torch_version = torch.version.__version__\n        if int(torch_version[0]) >= 2:\n            if ENABLE_PYTORCH_ATTENTION == False and args.use_split_cross_attention == False and args.use_quad_cross_attention == False:\n                ENABLE_PYTORCH_ATTENTION = True\n            if torch.cuda.is_bf16_supported():\n                VAE_DTYPE = torch.bfloat16\n    if is_intel_xpu():",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "FORCE_FP32",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "FORCE_FP32 = False\nFORCE_FP16 = False\nif args.force_fp32:\n    print(\"Forcing FP32, if this improves things please report it.\")\n    FORCE_FP32 = True\nif args.force_fp16:\n    print(\"Forcing FP16.\")\n    FORCE_FP16 = True\nif lowvram_available:\n    try:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "FORCE_FP16",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "FORCE_FP16 = False\nif args.force_fp32:\n    print(\"Forcing FP32, if this improves things please report it.\")\n    FORCE_FP32 = True\nif args.force_fp16:\n    print(\"Forcing FP16.\")\n    FORCE_FP16 = True\nif lowvram_available:\n    try:\n        import accelerate",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "DISABLE_SMART_MEMORY",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "DISABLE_SMART_MEMORY = args.disable_smart_memory\nif DISABLE_SMART_MEMORY:\n    print(\"Disabling smart memory management\")\ndef get_torch_device_name(device):\n    if hasattr(device, 'type'):\n        if device.type == \"cuda\":\n            try:\n                allocator_backend = torch.cuda.get_allocator_backend()\n            except:\n                allocator_backend = \"\"",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "current_loaded_models",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "current_loaded_models = []\nclass LoadedModel:\n    def __init__(self, model):\n        self.model = model\n        self.model_accelerated = False\n        self.device = model.load_device\n    def model_memory(self):\n        return self.model.model_size()\n    def model_memory_required(self, device):\n        if device == self.model.current_device:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "interrupt_processing_mutex",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "interrupt_processing_mutex = threading.RLock()\ninterrupt_processing = False\ndef interrupt_current_processing(value=True):\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        interrupt_processing = value\ndef processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "interrupt_processing",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.model_management",
        "description": "Fooocus.backend.headless.fcbh.model_management",
        "peekOfCode": "interrupt_processing = False\ndef interrupt_current_processing(value=True):\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:\n        interrupt_processing = value\ndef processing_interrupted():\n    global interrupt_processing\n    global interrupt_processing_mutex\n    with interrupt_processing_mutex:",
        "detail": "Fooocus.backend.headless.fcbh.model_management",
        "documentation": {}
    },
    {
        "label": "ModelPatcher",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_patcher",
        "description": "Fooocus.backend.headless.fcbh.model_patcher",
        "peekOfCode": "class ModelPatcher:\n    def __init__(self, model, load_device, offload_device, size=0, current_device=None, weight_inplace_update=False):\n        self.size = size\n        self.model = model\n        self.patches = {}\n        self.backup = {}\n        self.object_patches = {}\n        self.object_patches_backup = {}\n        self.model_options = {\"transformer_options\":{}}\n        self.model_size()",
        "detail": "Fooocus.backend.headless.fcbh.model_patcher",
        "documentation": {}
    },
    {
        "label": "EPS",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_sampling",
        "description": "Fooocus.backend.headless.fcbh.model_sampling",
        "peekOfCode": "class EPS:\n    def calculate_input(self, sigma, noise):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (noise.ndim - 1))\n        return noise / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input - model_output * sigma\nclass V_PREDICTION(EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))",
        "detail": "Fooocus.backend.headless.fcbh.model_sampling",
        "documentation": {}
    },
    {
        "label": "V_PREDICTION",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_sampling",
        "description": "Fooocus.backend.headless.fcbh.model_sampling",
        "peekOfCode": "class V_PREDICTION(EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        return model_input * self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2) - model_output * sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\nclass ModelSamplingDiscrete(torch.nn.Module):\n    def __init__(self, model_config=None):\n        super().__init__()\n        beta_schedule = \"linear\"\n        if model_config is not None:\n            beta_schedule = model_config.sampling_settings.get(\"beta_schedule\", beta_schedule)",
        "detail": "Fooocus.backend.headless.fcbh.model_sampling",
        "documentation": {}
    },
    {
        "label": "ModelSamplingDiscrete",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.model_sampling",
        "description": "Fooocus.backend.headless.fcbh.model_sampling",
        "peekOfCode": "class ModelSamplingDiscrete(torch.nn.Module):\n    def __init__(self, model_config=None):\n        super().__init__()\n        beta_schedule = \"linear\"\n        if model_config is not None:\n            beta_schedule = model_config.sampling_settings.get(\"beta_schedule\", beta_schedule)\n        self._register_schedule(given_betas=None, beta_schedule=beta_schedule, timesteps=1000, linear_start=0.00085, linear_end=0.012, cosine_s=8e-3)\n        self.sigma_data = 1.0\n    def _register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):",
        "detail": "Fooocus.backend.headless.fcbh.model_sampling",
        "documentation": {}
    },
    {
        "label": "Linear",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ops",
        "description": "Fooocus.backend.headless.fcbh.ops",
        "peekOfCode": "class Linear(torch.nn.Linear):\n    def reset_parameters(self):\n        return None\nclass Conv2d(torch.nn.Conv2d):\n    def reset_parameters(self):\n        return None\nclass Conv3d(torch.nn.Conv3d):\n    def reset_parameters(self):\n        return None\ndef conv_nd(dims, *args, **kwargs):",
        "detail": "Fooocus.backend.headless.fcbh.ops",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ops",
        "description": "Fooocus.backend.headless.fcbh.ops",
        "peekOfCode": "class Conv2d(torch.nn.Conv2d):\n    def reset_parameters(self):\n        return None\nclass Conv3d(torch.nn.Conv3d):\n    def reset_parameters(self):\n        return None\ndef conv_nd(dims, *args, **kwargs):\n    if dims == 2:\n        return Conv2d(*args, **kwargs)\n    elif dims == 3:",
        "detail": "Fooocus.backend.headless.fcbh.ops",
        "documentation": {}
    },
    {
        "label": "Conv3d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.ops",
        "description": "Fooocus.backend.headless.fcbh.ops",
        "peekOfCode": "class Conv3d(torch.nn.Conv3d):\n    def reset_parameters(self):\n        return None\ndef conv_nd(dims, *args, **kwargs):\n    if dims == 2:\n        return Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return Conv3d(*args, **kwargs)\n    else:\n        raise ValueError(f\"unsupported dimensions: {dims}\")",
        "detail": "Fooocus.backend.headless.fcbh.ops",
        "documentation": {}
    },
    {
        "label": "conv_nd",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ops",
        "description": "Fooocus.backend.headless.fcbh.ops",
        "peekOfCode": "def conv_nd(dims, *args, **kwargs):\n    if dims == 2:\n        return Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return Conv3d(*args, **kwargs)\n    else:\n        raise ValueError(f\"unsupported dimensions: {dims}\")\n@contextmanager\ndef use_fcbh_ops(device=None, dtype=None): # Kind of an ugly hack but I can't think of a better way\n    old_torch_nn_linear = torch.nn.Linear",
        "detail": "Fooocus.backend.headless.fcbh.ops",
        "documentation": {}
    },
    {
        "label": "use_fcbh_ops",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.ops",
        "description": "Fooocus.backend.headless.fcbh.ops",
        "peekOfCode": "def use_fcbh_ops(device=None, dtype=None): # Kind of an ugly hack but I can't think of a better way\n    old_torch_nn_linear = torch.nn.Linear\n    force_device = device\n    force_dtype = dtype\n    def linear_with_dtype(in_features: int, out_features: int, bias: bool = True, device=None, dtype=None):\n        if force_device is not None:\n            device = force_device\n        if force_dtype is not None:\n            dtype = force_dtype\n        return Linear(in_features, out_features, bias=bias, device=device, dtype=dtype)",
        "detail": "Fooocus.backend.headless.fcbh.ops",
        "documentation": {}
    },
    {
        "label": "enable_args_parsing",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.options",
        "description": "Fooocus.backend.headless.fcbh.options",
        "peekOfCode": "def enable_args_parsing(enable=True):\n    global args_parsing\n    args_parsing = enable",
        "detail": "Fooocus.backend.headless.fcbh.options",
        "documentation": {}
    },
    {
        "label": "args_parsing",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.options",
        "description": "Fooocus.backend.headless.fcbh.options",
        "peekOfCode": "args_parsing = False\ndef enable_args_parsing(enable=True):\n    global args_parsing\n    args_parsing = enable",
        "detail": "Fooocus.backend.headless.fcbh.options",
        "documentation": {}
    },
    {
        "label": "prepare_noise",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def prepare_noise(latent_image, seed, noise_inds=None):\n    \"\"\"\n    creates random noise given a latent image and a seed.\n    optional arg skip can be used to skip and discard x number of noise generations for a given seed\n    \"\"\"\n    generator = torch.manual_seed(seed)\n    if noise_inds is None:\n        return torch.randn(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, generator=generator, device=\"cpu\")\n    unique_inds, inverse = np.unique(noise_inds, return_inverse=True)\n    noises = []",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "prepare_mask",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def prepare_mask(noise_mask, shape, device):\n    \"\"\"ensures noise mask is of proper dimensions\"\"\"\n    noise_mask = torch.nn.functional.interpolate(noise_mask.reshape((-1, 1, noise_mask.shape[-2], noise_mask.shape[-1])), size=(shape[2], shape[3]), mode=\"bilinear\")\n    noise_mask = noise_mask.round()\n    noise_mask = torch.cat([noise_mask] * shape[1], dim=1)\n    noise_mask = fcbh.utils.repeat_to_batch_size(noise_mask, shape[0])\n    noise_mask = noise_mask.to(device)\n    return noise_mask\ndef get_models_from_cond(cond, model_type):\n    models = []",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "get_models_from_cond",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def get_models_from_cond(cond, model_type):\n    models = []\n    for c in cond:\n        if model_type in c:\n            models += [c[model_type]]\n    return models\ndef convert_cond(cond):\n    out = []\n    for c in cond:\n        temp = c[1].copy()",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "convert_cond",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def convert_cond(cond):\n    out = []\n    for c in cond:\n        temp = c[1].copy()\n        model_conds = temp.get(\"model_conds\", {})\n        if c[0] is not None:\n            model_conds[\"c_crossattn\"] = fcbh.conds.CONDCrossAttn(c[0])\n        temp[\"model_conds\"] = model_conds\n        out.append(temp)\n    return out",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "get_additional_models",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def get_additional_models(positive, negative, dtype):\n    \"\"\"loads additional models in positive and negative conditioning\"\"\"\n    control_nets = set(get_models_from_cond(positive, \"control\") + get_models_from_cond(negative, \"control\"))\n    inference_memory = 0\n    control_models = []\n    for m in control_nets:\n        control_models += m.get_models()\n        inference_memory += m.inference_memory_requirements(dtype)\n    gligen = get_models_from_cond(positive, \"gligen\") + get_models_from_cond(negative, \"gligen\")\n    gligen = [x[1] for x in gligen]",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "cleanup_additional_models",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def cleanup_additional_models(models):\n    \"\"\"cleanup additional models that were loaded\"\"\"\n    for m in models:\n        if hasattr(m, 'cleanup'):\n            m.cleanup()\ndef prepare_sampling(model, noise_shape, positive, negative, noise_mask):\n    device = model.load_device\n    positive = convert_cond(positive)\n    negative = convert_cond(negative)\n    if noise_mask is not None:",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "prepare_sampling",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def prepare_sampling(model, noise_shape, positive, negative, noise_mask):\n    device = model.load_device\n    positive = convert_cond(positive)\n    negative = convert_cond(negative)\n    if noise_mask is not None:\n        noise_mask = prepare_mask(noise_mask, noise_shape, device)\n    real_model = None\n    models, inference_memory = get_additional_models(positive, negative, model.model_dtype())\n    fcbh.model_management.load_models_gpu([model] + models, model.memory_required(noise_shape) + inference_memory)\n    real_model = model.model",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "sample",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False, noise_mask=None, sigmas=None, callback=None, disable_pbar=False, seed=None):\n    real_model, positive_copy, negative_copy, noise_mask, models = prepare_sampling(model, noise.shape, positive, negative, noise_mask)\n    noise = noise.to(model.load_device)\n    latent_image = latent_image.to(model.load_device)\n    sampler = fcbh.samplers.KSampler(real_model, steps=steps, device=model.load_device, sampler=sampler_name, scheduler=scheduler, denoise=denoise, model_options=model.model_options)\n    samples = sampler.sample(noise, positive_copy, negative_copy, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    samples = samples.cpu()\n    cleanup_additional_models(models)\n    cleanup_additional_models(set(get_models_from_cond(positive, \"control\") + get_models_from_cond(negative, \"control\")))\n    return samples",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "sample_custom",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sample",
        "description": "Fooocus.backend.headless.fcbh.sample",
        "peekOfCode": "def sample_custom(model, noise, cfg, sampler, sigmas, positive, negative, latent_image, noise_mask=None, callback=None, disable_pbar=False, seed=None):\n    real_model, positive_copy, negative_copy, noise_mask, models = prepare_sampling(model, noise.shape, positive, negative, noise_mask)\n    noise = noise.to(model.load_device)\n    latent_image = latent_image.to(model.load_device)\n    sigmas = sigmas.to(model.load_device)\n    samples = fcbh.samplers.sample(real_model, noise, positive_copy, negative_copy, cfg, model.load_device, sampler, sigmas, model_options=model.model_options, latent_image=latent_image, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    samples = samples.cpu()\n    cleanup_additional_models(models)\n    cleanup_additional_models(set(get_models_from_cond(positive, \"control\") + get_models_from_cond(negative, \"control\")))\n    return samples",
        "detail": "Fooocus.backend.headless.fcbh.sample",
        "documentation": {}
    },
    {
        "label": "CFGNoisePredictor",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "class CFGNoisePredictor(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.inner_model = model\n    def apply_model(self, x, timestep, cond, uncond, cond_scale, model_options={}, seed=None):\n        out = sampling_function(self.inner_model, x, timestep, uncond, cond, cond_scale, model_options=model_options, seed=seed)\n        return out\n    def forward(self, *args, **kwargs):\n        return self.apply_model(*args, **kwargs)\nclass KSamplerX0Inpaint(torch.nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "KSamplerX0Inpaint",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "class KSamplerX0Inpaint(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.inner_model = model\n    def forward(self, x, sigma, uncond, cond, cond_scale, denoise_mask, model_options={}, seed=None):\n        if denoise_mask is not None:\n            latent_mask = 1. - denoise_mask\n            x = x * denoise_mask + (self.latent_image + self.noise * sigma.reshape([sigma.shape[0]] + [1] * (len(self.noise.shape) - 1))) * latent_mask\n        out = self.inner_model(x, sigma, cond=cond, uncond=uncond, cond_scale=cond_scale, model_options=model_options, seed=seed)\n        if denoise_mask is not None:",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "class Sampler:\n    def sample(self):\n        pass\n    def max_denoise(self, model_wrap, sigmas):\n        max_sigma = float(model_wrap.inner_model.model_sampling.sigma_max)\n        sigma = float(sigmas[0])\n        return math.isclose(max_sigma, sigma, rel_tol=1e-05) or sigma > max_sigma\nclass UNIPC(Sampler):\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        return uni_pc.sample_unipc(model_wrap, noise, latent_image, sigmas, max_denoise=self.max_denoise(model_wrap, sigmas), extra_args=extra_args, noise_mask=denoise_mask, callback=callback, disable=disable_pbar)",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "UNIPC",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "class UNIPC(Sampler):\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        return uni_pc.sample_unipc(model_wrap, noise, latent_image, sigmas, max_denoise=self.max_denoise(model_wrap, sigmas), extra_args=extra_args, noise_mask=denoise_mask, callback=callback, disable=disable_pbar)\nclass UNIPCBH2(Sampler):\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        return uni_pc.sample_unipc(model_wrap, noise, latent_image, sigmas, max_denoise=self.max_denoise(model_wrap, sigmas), extra_args=extra_args, noise_mask=denoise_mask, callback=callback, variant='bh2', disable=disable_pbar)\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nclass KSAMPLER(Sampler):",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "UNIPCBH2",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "class UNIPCBH2(Sampler):\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        return uni_pc.sample_unipc(model_wrap, noise, latent_image, sigmas, max_denoise=self.max_denoise(model_wrap, sigmas), extra_args=extra_args, noise_mask=denoise_mask, callback=callback, variant='bh2', disable=disable_pbar)\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nclass KSAMPLER(Sampler):\n    def __init__(self, sampler_function, extra_options={}, inpaint_options={}):\n        self.sampler_function = sampler_function\n        self.extra_options = extra_options",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "KSAMPLER",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "class KSAMPLER(Sampler):\n    def __init__(self, sampler_function, extra_options={}, inpaint_options={}):\n        self.sampler_function = sampler_function\n        self.extra_options = extra_options\n        self.inpaint_options = inpaint_options\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        extra_args[\"denoise_mask\"] = denoise_mask\n        model_k = KSamplerX0Inpaint(model_wrap)\n        model_k.latent_image = latent_image\n        if self.inpaint_options.get(\"random\", False): #TODO: Should this be the default?",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "KSampler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "class KSampler:\n    SCHEDULERS = SCHEDULER_NAMES\n    SAMPLERS = SAMPLER_NAMES\n    def __init__(self, model, steps, device, sampler=None, scheduler=None, denoise=None, model_options={}):\n        self.model = model\n        self.device = device\n        if scheduler not in self.SCHEDULERS:\n            scheduler = self.SCHEDULERS[0]\n        if sampler not in self.SAMPLERS:\n            sampler = self.SAMPLERS[0]",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "sampling_function",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def sampling_function(model, x, timestep, uncond, cond, cond_scale, model_options={}, seed=None):\n        def get_area_and_mult(conds, x_in, timestep_in):\n            area = (x_in.shape[2], x_in.shape[3], 0, 0)\n            strength = 1.0\n            if 'timestep_start' in conds:\n                timestep_start = conds['timestep_start']\n                if timestep_in[0] > timestep_start:\n                    return None\n            if 'timestep_end' in conds:\n                timestep_end = conds['timestep_end']",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "simple_scheduler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def simple_scheduler(model, steps):\n    s = model.model_sampling\n    sigs = []\n    ss = len(s.sigmas) / steps\n    for x in range(steps):\n        sigs += [float(s.sigmas[-(1 + int(x * ss))])]\n    sigs += [0.0]\n    return torch.FloatTensor(sigs)\ndef ddim_scheduler(model, steps):\n    s = model.model_sampling",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "ddim_scheduler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def ddim_scheduler(model, steps):\n    s = model.model_sampling\n    sigs = []\n    ss = len(s.sigmas) // steps\n    x = 1\n    while x < len(s.sigmas):\n        sigs += [float(s.sigmas[x])]\n        x += ss\n    sigs = sigs[::-1]\n    sigs += [0.0]",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "normal_scheduler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def normal_scheduler(model, steps, sgm=False, floor=False):\n    s = model.model_sampling\n    start = s.timestep(s.sigma_max)\n    end = s.timestep(s.sigma_min)\n    if sgm:\n        timesteps = torch.linspace(start, end, steps + 1)[:-1]\n    else:\n        timesteps = torch.linspace(start, end, steps)\n    sigs = []\n    for x in range(len(timesteps)):",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "get_mask_aabb",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def get_mask_aabb(masks):\n    if masks.numel() == 0:\n        return torch.zeros((0, 4), device=masks.device, dtype=torch.int)\n    b = masks.shape[0]\n    bounding_boxes = torch.zeros((b, 4), device=masks.device, dtype=torch.int)\n    is_empty = torch.zeros((b), device=masks.device, dtype=torch.bool)\n    for i in range(b):\n        mask = masks[i]\n        if mask.numel() == 0:\n            continue",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "resolve_areas_and_cond_masks",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def resolve_areas_and_cond_masks(conditions, h, w, device):\n    # We need to decide on an area outside the sampling loop in order to properly generate opposite areas of equal sizes.\n    # While we're doing this, we can also resolve the mask device and scaling for performance reasons\n    for i in range(len(conditions)):\n        c = conditions[i]\n        if 'area' in c:\n            area = c['area']\n            if area[0] == \"percentage\":\n                modified = c.copy()\n                area = (max(1, round(area[1] * h)), max(1, round(area[2] * w)), round(area[3] * h), round(area[4] * w))",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "create_cond_with_same_area_if_none",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def create_cond_with_same_area_if_none(conds, c):\n    if 'area' not in c:\n        return\n    c_area = c['area']\n    smallest = None\n    for x in conds:\n        if 'area' in x:\n            a = x['area']\n            if c_area[2] >= a[2] and c_area[3] >= a[3]:\n                if a[0] + a[2] >= c_area[0] + c_area[2]:",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "calculate_start_end_timesteps",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def calculate_start_end_timesteps(model, conds):\n    s = model.model_sampling\n    for t in range(len(conds)):\n        x = conds[t]\n        timestep_start = None\n        timestep_end = None\n        if 'start_percent' in x:\n            timestep_start = s.percent_to_sigma(x['start_percent'])\n        if 'end_percent' in x:\n            timestep_end = s.percent_to_sigma(x['end_percent'])",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "pre_run_control",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def pre_run_control(model, conds):\n    s = model.model_sampling\n    for t in range(len(conds)):\n        x = conds[t]\n        timestep_start = None\n        timestep_end = None\n        percent_to_timestep_function = lambda a: s.percent_to_sigma(a)\n        if 'control' in x:\n            x['control'].pre_run(model, percent_to_timestep_function)\ndef apply_empty_x_to_equal_area(conds, uncond, name, uncond_fill_func):",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "apply_empty_x_to_equal_area",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def apply_empty_x_to_equal_area(conds, uncond, name, uncond_fill_func):\n    cond_cnets = []\n    cond_other = []\n    uncond_cnets = []\n    uncond_other = []\n    for t in range(len(conds)):\n        x = conds[t]\n        if 'area' not in x:\n            if name in x and x[name] is not None:\n                cond_cnets.append(x[name])",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "encode_model_conds",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def encode_model_conds(model_function, conds, noise, device, prompt_type, **kwargs):\n    for t in range(len(conds)):\n        x = conds[t]\n        params = x.copy()\n        params[\"device\"] = device\n        params[\"noise\"] = noise\n        params[\"width\"] = params.get(\"width\", noise.shape[3] * 8)\n        params[\"height\"] = params.get(\"height\", noise.shape[2] * 8)\n        params[\"prompt_type\"] = params.get(\"prompt_type\", prompt_type)\n        for k in kwargs:",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "ksampler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def ksampler(sampler_name, extra_options={}, inpaint_options={}):\n    if sampler_name == \"dpm_fast\":\n        def dpm_fast_function(model, noise, sigmas, extra_args, callback, disable):\n            sigma_min = sigmas[-1]\n            if sigma_min == 0:\n                sigma_min = sigmas[-2]\n            total_steps = len(sigmas) - 1\n            return k_diffusion_sampling.sample_dpm_fast(model, noise, sigma_min, sigmas[0], total_steps, extra_args=extra_args, callback=callback, disable=disable)\n        sampler_function = dpm_fast_function\n    elif sampler_name == \"dpm_adaptive\":",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "wrap_model",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def wrap_model(model):\n    model_denoise = CFGNoisePredictor(model)\n    return model_denoise\ndef sample(model, noise, positive, negative, cfg, device, sampler, sigmas, model_options={}, latent_image=None, denoise_mask=None, callback=None, disable_pbar=False, seed=None):\n    positive = positive[:]\n    negative = negative[:]\n    resolve_areas_and_cond_masks(positive, noise.shape[2], noise.shape[3], device)\n    resolve_areas_and_cond_masks(negative, noise.shape[2], noise.shape[3], device)\n    model_wrap = wrap_model(model)\n    calculate_start_end_timesteps(model, negative)",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "sample",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def sample(model, noise, positive, negative, cfg, device, sampler, sigmas, model_options={}, latent_image=None, denoise_mask=None, callback=None, disable_pbar=False, seed=None):\n    positive = positive[:]\n    negative = negative[:]\n    resolve_areas_and_cond_masks(positive, noise.shape[2], noise.shape[3], device)\n    resolve_areas_and_cond_masks(negative, noise.shape[2], noise.shape[3], device)\n    model_wrap = wrap_model(model)\n    calculate_start_end_timesteps(model, negative)\n    calculate_start_end_timesteps(model, positive)\n    #make sure each cond area has an opposite one with the same area\n    for c in positive:",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "calculate_sigmas_scheduler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def calculate_sigmas_scheduler(model, scheduler_name, steps):\n    if scheduler_name == \"karras\":\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"exponential\":\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"normal\":\n        sigmas = normal_scheduler(model, steps)\n    elif scheduler_name == \"simple\":\n        sigmas = simple_scheduler(model, steps)\n    elif scheduler_name == \"ddim_uniform\":",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "sampler_object",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "def sampler_object(name):\n    if name == \"uni_pc\":\n        sampler = UNIPC()\n    elif name == \"uni_pc_bh2\":\n        sampler = UNIPCBH2()\n    elif name == \"ddim\":\n        sampler = ksampler(\"euler\", inpaint_options={\"random\": True})\n    else:\n        sampler = ksampler(name)\n    return sampler",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "KSAMPLER_NAMES",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "KSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nclass KSAMPLER(Sampler):\n    def __init__(self, sampler_function, extra_options={}, inpaint_options={}):\n        self.sampler_function = sampler_function\n        self.extra_options = extra_options\n        self.inpaint_options = inpaint_options\n    def sample(self, model_wrap, sigmas, extra_args, callback, noise, latent_image=None, denoise_mask=None, disable_pbar=False):\n        extra_args[\"denoise_mask\"] = denoise_mask",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "SCHEDULER_NAMES",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "SCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\ndef calculate_sigmas_scheduler(model, scheduler_name, steps):\n    if scheduler_name == \"karras\":\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"exponential\":\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"normal\":\n        sigmas = normal_scheduler(model, steps)\n    elif scheduler_name == \"simple\":",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "SAMPLER_NAMES",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.samplers",
        "description": "Fooocus.backend.headless.fcbh.samplers",
        "peekOfCode": "SAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\ndef calculate_sigmas_scheduler(model, scheduler_name, steps):\n    if scheduler_name == \"karras\":\n        sigmas = k_diffusion_sampling.get_sigmas_karras(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"exponential\":\n        sigmas = k_diffusion_sampling.get_sigmas_exponential(n=steps, sigma_min=float(model.model_sampling.sigma_min), sigma_max=float(model.model_sampling.sigma_max))\n    elif scheduler_name == \"normal\":\n        sigmas = normal_scheduler(model, steps)\n    elif scheduler_name == \"simple\":\n        sigmas = simple_scheduler(model, steps)",
        "detail": "Fooocus.backend.headless.fcbh.samplers",
        "documentation": {}
    },
    {
        "label": "CLIP",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "class CLIP:\n    def __init__(self, target=None, embedding_directory=None, no_init=False):\n        if no_init:\n            return\n        params = target.params.copy()\n        clip = target.clip\n        tokenizer = target.tokenizer\n        load_device = model_management.text_encoder_device()\n        offload_device = model_management.text_encoder_offload_device()\n        params['device'] = offload_device",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "VAE",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "class VAE:\n    def __init__(self, sd=None, device=None, config=None):\n        if 'decoder.up_blocks.0.resnets.0.norm1.weight' in sd.keys(): #diffusers format\n            sd = diffusers_convert.convert_vae_state_dict(sd)\n        self.memory_used_encode = lambda shape, dtype: (1767 * shape[2] * shape[3]) * model_management.dtype_size(dtype) #These are for AutoencoderKL and need tweaking (should be lower)\n        self.memory_used_decode = lambda shape, dtype: (2178 * shape[2] * shape[3] * 64) * model_management.dtype_size(dtype)\n        if config is None:\n            if \"taesd_decoder.1.weight\" in sd:\n                self.first_stage_model = fcbh.taesd.taesd.TAESD()\n            else:",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "StyleModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "class StyleModel:\n    def __init__(self, model, device=\"cpu\"):\n        self.model = model\n    def get_cond(self, input):\n        return self.model(input.last_hidden_state)\ndef load_style_model(ckpt_path):\n    model_data = fcbh.utils.load_torch_file(ckpt_path, safe_load=True)\n    keys = model_data.keys()\n    if \"style_embedding\" in keys:\n        model = fcbh.t2i_adapter.adapter.StyleAdapter(width=1024, context_dim=768, num_head=8, n_layes=3, num_token=8)",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_model_weights",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_model_weights(model, sd):\n    m, u = model.load_state_dict(sd, strict=False)\n    m = set(m)\n    unexpected_keys = set(u)\n    k = list(sd.keys())\n    for x in k:\n        if x not in unexpected_keys:\n            w = sd.pop(x)\n            del w\n    if len(m) > 0:",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_clip_weights",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_clip_weights(model, sd):\n    k = list(sd.keys())\n    for x in k:\n        if x.startswith(\"cond_stage_model.transformer.\") and not x.startswith(\"cond_stage_model.transformer.text_model.\"):\n            y = x.replace(\"cond_stage_model.transformer.\", \"cond_stage_model.transformer.text_model.\")\n            sd[y] = sd.pop(x)\n    if 'cond_stage_model.transformer.text_model.embeddings.position_ids' in sd:\n        ids = sd['cond_stage_model.transformer.text_model.embeddings.position_ids']\n        if ids.dtype == torch.float32:\n            sd['cond_stage_model.transformer.text_model.embeddings.position_ids'] = ids.round()",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_lora_for_models",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_lora_for_models(model, clip, lora, strength_model, strength_clip):\n    key_map = {}\n    if model is not None:\n        key_map = fcbh.lora.model_lora_keys_unet(model.model, key_map)\n    if clip is not None:\n        key_map = fcbh.lora.model_lora_keys_clip(clip.cond_stage_model, key_map)\n    loaded = fcbh.lora.load_lora(lora, key_map)\n    if model is not None:\n        new_modelpatcher = model.clone()\n        k = new_modelpatcher.add_patches(loaded, strength_model)",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_style_model",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_style_model(ckpt_path):\n    model_data = fcbh.utils.load_torch_file(ckpt_path, safe_load=True)\n    keys = model_data.keys()\n    if \"style_embedding\" in keys:\n        model = fcbh.t2i_adapter.adapter.StyleAdapter(width=1024, context_dim=768, num_head=8, n_layes=3, num_token=8)\n    else:\n        raise Exception(\"invalid style model {}\".format(ckpt_path))\n    model.load_state_dict(model_data)\n    return StyleModel(model)\ndef load_clip(ckpt_paths, embedding_directory=None):",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_clip",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_clip(ckpt_paths, embedding_directory=None):\n    clip_data = []\n    for p in ckpt_paths:\n        clip_data.append(fcbh.utils.load_torch_file(p, safe_load=True))\n    class EmptyClass:\n        pass\n    for i in range(len(clip_data)):\n        if \"transformer.resblocks.0.ln_1.weight\" in clip_data[i]:\n            clip_data[i] = fcbh.utils.transformers_convert(clip_data[i], \"\", \"text_model.\", 32)\n    clip_target = EmptyClass()",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_gligen",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_gligen(ckpt_path):\n    data = fcbh.utils.load_torch_file(ckpt_path, safe_load=True)\n    model = gligen.load_gligen(data)\n    if model_management.should_use_fp16():\n        model = model.half()\n    return fcbh.model_patcher.ModelPatcher(model, load_device=model_management.get_torch_device(), offload_device=model_management.unet_offload_device())\ndef load_checkpoint(config_path=None, ckpt_path=None, output_vae=True, output_clip=True, embedding_directory=None, state_dict=None, config=None):\n    #TODO: this function is a mess and should be removed eventually\n    if config is None:\n        with open(config_path, 'r') as stream:",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_checkpoint(config_path=None, ckpt_path=None, output_vae=True, output_clip=True, embedding_directory=None, state_dict=None, config=None):\n    #TODO: this function is a mess and should be removed eventually\n    if config is None:\n        with open(config_path, 'r') as stream:\n            config = yaml.safe_load(stream)\n    model_config_params = config['model']['params']\n    clip_config = model_config_params['cond_stage_config']\n    scale_factor = model_config_params['scale_factor']\n    vae_config = model_config_params['first_stage_config']\n    fp16 = False",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_checkpoint_guess_config",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, output_clipvision=False, embedding_directory=None, output_model=True):\n    sd = fcbh.utils.load_torch_file(ckpt_path)\n    sd_keys = sd.keys()\n    clip = None\n    clipvision = None\n    vae = None\n    model = None\n    model_patcher = None\n    clip_target = None\n    parameters = fcbh.utils.calculate_parameters(sd, \"model.diffusion_model.\")",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "load_unet",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def load_unet(unet_path): #load unet in diffusers format\n    sd = fcbh.utils.load_torch_file(unet_path)\n    parameters = fcbh.utils.calculate_parameters(sd)\n    unet_dtype = model_management.unet_dtype(model_params=parameters)\n    if \"input_blocks.0.0.weight\" in sd: #ldm\n        model_config = model_detection.model_config_from_unet(sd, \"\", unet_dtype)\n        if model_config is None:\n            raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(unet_path))\n        new_sd = sd\n    else: #diffusers",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd",
        "description": "Fooocus.backend.headless.fcbh.sd",
        "peekOfCode": "def save_checkpoint(output_path, model, clip, vae, metadata=None):\n    model_management.load_models_gpu([model, clip.load_model()])\n    sd = model.model.state_dict_for_saving(clip.get_sd(), vae.get_sd())\n    fcbh.utils.save_torch_file(sd, output_path, metadata=metadata)",
        "detail": "Fooocus.backend.headless.fcbh.sd",
        "documentation": {}
    },
    {
        "label": "ClipTokenWeightEncoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "class ClipTokenWeightEncoder:\n    def encode_token_weights(self, token_weight_pairs):\n        to_encode = list()\n        max_token_len = 0\n        has_weights = False\n        for x in token_weight_pairs:\n            tokens = list(map(lambda a: a[0], x))\n            max_token_len = max(len(tokens), max_token_len)\n            has_weights = has_weights or not all(map(lambda a: a[1] == 1.0, x))\n            to_encode.append(tokens)",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "SDClipModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "class SDClipModel(torch.nn.Module, ClipTokenWeightEncoder):\n    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n    LAYERS = [\n        \"last\",\n        \"pooled\",\n        \"hidden\"\n    ]\n    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cpu\", max_length=77,\n                 freeze=True, layer=\"last\", layer_idx=None, textmodel_json_config=None, textmodel_path=None, dtype=None,\n                 special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 49407},layer_norm_hidden_state=True, config_class=CLIPTextConfig,",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "SDTokenizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "class SDTokenizer:\n    def __init__(self, tokenizer_path=None, max_length=77, pad_with_end=True, embedding_directory=None, embedding_size=768, embedding_key='clip_l', tokenizer_class=CLIPTokenizer, has_start_token=True, pad_to_max_length=True):\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd1_tokenizer\")\n        self.tokenizer = tokenizer_class.from_pretrained(tokenizer_path)\n        self.max_length = max_length\n        empty = self.tokenizer('')[\"input_ids\"]\n        if has_start_token:\n            self.tokens_start = 1\n            self.start_token = empty[0]",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "SD1Tokenizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "class SD1Tokenizer:\n    def __init__(self, embedding_directory=None, clip_name=\"l\", tokenizer=SDTokenizer):\n        self.clip_name = clip_name\n        self.clip = \"clip_{}\".format(self.clip_name)\n        setattr(self, self.clip, tokenizer(embedding_directory=embedding_directory))\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[self.clip_name] = getattr(self, self.clip).tokenize_with_weights(text, return_word_ids)\n        return out\n    def untokenize(self, token_weight_pair):",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "SD1ClipModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "class SD1ClipModel(torch.nn.Module):\n    def __init__(self, device=\"cpu\", dtype=None, clip_name=\"l\", clip_model=SDClipModel, **kwargs):\n        super().__init__()\n        self.clip_name = clip_name\n        self.clip = \"clip_{}\".format(self.clip_name)\n        setattr(self, self.clip, clip_model(device=device, dtype=dtype, **kwargs))\n    def clip_layer(self, layer_idx):\n        getattr(self, self.clip).clip_layer(layer_idx)\n    def reset_clip_layer(self):\n        getattr(self, self.clip).reset_clip_layer()",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "gen_empty_tokens",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def gen_empty_tokens(special_tokens, length):\n    start_token = special_tokens.get(\"start\", None)\n    end_token = special_tokens.get(\"end\", None)\n    pad_token = special_tokens.get(\"pad\")\n    output = []\n    if start_token is not None:\n        output.append(start_token)\n    if end_token is not None:\n        output.append(end_token)\n    output += [pad_token] * (length - len(output))",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "parse_parentheses",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def parse_parentheses(string):\n    result = []\n    current_item = \"\"\n    nesting_level = 0\n    for char in string:\n        if char == \"(\":\n            if nesting_level == 0:\n                if current_item:\n                    result.append(current_item)\n                    current_item = \"(\"",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "token_weights",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def token_weights(string, current_weight):\n    a = parse_parentheses(string)\n    out = []\n    for x in a:\n        weight = current_weight\n        if len(x) >= 2 and x[-1] == ')' and x[0] == '(':\n            x = x[1:-1]\n            xx = x.rfind(\":\")\n            weight *= 1.1\n            if xx > 0:",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "escape_important",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def escape_important(text):\n    text = text.replace(\"\\\\)\", \"\\0\\1\")\n    text = text.replace(\"\\\\(\", \"\\0\\2\")\n    return text\ndef unescape_important(text):\n    text = text.replace(\"\\0\\1\", \")\")\n    text = text.replace(\"\\0\\2\", \"(\")\n    return text\ndef safe_load_embed_zip(embed_path):\n    with zipfile.ZipFile(embed_path) as myzip:",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "unescape_important",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def unescape_important(text):\n    text = text.replace(\"\\0\\1\", \")\")\n    text = text.replace(\"\\0\\2\", \"(\")\n    return text\ndef safe_load_embed_zip(embed_path):\n    with zipfile.ZipFile(embed_path) as myzip:\n        names = list(filter(lambda a: \"data/\" in a, myzip.namelist()))\n        names.reverse()\n        for n in names:\n            with myzip.open(n) as myfile:",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "safe_load_embed_zip",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def safe_load_embed_zip(embed_path):\n    with zipfile.ZipFile(embed_path) as myzip:\n        names = list(filter(lambda a: \"data/\" in a, myzip.namelist()))\n        names.reverse()\n        for n in names:\n            with myzip.open(n) as myfile:\n                data = myfile.read()\n                number = len(data) // 4\n                length_embed = 1024 #sd2.x\n                if number < 768:",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "expand_directory_list",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def expand_directory_list(directories):\n    dirs = set()\n    for x in directories:\n        dirs.add(x)\n        for root, subdir, file in os.walk(x, followlinks=True):\n            dirs.add(root)\n    return list(dirs)\ndef load_embed(embedding_name, embedding_directory, embedding_size, embed_key=None):\n    if isinstance(embedding_directory, str):\n        embedding_directory = [embedding_directory]",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "load_embed",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.sd1_clip",
        "description": "Fooocus.backend.headless.fcbh.sd1_clip",
        "peekOfCode": "def load_embed(embedding_name, embedding_directory, embedding_size, embed_key=None):\n    if isinstance(embedding_directory, str):\n        embedding_directory = [embedding_directory]\n    embedding_directory = expand_directory_list(embedding_directory)\n    valid_file = None\n    for embed_dir in embedding_directory:\n        embed_path = os.path.abspath(os.path.join(embed_dir, embedding_name))\n        embed_dir = os.path.abspath(embed_dir)\n        try:\n            if os.path.commonpath((embed_dir, embed_path)) != embed_dir:",
        "detail": "Fooocus.backend.headless.fcbh.sd1_clip",
        "documentation": {}
    },
    {
        "label": "SD2ClipHModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd2_clip",
        "description": "Fooocus.backend.headless.fcbh.sd2_clip",
        "peekOfCode": "class SD2ClipHModel(sd1_clip.SDClipModel):\n    def __init__(self, arch=\"ViT-H-14\", device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\", layer_idx=None, textmodel_path=None, dtype=None):\n        if layer == \"penultimate\":\n            layer=\"hidden\"\n            layer_idx=23\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"sd2_clip_config.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, textmodel_path=textmodel_path, dtype=dtype, special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 0})\nclass SD2ClipHTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1024)",
        "detail": "Fooocus.backend.headless.fcbh.sd2_clip",
        "documentation": {}
    },
    {
        "label": "SD2ClipHTokenizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd2_clip",
        "description": "Fooocus.backend.headless.fcbh.sd2_clip",
        "peekOfCode": "class SD2ClipHTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1024)\nclass SD2Tokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"h\", tokenizer=SD2ClipHTokenizer)\nclass SD2ClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"h\", clip_model=SD2ClipHModel, **kwargs)",
        "detail": "Fooocus.backend.headless.fcbh.sd2_clip",
        "documentation": {}
    },
    {
        "label": "SD2Tokenizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd2_clip",
        "description": "Fooocus.backend.headless.fcbh.sd2_clip",
        "peekOfCode": "class SD2Tokenizer(sd1_clip.SD1Tokenizer):\n    def __init__(self, embedding_directory=None):\n        super().__init__(embedding_directory=embedding_directory, clip_name=\"h\", tokenizer=SD2ClipHTokenizer)\nclass SD2ClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"h\", clip_model=SD2ClipHModel, **kwargs)",
        "detail": "Fooocus.backend.headless.fcbh.sd2_clip",
        "documentation": {}
    },
    {
        "label": "SD2ClipModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sd2_clip",
        "description": "Fooocus.backend.headless.fcbh.sd2_clip",
        "peekOfCode": "class SD2ClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None, **kwargs):\n        super().__init__(device=device, dtype=dtype, clip_name=\"h\", clip_model=SD2ClipHModel, **kwargs)",
        "detail": "Fooocus.backend.headless.fcbh.sd2_clip",
        "documentation": {}
    },
    {
        "label": "SDXLClipG",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "description": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "peekOfCode": "class SDXLClipG(sd1_clip.SDClipModel):\n    def __init__(self, device=\"cpu\", max_length=77, freeze=True, layer=\"penultimate\", layer_idx=None, textmodel_path=None, dtype=None):\n        if layer == \"penultimate\":\n            layer=\"hidden\"\n            layer_idx=-2\n        textmodel_json_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"clip_config_bigg.json\")\n        super().__init__(device=device, freeze=freeze, layer=layer, layer_idx=layer_idx, textmodel_json_config=textmodel_json_config, textmodel_path=textmodel_path, dtype=dtype,\n                         special_tokens={\"start\": 49406, \"end\": 49407, \"pad\": 0}, layer_norm_hidden_state=False)\n    def load_sd(self, sd):\n        return super().load_sd(sd)",
        "detail": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "documentation": {}
    },
    {
        "label": "SDXLClipGTokenizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "description": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "peekOfCode": "class SDXLClipGTokenizer(sd1_clip.SDTokenizer):\n    def __init__(self, tokenizer_path=None, embedding_directory=None):\n        super().__init__(tokenizer_path, pad_with_end=False, embedding_directory=embedding_directory, embedding_size=1280, embedding_key='clip_g')\nclass SDXLTokenizer:\n    def __init__(self, embedding_directory=None):\n        self.clip_l = sd1_clip.SDTokenizer(embedding_directory=embedding_directory)\n        self.clip_g = SDXLClipGTokenizer(embedding_directory=embedding_directory)\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[\"g\"] = self.clip_g.tokenize_with_weights(text, return_word_ids)",
        "detail": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "documentation": {}
    },
    {
        "label": "SDXLTokenizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "description": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "peekOfCode": "class SDXLTokenizer:\n    def __init__(self, embedding_directory=None):\n        self.clip_l = sd1_clip.SDTokenizer(embedding_directory=embedding_directory)\n        self.clip_g = SDXLClipGTokenizer(embedding_directory=embedding_directory)\n    def tokenize_with_weights(self, text:str, return_word_ids=False):\n        out = {}\n        out[\"g\"] = self.clip_g.tokenize_with_weights(text, return_word_ids)\n        out[\"l\"] = self.clip_l.tokenize_with_weights(text, return_word_ids)\n        return out\n    def untokenize(self, token_weight_pair):",
        "detail": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "documentation": {}
    },
    {
        "label": "SDXLClipModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "description": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "peekOfCode": "class SDXLClipModel(torch.nn.Module):\n    def __init__(self, device=\"cpu\", dtype=None):\n        super().__init__()\n        self.clip_l = sd1_clip.SDClipModel(layer=\"hidden\", layer_idx=11, device=device, dtype=dtype, layer_norm_hidden_state=False)\n        self.clip_g = SDXLClipG(device=device, dtype=dtype)\n    def clip_layer(self, layer_idx):\n        self.clip_l.clip_layer(layer_idx)\n        self.clip_g.clip_layer(layer_idx)\n    def reset_clip_layer(self):\n        self.clip_g.reset_clip_layer()",
        "detail": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "documentation": {}
    },
    {
        "label": "SDXLRefinerClipModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "description": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "peekOfCode": "class SDXLRefinerClipModel(sd1_clip.SD1ClipModel):\n    def __init__(self, device=\"cpu\", dtype=None):\n        super().__init__(device=device, dtype=dtype, clip_name=\"g\", clip_model=SDXLClipG)",
        "detail": "Fooocus.backend.headless.fcbh.sdxl_clip",
        "documentation": {}
    },
    {
        "label": "SD15",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "class SD15(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 768,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": False,\n        \"adm_in_channels\": None,\n    }\n    unet_extra_config = {\n        \"num_heads\": 8,\n        \"num_head_channels\": -1,",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "SD20",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "class SD20(supported_models_base.BASE):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": None,\n    }\n    latent_format = latent_formats.SD15\n    def model_type(self, state_dict, prefix=\"\"):\n        if self.unet_config[\"in_channels\"] == 4: #SD2.0 inpainting models are not v prediction",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "SD21UnclipL",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "class SD21UnclipL(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": 1536,\n    }\n    clip_vision_prefix = \"embedder.model.visual.\"\n    noise_aug_config = {\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 768}\nclass SD21UnclipH(SD20):",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "SD21UnclipH",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "class SD21UnclipH(SD20):\n    unet_config = {\n        \"context_dim\": 1024,\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"adm_in_channels\": 2048,\n    }\n    clip_vision_prefix = \"embedder.model.visual.\"\n    noise_aug_config = {\"noise_schedule_config\": {\"timesteps\": 1000, \"beta_schedule\": \"squaredcos_cap_v2\"}, \"timestep_dim\": 1024}\nclass SDXLRefiner(supported_models_base.BASE):",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "SDXLRefiner",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "class SDXLRefiner(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 384,\n        \"use_linear_in_transformer\": True,\n        \"context_dim\": 1280,\n        \"adm_in_channels\": 2560,\n        \"transformer_depth\": [0, 0, 4, 4, 4, 4, 0, 0],\n    }\n    latent_format = latent_formats.SDXL\n    def get_model(self, state_dict, prefix=\"\", device=None):",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "SDXL",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "class SDXL(supported_models_base.BASE):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 2, 2, 10, 10],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816\n    }\n    latent_format = latent_formats.SDXL\n    def model_type(self, state_dict, prefix=\"\"):",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "SSD1B",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "class SSD1B(SDXL):\n    unet_config = {\n        \"model_channels\": 320,\n        \"use_linear_in_transformer\": True,\n        \"transformer_depth\": [0, 0, 2, 2, 4, 4],\n        \"context_dim\": 2048,\n        \"adm_in_channels\": 2816\n    }\nmodels = [SD15, SD20, SD21UnclipL, SD21UnclipH, SDXLRefiner, SDXL, SSD1B]",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models",
        "description": "Fooocus.backend.headless.fcbh.supported_models",
        "peekOfCode": "models = [SD15, SD20, SD21UnclipL, SD21UnclipH, SDXLRefiner, SDXL, SSD1B]",
        "detail": "Fooocus.backend.headless.fcbh.supported_models",
        "documentation": {}
    },
    {
        "label": "ClipTarget",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models_base",
        "description": "Fooocus.backend.headless.fcbh.supported_models_base",
        "peekOfCode": "class ClipTarget:\n    def __init__(self, tokenizer, clip):\n        self.clip = clip\n        self.tokenizer = tokenizer\n        self.params = {}\nclass BASE:\n    unet_config = {}\n    unet_extra_config = {\n        \"num_heads\": -1,\n        \"num_head_channels\": 64,",
        "detail": "Fooocus.backend.headless.fcbh.supported_models_base",
        "documentation": {}
    },
    {
        "label": "BASE",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.supported_models_base",
        "description": "Fooocus.backend.headless.fcbh.supported_models_base",
        "peekOfCode": "class BASE:\n    unet_config = {}\n    unet_extra_config = {\n        \"num_heads\": -1,\n        \"num_head_channels\": 64,\n    }\n    clip_prefix = []\n    clip_vision_prefix = None\n    noise_aug_config = None\n    sampling_settings = {}",
        "detail": "Fooocus.backend.headless.fcbh.supported_models_base",
        "documentation": {}
    },
    {
        "label": "ProgressBar",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "class ProgressBar:\n    def __init__(self, total):\n        global PROGRESS_BAR_HOOK\n        self.total = total\n        self.current = 0\n        self.hook = PROGRESS_BAR_HOOK\n    def update_absolute(self, value, total=None, preview=None):\n        if total is not None:\n            self.total = total\n        if value > self.total:",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "load_torch_file",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def load_torch_file(ckpt, safe_load=False, device=None):\n    if device is None:\n        device = torch.device(\"cpu\")\n    if ckpt.lower().endswith(\".safetensors\"):\n        sd = safetensors.torch.load_file(ckpt, device=device.type)\n    else:\n        if safe_load:\n            if not 'weights_only' in torch.load.__code__.co_varnames:\n                print(\"Warning torch.load doesn't support weights_only on this pytorch version, loading unsafely.\")\n                safe_load = False",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "save_torch_file",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def save_torch_file(sd, ckpt, metadata=None):\n    if metadata is not None:\n        safetensors.torch.save_file(sd, ckpt, metadata=metadata)\n    else:\n        safetensors.torch.save_file(sd, ckpt)\ndef calculate_parameters(sd, prefix=\"\"):\n    params = 0\n    for k in sd.keys():\n        if k.startswith(prefix):\n            params += sd[k].nelement()",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "calculate_parameters",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def calculate_parameters(sd, prefix=\"\"):\n    params = 0\n    for k in sd.keys():\n        if k.startswith(prefix):\n            params += sd[k].nelement()\n    return params\ndef state_dict_key_replace(state_dict, keys_to_replace):\n    for x in keys_to_replace:\n        if x in state_dict:\n            state_dict[keys_to_replace[x]] = state_dict.pop(x)",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "state_dict_key_replace",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def state_dict_key_replace(state_dict, keys_to_replace):\n    for x in keys_to_replace:\n        if x in state_dict:\n            state_dict[keys_to_replace[x]] = state_dict.pop(x)\n    return state_dict\ndef state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=False):\n    if filter_keys:\n        out = {}\n    else:\n        out = state_dict",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "state_dict_prefix_replace",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def state_dict_prefix_replace(state_dict, replace_prefix, filter_keys=False):\n    if filter_keys:\n        out = {}\n    else:\n        out = state_dict\n    for rp in replace_prefix:\n        replace = list(map(lambda a: (a, \"{}{}\".format(replace_prefix[rp], a[len(rp):])), filter(lambda a: a.startswith(rp), state_dict.keys())))\n        for x in replace:\n            w = state_dict.pop(x[0])\n            out[x[1]] = w",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "transformers_convert",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def transformers_convert(sd, prefix_from, prefix_to, number):\n    keys_to_replace = {\n        \"{}positional_embedding\": \"{}embeddings.position_embedding.weight\",\n        \"{}token_embedding.weight\": \"{}embeddings.token_embedding.weight\",\n        \"{}ln_final.weight\": \"{}final_layer_norm.weight\",\n        \"{}ln_final.bias\": \"{}final_layer_norm.bias\",\n    }\n    for k in keys_to_replace:\n        x = k.format(prefix_from)\n        if x in sd:",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "unet_to_diffusers",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def unet_to_diffusers(unet_config):\n    num_res_blocks = unet_config[\"num_res_blocks\"]\n    channel_mult = unet_config[\"channel_mult\"]\n    transformer_depth = unet_config[\"transformer_depth\"][:]\n    transformer_depth_output = unet_config[\"transformer_depth_output\"][:]\n    num_blocks = len(channel_mult)\n    transformers_mid = unet_config.get(\"transformer_depth_middle\", None)\n    diffusers_unet_map = {}\n    for x in range(num_blocks):\n        n = 1 + (num_res_blocks[x] + 1) * x",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "repeat_to_batch_size",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def repeat_to_batch_size(tensor, batch_size):\n    if tensor.shape[0] > batch_size:\n        return tensor[:batch_size]\n    elif tensor.shape[0] < batch_size:\n        return tensor.repeat([math.ceil(batch_size / tensor.shape[0])] + [1] * (len(tensor.shape) - 1))[:batch_size]\n    return tensor\ndef convert_sd_to(state_dict, dtype):\n    keys = list(state_dict.keys())\n    for k in keys:\n        state_dict[k] = state_dict[k].to(dtype)",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "convert_sd_to",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def convert_sd_to(state_dict, dtype):\n    keys = list(state_dict.keys())\n    for k in keys:\n        state_dict[k] = state_dict[k].to(dtype)\n    return state_dict\ndef safetensors_header(safetensors_path, max_size=100*1024*1024):\n    with open(safetensors_path, \"rb\") as f:\n        header = f.read(8)\n        length_of_header = struct.unpack('<Q', header)[0]\n        if length_of_header > max_size:",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "safetensors_header",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def safetensors_header(safetensors_path, max_size=100*1024*1024):\n    with open(safetensors_path, \"rb\") as f:\n        header = f.read(8)\n        length_of_header = struct.unpack('<Q', header)[0]\n        if length_of_header > max_size:\n            return None\n        return f.read(length_of_header)\ndef set_attr(obj, attr, value):\n    attrs = attr.split(\".\")\n    for name in attrs[:-1]:",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "set_attr",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def set_attr(obj, attr, value):\n    attrs = attr.split(\".\")\n    for name in attrs[:-1]:\n        obj = getattr(obj, name)\n    prev = getattr(obj, attrs[-1])\n    setattr(obj, attrs[-1], torch.nn.Parameter(value, requires_grad=False))\n    del prev\ndef copy_to_param(obj, attr, value):\n    # inplace update tensor instead of replacing it\n    attrs = attr.split(\".\")",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "copy_to_param",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def copy_to_param(obj, attr, value):\n    # inplace update tensor instead of replacing it\n    attrs = attr.split(\".\")\n    for name in attrs[:-1]:\n        obj = getattr(obj, name)\n    prev = getattr(obj, attrs[-1])\n    prev.data.copy_(value)\ndef get_attr(obj, attr):\n    attrs = attr.split(\".\")\n    for name in attrs:",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "get_attr",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def get_attr(obj, attr):\n    attrs = attr.split(\".\")\n    for name in attrs:\n        obj = getattr(obj, name)\n    return obj\ndef bislerp(samples, width, height):\n    def slerp(b1, b2, r):\n        '''slerps batches b1, b2 according to ratio r, batches should be flat e.g. NxC'''\n        c = b1.shape[-1]\n        #norms",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "bislerp",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def bislerp(samples, width, height):\n    def slerp(b1, b2, r):\n        '''slerps batches b1, b2 according to ratio r, batches should be flat e.g. NxC'''\n        c = b1.shape[-1]\n        #norms\n        b1_norms = torch.norm(b1, dim=-1, keepdim=True)\n        b2_norms = torch.norm(b2, dim=-1, keepdim=True)\n        #normalize\n        b1_normalized = b1 / b1_norms\n        b2_normalized = b2 / b2_norms",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "lanczos",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def lanczos(samples, width, height):\n    images = [Image.fromarray(np.clip(255. * image.movedim(0, -1).cpu().numpy(), 0, 255).astype(np.uint8)) for image in samples]\n    images = [image.resize((width, height), resample=Image.Resampling.LANCZOS) for image in images]\n    images = [torch.from_numpy(np.array(image).astype(np.float32) / 255.0).movedim(-1, 0) for image in images]\n    result = torch.stack(images)\n    return result\ndef common_upscale(samples, width, height, upscale_method, crop):\n        if crop == \"center\":\n            old_width = samples.shape[3]\n            old_height = samples.shape[2]",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "common_upscale",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def common_upscale(samples, width, height, upscale_method, crop):\n        if crop == \"center\":\n            old_width = samples.shape[3]\n            old_height = samples.shape[2]\n            old_aspect = old_width / old_height\n            new_aspect = width / height\n            x = 0\n            y = 0\n            if old_aspect > new_aspect:\n                x = round((old_width - old_width * (new_aspect / old_aspect)) / 2)",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "get_tiled_scale_steps",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def get_tiled_scale_steps(width, height, tile_x, tile_y, overlap):\n    return math.ceil((height / (tile_y - overlap))) * math.ceil((width / (tile_x - overlap)))\n@torch.inference_mode()\ndef tiled_scale(samples, function, tile_x=64, tile_y=64, overlap = 8, upscale_amount = 4, out_channels = 3, pbar = None):\n    output = torch.empty((samples.shape[0], out_channels, round(samples.shape[2] * upscale_amount), round(samples.shape[3] * upscale_amount)), device=\"cpu\")\n    for b in range(samples.shape[0]):\n        s = samples[b:b+1]\n        out = torch.zeros((s.shape[0], out_channels, round(s.shape[2] * upscale_amount), round(s.shape[3] * upscale_amount)), device=\"cpu\")\n        out_div = torch.zeros((s.shape[0], out_channels, round(s.shape[2] * upscale_amount), round(s.shape[3] * upscale_amount)), device=\"cpu\")\n        for y in range(0, s.shape[2], tile_y - overlap):",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "tiled_scale",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def tiled_scale(samples, function, tile_x=64, tile_y=64, overlap = 8, upscale_amount = 4, out_channels = 3, pbar = None):\n    output = torch.empty((samples.shape[0], out_channels, round(samples.shape[2] * upscale_amount), round(samples.shape[3] * upscale_amount)), device=\"cpu\")\n    for b in range(samples.shape[0]):\n        s = samples[b:b+1]\n        out = torch.zeros((s.shape[0], out_channels, round(s.shape[2] * upscale_amount), round(s.shape[3] * upscale_amount)), device=\"cpu\")\n        out_div = torch.zeros((s.shape[0], out_channels, round(s.shape[2] * upscale_amount), round(s.shape[3] * upscale_amount)), device=\"cpu\")\n        for y in range(0, s.shape[2], tile_y - overlap):\n            for x in range(0, s.shape[3], tile_x - overlap):\n                s_in = s[:,:,y:y+tile_y,x:x+tile_x]\n                ps = function(s_in).cpu()",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "set_progress_bar_enabled",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def set_progress_bar_enabled(enabled):\n    global PROGRESS_BAR_ENABLED\n    PROGRESS_BAR_ENABLED = enabled\nPROGRESS_BAR_HOOK = None\ndef set_progress_bar_global_hook(function):\n    global PROGRESS_BAR_HOOK\n    PROGRESS_BAR_HOOK = function\nclass ProgressBar:\n    def __init__(self, total):\n        global PROGRESS_BAR_HOOK",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "set_progress_bar_global_hook",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "def set_progress_bar_global_hook(function):\n    global PROGRESS_BAR_HOOK\n    PROGRESS_BAR_HOOK = function\nclass ProgressBar:\n    def __init__(self, total):\n        global PROGRESS_BAR_HOOK\n        self.total = total\n        self.current = 0\n        self.hook = PROGRESS_BAR_HOOK\n    def update_absolute(self, value, total=None, preview=None):",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "UNET_MAP_ATTENTIONS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "UNET_MAP_ATTENTIONS = {\n    \"proj_in.weight\",\n    \"proj_in.bias\",\n    \"proj_out.weight\",\n    \"proj_out.bias\",\n    \"norm.weight\",\n    \"norm.bias\",\n}\nTRANSFORMER_BLOCKS = {\n    \"norm1.weight\",",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "TRANSFORMER_BLOCKS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "TRANSFORMER_BLOCKS = {\n    \"norm1.weight\",\n    \"norm1.bias\",\n    \"norm2.weight\",\n    \"norm2.bias\",\n    \"norm3.weight\",\n    \"norm3.bias\",\n    \"attn1.to_q.weight\",\n    \"attn1.to_k.weight\",\n    \"attn1.to_v.weight\",",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "UNET_MAP_RESNET",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "UNET_MAP_RESNET = {\n    \"in_layers.2.weight\": \"conv1.weight\",\n    \"in_layers.2.bias\": \"conv1.bias\",\n    \"emb_layers.1.weight\": \"time_emb_proj.weight\",\n    \"emb_layers.1.bias\": \"time_emb_proj.bias\",\n    \"out_layers.3.weight\": \"conv2.weight\",\n    \"out_layers.3.bias\": \"conv2.bias\",\n    \"skip_connection.weight\": \"conv_shortcut.weight\",\n    \"skip_connection.bias\": \"conv_shortcut.bias\",\n    \"in_layers.0.weight\": \"norm1.weight\",",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "UNET_MAP_BASIC",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "UNET_MAP_BASIC = {\n    (\"label_emb.0.0.weight\", \"class_embedding.linear_1.weight\"),\n    (\"label_emb.0.0.bias\", \"class_embedding.linear_1.bias\"),\n    (\"label_emb.0.2.weight\", \"class_embedding.linear_2.weight\"),\n    (\"label_emb.0.2.bias\", \"class_embedding.linear_2.bias\"),\n    (\"label_emb.0.0.weight\", \"add_embedding.linear_1.weight\"),\n    (\"label_emb.0.0.bias\", \"add_embedding.linear_1.bias\"),\n    (\"label_emb.0.2.weight\", \"add_embedding.linear_2.weight\"),\n    (\"label_emb.0.2.bias\", \"add_embedding.linear_2.bias\"),\n    (\"input_blocks.0.0.weight\", \"conv_in.weight\"),",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "PROGRESS_BAR_ENABLED",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "PROGRESS_BAR_ENABLED = True\ndef set_progress_bar_enabled(enabled):\n    global PROGRESS_BAR_ENABLED\n    PROGRESS_BAR_ENABLED = enabled\nPROGRESS_BAR_HOOK = None\ndef set_progress_bar_global_hook(function):\n    global PROGRESS_BAR_HOOK\n    PROGRESS_BAR_HOOK = function\nclass ProgressBar:\n    def __init__(self, total):",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "PROGRESS_BAR_HOOK",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh.utils",
        "description": "Fooocus.backend.headless.fcbh.utils",
        "peekOfCode": "PROGRESS_BAR_HOOK = None\ndef set_progress_bar_global_hook(function):\n    global PROGRESS_BAR_HOOK\n    PROGRESS_BAR_HOOK = function\nclass ProgressBar:\n    def __init__(self, total):\n        global PROGRESS_BAR_HOOK\n        self.total = total\n        self.current = 0\n        self.hook = PROGRESS_BAR_HOOK",
        "detail": "Fooocus.backend.headless.fcbh.utils",
        "documentation": {}
    },
    {
        "label": "CA_layer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "peekOfCode": "class CA_layer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(CA_layer, self).__init__()\n        # global average pooling\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Conv2d(channel, channel // reduction, kernel_size=(1, 1), bias=False),\n            nn.GELU(),\n            nn.Conv2d(channel // reduction, channel, kernel_size=(1, 1), bias=False),\n            # nn.Sigmoid()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "documentation": {}
    },
    {
        "label": "Simple_CA_layer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "peekOfCode": "class Simple_CA_layer(nn.Module):\n    def __init__(self, channel):\n        super(Simple_CA_layer, self).__init__()\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(\n            in_channels=channel,\n            out_channels=channel,\n            kernel_size=1,\n            padding=0,\n            stride=1,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "documentation": {}
    },
    {
        "label": "ECA_layer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "peekOfCode": "class ECA_layer(nn.Module):\n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n    \"\"\"\n    def __init__(self, channel):\n        super(ECA_layer, self).__init__()\n        b = 1\n        gamma = 2",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "documentation": {}
    },
    {
        "label": "ECA_MaxPool_layer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "peekOfCode": "class ECA_MaxPool_layer(nn.Module):\n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n    \"\"\"\n    def __init__(self, channel):\n        super(ECA_MaxPool_layer, self).__init__()\n        b = 1\n        gamma = 2",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.ChannelAttention",
        "documentation": {}
    },
    {
        "label": "PreNormResidual",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x\nclass Conv_PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Conv_PreNormResidual",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Conv_PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = LayerNorm2d(dim)\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult=2, dropout=0.0):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, mult=2, dropout=0.0):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        self.net = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout),",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Conv_FeedForward",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Conv_FeedForward(nn.Module):\n    def __init__(self, dim, mult=2, dropout=0.0):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        self.net = nn.Sequential(\n            nn.Conv2d(dim, inner_dim, 1, 1, 0),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Conv2d(inner_dim, dim, 1, 1, 0),\n            nn.Dropout(dropout),",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Gated_Conv_FeedForward",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Gated_Conv_FeedForward(nn.Module):\n    def __init__(self, dim, mult=1, bias=False, dropout=0.0):\n        super().__init__()\n        hidden_features = int(dim * mult)\n        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n        self.dwconv = nn.Conv2d(\n            hidden_features * 2,\n            hidden_features * 2,\n            kernel_size=3,\n            stride=1,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "SqueezeExcitation",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class SqueezeExcitation(nn.Module):\n    def __init__(self, dim, shrinkage_rate=0.25):\n        super().__init__()\n        hidden_dim = int(dim * shrinkage_rate)\n        self.gate = nn.Sequential(\n            Reduce(\"b c h w -> b c\", \"mean\"),\n            nn.Linear(dim, hidden_dim, bias=False),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, dim, bias=False),\n            nn.Sigmoid(),",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "MBConvResidual",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class MBConvResidual(nn.Module):\n    def __init__(self, fn, dropout=0.0):\n        super().__init__()\n        self.fn = fn\n        self.dropsample = Dropsample(dropout)\n    def forward(self, x):\n        out = self.fn(x)\n        out = self.dropsample(out)\n        return out + x\nclass Dropsample(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Dropsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Dropsample(nn.Module):\n    def __init__(self, prob=0):\n        super().__init__()\n        self.prob = prob\n    def forward(self, x):\n        device = x.device\n        if self.prob == 0.0 or (not self.training):\n            return x\n        keep_mask = (\n            torch.FloatTensor((x.shape[0], 1, 1, 1), device=device).uniform_()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=32,\n        dropout=0.0,\n        window_size=7,\n        with_pe=True,\n    ):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Block_Attention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Block_Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=32,\n        bias=False,\n        dropout=0.0,\n        window_size=7,\n        with_pe=True,\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Channel_Attention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Channel_Attention(nn.Module):\n    def __init__(self, dim, heads, bias=False, dropout=0.0, window_size=7):\n        super(Channel_Attention, self).__init__()\n        self.heads = heads\n        self.temperature = nn.Parameter(torch.ones(heads, 1, 1))\n        self.ps = window_size\n        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(\n            dim * 3,\n            dim * 3,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "Channel_Attention_grid",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class Channel_Attention_grid(nn.Module):\n    def __init__(self, dim, heads, bias=False, dropout=0.0, window_size=7):\n        super(Channel_Attention_grid, self).__init__()\n        self.heads = heads\n        self.temperature = nn.Parameter(torch.ones(heads, 1, 1))\n        self.ps = window_size\n        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(\n            dim * 3,\n            dim * 3,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "OSA_Block",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "class OSA_Block(nn.Module):\n    def __init__(\n        self,\n        channel_num=64,\n        bias=True,\n        ffn_bias=True,\n        window_size=8,\n        with_pe=False,\n        dropout=0.0,\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef default(val, d):\n    return val if exists(val) else d\ndef cast_tuple(val, length=1):\n    return val if isinstance(val, tuple) else ((val,) * length)\n# helper classes\nclass PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "def default(val, d):\n    return val if exists(val) else d\ndef cast_tuple(val, length=1):\n    return val if isinstance(val, tuple) else ((val,) * length)\n# helper classes\nclass PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "cast_tuple",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "def cast_tuple(val, length=1):\n    return val if isinstance(val, tuple) else ((val,) * length)\n# helper classes\nclass PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "MBConv",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "peekOfCode": "def MBConv(\n    dim_in, dim_out, *, downsample, expansion_rate=4, shrinkage_rate=0.25, dropout=0.0\n):\n    hidden_dim = int(expansion_rate * dim_out)\n    stride = 2 if downsample else 1\n    net = nn.Sequential(\n        nn.Conv2d(dim_in, hidden_dim, 1),\n        # nn.BatchNorm2d(hidden_dim),\n        nn.GELU(),\n        nn.Conv2d(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSA",
        "documentation": {}
    },
    {
        "label": "OSAG",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSAG",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSAG",
        "peekOfCode": "class OSAG(nn.Module):\n    def __init__(\n        self,\n        channel_num=64,\n        bias=True,\n        block_num=4,\n        ffn_bias=False,\n        window_size=0,\n        pe=False,\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OSAG",
        "documentation": {}
    },
    {
        "label": "OmniSR",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OmniSR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OmniSR",
        "peekOfCode": "class OmniSR(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        **kwargs,\n    ):\n        super(OmniSR, self).__init__()\n        self.state = state_dict\n        bias = True  # Fine to assume this for now\n        block_num = 1  # Fine to assume this for now",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.OmniSR",
        "documentation": {}
    },
    {
        "label": "ESA",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "peekOfCode": "class ESA(nn.Module):\n    \"\"\"\n    Modification of Enhanced Spatial Attention (ESA), which is proposed by\n    `Residual Feature Aggregation Network for Image Super-Resolution`\n    Note: `conv_max` and `conv3_` are NOT used here, so the corresponding codes\n    are deleted.\n    \"\"\"\n    def __init__(self, esa_channels, n_feats, conv=nn.Conv2d):\n        super(ESA, self).__init__()\n        f = esa_channels",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "documentation": {}
    },
    {
        "label": "LK_ESA",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "peekOfCode": "class LK_ESA(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(LK_ESA, self).__init__()\n        f = esa_channels\n        self.conv1 = conv(n_feats, f, kernel_size=1)\n        self.conv_f = conv(f, f, kernel_size=1)\n        kernel_size = 17\n        kernel_expand = kernel_expand",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "documentation": {}
    },
    {
        "label": "LK_ESA_LN",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "peekOfCode": "class LK_ESA_LN(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(LK_ESA_LN, self).__init__()\n        f = esa_channels\n        self.conv1 = conv(n_feats, f, kernel_size=1)\n        self.conv_f = conv(f, f, kernel_size=1)\n        kernel_size = 17\n        kernel_expand = kernel_expand",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "documentation": {}
    },
    {
        "label": "AdaGuidedFilter",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "peekOfCode": "class AdaGuidedFilter(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(AdaGuidedFilter, self).__init__()\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(\n            in_channels=n_feats,\n            out_channels=1,\n            kernel_size=1,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "documentation": {}
    },
    {
        "label": "AdaConvGuidedFilter",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "peekOfCode": "class AdaConvGuidedFilter(nn.Module):\n    def __init__(\n        self, esa_channels, n_feats, conv=nn.Conv2d, kernel_expand=1, bias=True\n    ):\n        super(AdaConvGuidedFilter, self).__init__()\n        f = esa_channels\n        self.conv_f = conv(f, f, kernel_size=1)\n        kernel_size = 17\n        kernel_expand = kernel_expand\n        padding = kernel_size // 2",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "documentation": {}
    },
    {
        "label": "moment",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "peekOfCode": "def moment(x, dim=(2, 3), k=2):\n    assert len(x.size()) == 4\n    mean = torch.mean(x, dim=dim).unsqueeze(-1).unsqueeze(-1)\n    mk = (1 / (x.size(2) * x.size(3))) * torch.sum(torch.pow(x - mean, k), dim=dim)\n    return mk\nclass ESA(nn.Module):\n    \"\"\"\n    Modification of Enhanced Spatial Attention (ESA), which is proposed by\n    `Residual Feature Aggregation Network for Image Super-Resolution`\n    Note: `conv_max` and `conv3_` are NOT used here, so the corresponding codes",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.esa",
        "documentation": {}
    },
    {
        "label": "LayerNormFunction",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "peekOfCode": "class LayerNormFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, eps):\n        ctx.eps = eps\n        N, C, H, W = x.size()\n        mu = x.mean(1, keepdim=True)\n        var = (x - mu).pow(2).mean(1, keepdim=True)\n        y = (x - mu) / (var + eps).sqrt()\n        ctx.save_for_backward(y, var, weight)\n        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "documentation": {}
    },
    {
        "label": "LayerNorm2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "peekOfCode": "class LayerNorm2d(nn.Module):\n    def __init__(self, channels, eps=1e-6):\n        super(LayerNorm2d, self).__init__()\n        self.register_parameter(\"weight\", nn.Parameter(torch.ones(channels)))\n        self.register_parameter(\"bias\", nn.Parameter(torch.zeros(channels)))\n        self.eps = eps\n    def forward(self, x):\n        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\nclass GRN(nn.Module):\n    \"\"\"GRN (Global Response Normalization) layer\"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "documentation": {}
    },
    {
        "label": "GRN",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "peekOfCode": "class GRN(nn.Module):\n    \"\"\"GRN (Global Response Normalization) layer\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.zeros(1, dim, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))\n    def forward(self, x):\n        Gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n        Nx = Gx / (Gx.mean(dim=1, keepdim=True) + 1e-6)\n        return self.gamma * (x * Nx) + self.beta + x",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.layernorm",
        "documentation": {}
    },
    {
        "label": "pixelshuffle_block",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.pixelshuffle",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.pixelshuffle",
        "peekOfCode": "def pixelshuffle_block(\n    in_channels, out_channels, upscale_factor=2, kernel_size=3, bias=False\n):\n    \"\"\"\n    Upsample features according to `upscale_factor`.\n    \"\"\"\n    padding = kernel_size // 2\n    conv = nn.Conv2d(\n        in_channels,\n        out_channels * (upscale_factor**2),",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.OmniSR.pixelshuffle",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "peekOfCode": "class BasicBlock(nn.Module):\n    \"\"\"Basic residual block used in the ResNetArcFace architecture.\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n    \"\"\"\n    expansion = 1  # output channel expansion ratio\n    def __init__(self, inplanes, planes, stride=1, downsample=None):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "documentation": {}
    },
    {
        "label": "IRBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "peekOfCode": "class IRBlock(nn.Module):\n    \"\"\"Improved residual block (IR Block) used in the ResNetArcFace architecture.\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n    \"\"\"\n    expansion = 1  # output channel expansion ratio",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "peekOfCode": "class Bottleneck(nn.Module):\n    \"\"\"Bottleneck block used in the ResNetArcFace architecture.\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n    \"\"\"\n    expansion = 4  # output channel expansion ratio\n    def __init__(self, inplanes, planes, stride=1, downsample=None):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "documentation": {}
    },
    {
        "label": "SEBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "peekOfCode": "class SEBlock(nn.Module):\n    \"\"\"The squeeze-and-excitation block (SEBlock) used in the IRBlock.\n    Args:\n        channel (int): Channel number of inputs.\n        reduction (int): Channel reduction ration. Default: 16.\n    \"\"\"\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(\n            1",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "documentation": {}
    },
    {
        "label": "ResNetArcFace",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "peekOfCode": "class ResNetArcFace(nn.Module):\n    \"\"\"ArcFace with ResNet architectures.\n    Ref: ArcFace: Additive Angular Margin Loss for Deep Face Recognition.\n    Args:\n        block (str): Block used in the ArcFace architecture.\n        layers (tuple(int)): Block numbers in each layer.\n        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n    \"\"\"\n    def __init__(self, block, layers, use_se=True):\n        if block == \"IRBlock\":",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "peekOfCode": "def conv3x3(inplanes, outplanes, stride=1):\n    \"\"\"A simple wrapper for 3x3 convolution with padding.\n    Args:\n        inplanes (int): Channel number of inputs.\n        outplanes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n    \"\"\"\n    return nn.Conv2d(\n        inplanes, outplanes, kernel_size=3, stride=stride, padding=1, bias=False\n    )",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.arcface_arch",
        "documentation": {}
    },
    {
        "label": "VectorQuantizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class VectorQuantizer(nn.Module):\n    def __init__(self, codebook_size, emb_dim, beta):\n        super(VectorQuantizer, self).__init__()\n        self.codebook_size = codebook_size  # number of embeddings\n        self.emb_dim = emb_dim  # dimension of embedding\n        self.beta = beta  # commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n        self.embedding = nn.Embedding(self.codebook_size, self.emb_dim)\n        self.embedding.weight.data.uniform_(\n            -1.0 / self.codebook_size, 1.0 / self.codebook_size\n        )",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "GumbelQuantizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class GumbelQuantizer(nn.Module):\n    def __init__(\n        self,\n        codebook_size,\n        emb_dim,\n        num_hiddens,\n        straight_through=False,\n        kl_weight=5e-4,\n        temp_init=1.0,\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class Downsample(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=3, stride=2, padding=0\n        )\n    def forward(self, x):\n        pad = (0, 1, 0, 1)\n        x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n        x = self.conv(x)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class Upsample(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n        )\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        x = self.conv(x)\n        return x",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "AttnBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.norm = normalize(in_channels)\n        self.q = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.k = torch.nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        nf,\n        out_channels,\n        ch_mult,\n        num_res_blocks,\n        resolution,\n        attn_resolutions,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "Generator",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class Generator(nn.Module):\n    def __init__(self, nf, ch_mult, res_blocks, img_size, attn_resolutions, emb_dim):\n        super().__init__()\n        self.nf = nf\n        self.ch_mult = ch_mult\n        self.num_resolutions = len(self.ch_mult)\n        self.num_res_blocks = res_blocks\n        self.resolution = img_size\n        self.attn_resolutions = attn_resolutions\n        self.in_channels = emb_dim",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "VQAutoEncoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class VQAutoEncoder(nn.Module):\n    def __init__(\n        self,\n        img_size,\n        nf,\n        ch_mult,\n        quantizer=\"nearest\",\n        res_blocks=2,\n        attn_resolutions=[16],\n        codebook_size=1024,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(\n        self, num_pos_feats=64, temperature=10000, normalize=False, scale=None\n    ):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "TransformerSALayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class TransformerSALayer(nn.Module):\n    def __init__(\n        self, embed_dim, nhead=8, dim_mlp=2048, dropout=0.0, activation=\"gelu\"\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(embed_dim, nhead, dropout=dropout)\n        # Implementation of Feedforward model - MLP\n        self.linear1 = nn.Linear(embed_dim, dim_mlp)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_mlp, embed_dim)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=None):\n        super(ResBlock, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = in_channels if out_channels is None else out_channels\n        self.norm1 = normalize(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1  # type: ignore\n        )\n        self.norm2 = normalize(out_channels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "Fuse_sft_block",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class Fuse_sft_block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.encode_enc = ResBlock(2 * in_ch, out_ch)\n        self.scale = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n        )\n        self.shift = nn.Sequential(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "CodeFormer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "class CodeFormer(VQAutoEncoder):\n    def __init__(self, state_dict):\n        dim_embd = 512\n        n_head = 8\n        n_layers = 9\n        codebook_size = 1024\n        latent_size = 256\n        connect_list = [\"32\", \"64\", \"128\", \"256\"]\n        fix_modules = [\"quantize\", \"generator\"]\n        # This is just a guess as I only have one model to look at",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "calc_mean_std",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "def calc_mean_std(feat, eps=1e-5):\n    \"\"\"Calculate mean and std for adaptive_instance_normalization.\n    Args:\n        feat (Tensor): 4D tensor.\n        eps (float): A small value added to the variance to avoid\n            divide-by-zero. Default: 1e-5.\n    \"\"\"\n    size = feat.size()\n    assert len(size) == 4, \"The input feature should be 4D tensor.\"\n    b, c = size[:2]",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "adaptive_instance_normalization",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "def adaptive_instance_normalization(content_feat, style_feat):\n    \"\"\"Adaptive instance normalization.\n    Adjust the reference features to have the similar color and illuminations\n    as those in the degradate features.\n    Args:\n        content_feat (Tensor): The reference feature.\n        style_feat (Tensor): The degradate features.\n    \"\"\"\n    size = content_feat.size()\n    style_mean, style_std = calc_mean_std(style_feat)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "def normalize(in_channels):\n    return torch.nn.GroupNorm(\n        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n    )\n@torch.jit.script  # type: ignore\ndef swish(x):\n    return x * torch.sigmoid(x)\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=None):\n        super(ResBlock, self).__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "swish",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "peekOfCode": "def swish(x):\n    return x * torch.sigmoid(x)\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=None):\n        super(ResBlock, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = in_channels if out_channels is None else out_channels\n        self.norm1 = normalize(in_channels)\n        self.conv1 = nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1  # type: ignore",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.codeformer",
        "documentation": {}
    },
    {
        "label": "FusedLeakyReLUFunctionBackward",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "peekOfCode": "class FusedLeakyReLUFunctionBackward(Function):\n    @staticmethod\n    def forward(ctx, grad_output, out, negative_slope, scale):\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n        empty = grad_output.new_empty(0)\n        grad_input = fused_act_ext.fused_bias_act(\n            grad_output, empty, out, 3, 1, negative_slope, scale\n        )",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "documentation": {}
    },
    {
        "label": "FusedLeakyReLUFunction",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "peekOfCode": "class FusedLeakyReLUFunction(Function):\n    @staticmethod\n    def forward(ctx, input, bias, negative_slope, scale):\n        empty = input.new_empty(0)\n        out = fused_act_ext.fused_bias_act(\n            input, bias, empty, 3, 0, negative_slope, scale\n        )\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "documentation": {}
    },
    {
        "label": "FusedLeakyReLU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "peekOfCode": "class FusedLeakyReLU(nn.Module):\n    def __init__(self, channel, negative_slope=0.2, scale=2**0.5):\n        super().__init__()\n        self.bias = nn.Parameter(torch.zeros(channel))\n        self.negative_slope = negative_slope\n        self.scale = scale\n    def forward(self, input):\n        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\ndef fused_leaky_relu(input, bias, negative_slope=0.2, scale=2**0.5):\n    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "documentation": {}
    },
    {
        "label": "fused_leaky_relu",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "peekOfCode": "def fused_leaky_relu(input, bias, negative_slope=0.2, scale=2**0.5):\n    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "documentation": {}
    },
    {
        "label": "fused_act_ext",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "peekOfCode": "fused_act_ext = None\nclass FusedLeakyReLUFunctionBackward(Function):\n    @staticmethod\n    def forward(ctx, grad_output, out, negative_slope, scale):\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n        empty = grad_output.new_empty(0)\n        grad_input = fused_act_ext.fused_bias_act(\n            grad_output, empty, out, 3, 1, negative_slope, scale",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.fused_act",
        "documentation": {}
    },
    {
        "label": "StyleGAN2GeneratorBilinearSFT",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpgan_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpgan_bilinear_arch",
        "peekOfCode": "class StyleGAN2GeneratorBilinearSFT(StyleGAN2GeneratorBilinear):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n    It is the bilinear version. It does not use the complicated UpFirDnSmooth function that is not friendly for\n    deployment. It can be easily converted to the clean version: StyleGAN2GeneratorCSFT.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpgan_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "GFPGANBilinear",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpgan_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpgan_bilinear_arch",
        "peekOfCode": "class GFPGANBilinear(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n    It is the bilinear version and it does not use the complicated UpFirDnSmooth function that is not friendly for\n    deployment. It can be easily converted to the clean version: GFPGANv1Clean.\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpgan_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "StyleGAN2GeneratorSFT",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "peekOfCode": "class StyleGAN2GeneratorSFT(StyleGAN2Generator):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel magnitude. A cross production will be\n            applied to extent 1D resample kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "documentation": {}
    },
    {
        "label": "ConvUpLayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "peekOfCode": "class ConvUpLayer(nn.Module):\n    \"\"\"Convolutional upsampling layer. It uses bilinear upsampler + Conv.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input. Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "documentation": {}
    },
    {
        "label": "ResUpBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "peekOfCode": "class ResUpBlock(nn.Module):\n    \"\"\"Residual block with upsampling.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super(ResUpBlock, self).__init__()\n        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n        self.conv2 = ConvUpLayer(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "documentation": {}
    },
    {
        "label": "GFPGANv1",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "peekOfCode": "class GFPGANv1(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel magnitude. A cross production will be\n            applied to extent 1D resample kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "documentation": {}
    },
    {
        "label": "FacialComponentDiscriminator",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "peekOfCode": "class FacialComponentDiscriminator(nn.Module):\n    \"\"\"Facial component (eyes, mouth, noise) discriminator used in GFPGAN.\"\"\"\n    def __init__(self):\n        super(FacialComponentDiscriminator, self).__init__()\n        # It now uses a VGG-style architectrue with fixed model size\n        self.conv1 = ConvLayer(\n            3,\n            64,\n            3,\n            downsample=False,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_arch",
        "documentation": {}
    },
    {
        "label": "StyleGAN2GeneratorCSFT",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "peekOfCode": "class StyleGAN2GeneratorCSFT(StyleGAN2GeneratorClean):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "peekOfCode": "class ResBlock(nn.Module):\n    \"\"\"Residual block with bilinear upsampling/downsampling.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        mode (str): Upsampling/downsampling mode. Options: down | up. Default: down.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, mode=\"down\"):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, 1, 1)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "documentation": {}
    },
    {
        "label": "GFPGANv1Clean",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "peekOfCode": "class GFPGANv1Clean(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n        fix_decoder (bool): Whether to fix the decoder. Default: True.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.gfpganv1_clean_arch",
        "documentation": {}
    },
    {
        "label": "VectorQuantizer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class VectorQuantizer(nn.Module):\n    \"\"\"\n    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n    ____________________________________________\n    Discretization bottleneck part of the VQ-VAE.\n    Inputs:\n    - n_e : number of embeddings\n    - e_dim : dimension of embedding\n    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n    _____________________________________________",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(\n                in_channels, in_channels, kernel_size=3, stride=1, padding=1\n            )\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(\n                in_channels, in_channels, kernel_size=3, stride=2, padding=0\n            )\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "ResnetBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        conv_shortcut=False,\n        dropout,\n        temb_channels=512\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttnBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class MultiHeadAttnBlock(nn.Module):\n    def __init__(self, in_channels, head_size=1):\n        super().__init__()\n        self.in_channels = in_channels\n        self.head_size = head_size\n        self.att_size = in_channels // head_size\n        assert (\n            in_channels % head_size == 0\n        ), \"The size of head should be divided by the number of channels.\"\n        self.norm1 = Normalize(in_channels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "MultiHeadEncoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class MultiHeadEncoder(nn.Module):\n    def __init__(\n        self,\n        ch,\n        out_ch,\n        ch_mult=(1, 2, 4, 8),\n        num_res_blocks=2,\n        attn_resolutions=(16,),\n        dropout=0.0,\n        resamp_with_conv=True,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "MultiHeadDecoder",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class MultiHeadDecoder(nn.Module):\n    def __init__(\n        self,\n        ch,\n        out_ch,\n        ch_mult=(1, 2, 4, 8),\n        num_res_blocks=2,\n        attn_resolutions=(16,),\n        dropout=0.0,\n        resamp_with_conv=True,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "MultiHeadDecoderTransformer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class MultiHeadDecoderTransformer(nn.Module):\n    def __init__(\n        self,\n        ch,\n        out_ch,\n        ch_mult=(1, 2, 4, 8),\n        num_res_blocks=2,\n        attn_resolutions=(16,),\n        dropout=0.0,\n        resamp_with_conv=True,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "RestoreFormer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "class RestoreFormer(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n    ):\n        super(RestoreFormer, self).__init__()\n        n_embed = 1024\n        embed_dim = 256\n        ch = 64\n        out_ch = 3",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "nonlinearity",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "def nonlinearity(x):\n    # swish\n    return x * torch.sigmoid(x)\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(\n        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n    )\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "peekOfCode": "def Normalize(in_channels):\n    return torch.nn.GroupNorm(\n        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n    )\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.restoreformer_arch",
        "documentation": {}
    },
    {
        "label": "NormStyleCode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class NormStyleCode(nn.Module):\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\ndef make_resample_kernel(k):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "UpFirDnUpsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class UpFirDnUpsample(nn.Module):\n    \"\"\"Upsample, FIR filter, and downsample (upsampole version).\n    References:\n    1. https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.upfirdn.html  # noqa: E501\n    2. http://www.ece.northwestern.edu/local-apps/matlabhelp/toolbox/signal/upfirdn.html  # noqa: E501\n    Args:\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude.\n        factor (int): Upsampling scale factor. Default: 2.\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "UpFirDnDownsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class UpFirDnDownsample(nn.Module):\n    \"\"\"Upsample, FIR filter, and downsample (downsampole version).\n    Args:\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude.\n        factor (int): Downsampling scale factor. Default: 2.\n    \"\"\"\n    def __init__(self, resample_kernel, factor=2):\n        super(UpFirDnDownsample, self).__init__()\n        self.kernel = make_resample_kernel(resample_kernel)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "UpFirDnSmooth",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class UpFirDnSmooth(nn.Module):\n    \"\"\"Upsample, FIR filter, and downsample (smooth version).\n    Args:\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude.\n        upsample_factor (int): Upsampling scale factor. Default: 1.\n        downsample_factor (int): Downsampling scale factor. Default: 1.\n        kernel_size (int): Kernel size: Default: 1.\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "EqualLinear",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class EqualLinear(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n    Args:\n        in_channels (int): Size of each sample.\n        out_channels (int): Size of each output sample.\n        bias (bool): If set to ``False``, the layer will not learn an additive\n            bias. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        lr_mul (float): Learning rate multiplier. Default: 1.\n        activation (None | str): The activation after ``linear`` operation.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "ModulatedConv2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n    There is no bias in ModulatedConv2d.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer.\n            Default: True.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "StyleConv",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class StyleConv(nn.Module):\n    \"\"\"Style conv.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "ToRGB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class ToRGB(nn.Module):\n    \"\"\"To RGB from features.\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude. Default: (1, 3, 3, 1).\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "ConstantInput",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class ConstantInput(nn.Module):\n    \"\"\"Constant input.\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n    def forward(self, batch):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "StyleGAN2Generator",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class StyleGAN2Generator(nn.Module):\n    \"\"\"StyleGAN2 Generator.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of\n            StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel\n            magnitude. A cross production will be applied to extent 1D resample",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "ScaledLeakyReLU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class ScaledLeakyReLU(nn.Module):\n    \"\"\"Scaled LeakyReLU.\n    Args:\n        negative_slope (float): Negative slope. Default: 0.2.\n    \"\"\"\n    def __init__(self, negative_slope=0.2):\n        super(ScaledLeakyReLU, self).__init__()\n        self.negative_slope = negative_slope\n    def forward(self, x):\n        out = F.leaky_relu(x, negative_slope=self.negative_slope)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "EqualConv2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class EqualConv2d(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input.\n            Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "ConvLayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class ConvLayer(nn.Sequential):\n    \"\"\"Conv Layer used in StyleGAN2 Discriminator.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Kernel size.\n        downsample (bool): Whether downsample by a factor of 2.\n            Default: False.\n        resample_kernel (list[int]): A list indicating the 1D resample\n            kernel magnitude. A cross production will be applied to",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "class ResBlock(nn.Module):\n    \"\"\"Residual block used in StyleGAN2 Discriminator.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        resample_kernel (list[int]): A list indicating the 1D resample\n            kernel magnitude. A cross production will be applied to\n            extent 1D resample kernel to 2D resample kernel.\n            Default: (1, 3, 3, 1).\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "make_resample_kernel",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "peekOfCode": "def make_resample_kernel(k):\n    \"\"\"Make resampling kernel for UpFirDn.\n    Args:\n        k (list[int]): A list indicating the 1D resample kernel magnitude.\n    Returns:\n        Tensor: 2D resampled kernel.\n    \"\"\"\n    k = torch.tensor(k, dtype=torch.float32)\n    if k.ndim == 1:\n        k = k[None, :] * k[:, None]  # to 2D kernel, outer product",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_arch",
        "documentation": {}
    },
    {
        "label": "NormStyleCode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class NormStyleCode(nn.Module):\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\nclass EqualLinear(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "EqualLinear",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class EqualLinear(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n    Args:\n        in_channels (int): Size of each sample.\n        out_channels (int): Size of each output sample.\n        bias (bool): If set to ``False``, the layer will not learn an additive\n            bias. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        lr_mul (float): Learning rate multiplier. Default: 1.\n        activation (None | str): The activation after ``linear`` operation.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "ModulatedConv2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n    There is no bias in ModulatedConv2d.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer.\n            Default: True.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "StyleConv",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class StyleConv(nn.Module):\n    \"\"\"Style conv.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "ToRGB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class ToRGB(nn.Module):\n    \"\"\"To RGB from features.\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n    \"\"\"\n    def __init__(\n        self, in_channels, num_style_feat, upsample=True, interpolation_mode=\"bilinear\"\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "ConstantInput",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class ConstantInput(nn.Module):\n    \"\"\"Constant input.\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n    def forward(self, batch):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "StyleGAN2GeneratorBilinear",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class StyleGAN2GeneratorBilinear(nn.Module):\n    \"\"\"StyleGAN2 Generator.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of\n            StyleGAN2. Default: 2.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): Narrow ratio for channels. Default: 1.0.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "ScaledLeakyReLU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class ScaledLeakyReLU(nn.Module):\n    \"\"\"Scaled LeakyReLU.\n    Args:\n        negative_slope (float): Negative slope. Default: 0.2.\n    \"\"\"\n    def __init__(self, negative_slope=0.2):\n        super(ScaledLeakyReLU, self).__init__()\n        self.negative_slope = negative_slope\n    def forward(self, x):\n        out = F.leaky_relu(x, negative_slope=self.negative_slope)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "EqualConv2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class EqualConv2d(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input.\n            Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "ConvLayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class ConvLayer(nn.Sequential):\n    \"\"\"Conv Layer used in StyleGAN2 Discriminator.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Kernel size.\n        downsample (bool): Whether downsample by a factor of 2.\n            Default: False.\n        bias (bool): Whether with bias. Default: True.\n        activate (bool): Whether use activateion. Default: True.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "peekOfCode": "class ResBlock(nn.Module):\n    \"\"\"Residual block used in StyleGAN2 Discriminator.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, interpolation_mode=\"bilinear\"):\n        super(ResBlock, self).__init__()\n        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n        self.conv2 = ConvLayer(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_bilinear_arch",
        "documentation": {}
    },
    {
        "label": "NormStyleCode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "peekOfCode": "class NormStyleCode(nn.Module):\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\nclass ModulatedConv2d(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "documentation": {}
    },
    {
        "label": "ModulatedConv2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "peekOfCode": "class ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n    There is no bias in ModulatedConv2d.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "documentation": {}
    },
    {
        "label": "StyleConv",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "peekOfCode": "class StyleConv(nn.Module):\n    \"\"\"Style conv used in StyleGAN2.\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "documentation": {}
    },
    {
        "label": "ToRGB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "peekOfCode": "class ToRGB(nn.Module):\n    \"\"\"To RGB (image space) from features.\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n    \"\"\"\n    def __init__(self, in_channels, num_style_feat, upsample=True):\n        super(ToRGB, self).__init__()\n        self.upsample = upsample",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "documentation": {}
    },
    {
        "label": "ConstantInput",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "peekOfCode": "class ConstantInput(nn.Module):\n    \"\"\"Constant input.\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n    def forward(self, batch):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "documentation": {}
    },
    {
        "label": "StyleGAN2GeneratorClean",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "peekOfCode": "class StyleGAN2GeneratorClean(nn.Module):\n    \"\"\"Clean version of StyleGAN2 Generator.\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        narrow (float): Narrow ratio for channels. Default: 1.0.\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "documentation": {}
    },
    {
        "label": "default_init_weights",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "peekOfCode": "def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n    \"\"\"Initialize network weights.\n    Args:\n        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n        scale (float): Scale initialized weights, especially for residual\n            blocks. Default: 1.\n        bias_fill (float): The value to fill bias. Default: 0\n        kwargs (dict): Other arguments for initialization function.\n    \"\"\"\n    if not isinstance(module_list, list):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.stylegan2_clean_arch",
        "documentation": {}
    },
    {
        "label": "UpFirDn2dBackward",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "peekOfCode": "class UpFirDn2dBackward(Function):\n    @staticmethod\n    def forward(\n        ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size\n    ):\n        up_x, up_y = up\n        down_x, down_y = down\n        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad\n        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)\n        grad_input = upfirdn2d_ext.upfirdn2d(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "documentation": {}
    },
    {
        "label": "UpFirDn2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "peekOfCode": "class UpFirDn2d(Function):\n    @staticmethod\n    def forward(ctx, input, kernel, up, down, pad):\n        up_x, up_y = up\n        down_x, down_y = down\n        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n        kernel_h, kernel_w = kernel.shape\n        _, channel, in_h, in_w = input.shape\n        ctx.in_size = input.shape\n        input = input.reshape(-1, in_h, in_w, 1)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "documentation": {}
    },
    {
        "label": "upfirdn2d",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "peekOfCode": "def upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n    if input.device.type == \"cpu\":\n        out = upfirdn2d_native(\n            input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]\n        )\n    else:\n        out = UpFirDn2d.apply(\n            input, kernel, (up, up), (down, down), (pad[0], pad[1], pad[0], pad[1])\n        )\n    return out",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "documentation": {}
    },
    {
        "label": "upfirdn2d_native",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "peekOfCode": "def upfirdn2d_native(\n    input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n):\n    _, channel, in_h, in_w = input.shape\n    input = input.reshape(-1, in_h, in_w, 1)\n    _, in_h, in_w, minor = input.shape\n    kernel_h, kernel_w = kernel.shape\n    out = input.view(-1, in_h, 1, in_w, 1, minor)\n    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n    out = out.view(-1, in_h * up_y, in_w * up_x, minor)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "documentation": {}
    },
    {
        "label": "upfirdn2d_ext",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "peekOfCode": "upfirdn2d_ext = None\nclass UpFirDn2dBackward(Function):\n    @staticmethod\n    def forward(\n        ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size\n    ):\n        up_x, up_y = up\n        down_x, down_y = down\n        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad\n        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.face.upfirdn2d",
        "documentation": {}
    },
    {
        "label": "DropBlock2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "peekOfCode": "class DropBlock2d(nn.Module):\n    \"\"\"DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n    def __init__(\n        self,\n        drop_prob: float = 0.1,\n        block_size: int = 7,\n        gamma_scale: float = 1.0,\n        with_noise: bool = False,\n        inplace: bool = False,\n        batchwise: bool = False,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob: float = 0.0, scale_by_keep: bool = True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n    def extra_repr(self):\n        return f\"drop_prob={round(self.drop_prob,3):0.3f}\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "documentation": {}
    },
    {
        "label": "drop_block_2d",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "peekOfCode": "def drop_block_2d(\n    x,\n    drop_prob: float = 0.1,\n    block_size: int = 7,\n    gamma_scale: float = 1.0,\n    with_noise: bool = False,\n    inplace: bool = False,\n    batchwise: bool = False,\n):\n    \"\"\"DropBlock. See https://arxiv.org/pdf/1810.12890.pdf",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "documentation": {}
    },
    {
        "label": "drop_block_fast_2d",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "peekOfCode": "def drop_block_fast_2d(\n    x: torch.Tensor,\n    drop_prob: float = 0.1,\n    block_size: int = 7,\n    gamma_scale: float = 1.0,\n    with_noise: bool = False,\n    inplace: bool = False,\n):\n    \"\"\"DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "peekOfCode": "def drop_path(\n    x, drop_prob: float = 0.0, training: bool = False, scale_by_keep: bool = True\n):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.drop",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "peekOfCode": "def make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "documentation": {}
    },
    {
        "label": "to_1tuple",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "peekOfCode": "to_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "peekOfCode": "to_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "documentation": {}
    },
    {
        "label": "to_3tuple",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "peekOfCode": "to_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "documentation": {}
    },
    {
        "label": "to_4tuple",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "peekOfCode": "to_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "documentation": {}
    },
    {
        "label": "to_ntuple",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "peekOfCode": "to_ntuple = _ntuple\ndef make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.helpers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "peekOfCode": "def trunc_normal_(\n    tensor: torch.Tensor, mean=0.0, std=1.0, a=-2.0, b=2.0\n) -> torch.Tensor:\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_tf_",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "peekOfCode": "def trunc_normal_tf_(\n    tensor: torch.Tensor, mean=0.0, std=1.0, a=-2.0, b=2.0\n) -> torch.Tensor:\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "documentation": {}
    },
    {
        "label": "variance_scaling_",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "peekOfCode": "def variance_scaling_(tensor, scale=1.0, mode=\"fan_in\", distribution=\"normal\"):\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    if mode == \"fan_in\":\n        denom = fan_in\n    elif mode == \"fan_out\":\n        denom = fan_out\n    elif mode == \"fan_avg\":\n        denom = (fan_in + fan_out) / 2\n    variance = scale / denom  # type: ignore\n    if distribution == \"truncated_normal\":",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "documentation": {}
    },
    {
        "label": "lecun_normal_",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "peekOfCode": "def lecun_normal_(tensor):\n    variance_scaling_(tensor, mode=\"fan_in\", distribution=\"truncated_normal\")",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.timm.weight_init",
        "documentation": {}
    },
    {
        "label": "SpatialGate",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class SpatialGate(nn.Module):\n    \"\"\"Spatial-Gate.\n    Args:\n        dim (int): Half of input channels.\n    \"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.conv = nn.Conv2d(\n            dim, dim, kernel_size=3, stride=1, padding=1, groups=dim",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "SGFN",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class SGFN(nn.Module):\n    \"\"\"Spatial-Gate Feed-Forward Network.\n    Args:\n        in_features (int): Number of input channels.\n        hidden_features (int | None): Number of hidden channels. Default: None\n        out_features (int | None): Number of output channels. Default: None\n        act_layer (nn.Module): Activation layer. Default: nn.GELU\n        drop (float): Dropout rate. Default: 0.0\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "DynamicPosBias",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class DynamicPosBias(nn.Module):\n    # The implementation builds on Crossformer code https://github.com/cheerss/CrossFormer/blob/main/models/crossformer.py\n    \"\"\"Dynamic Relative Position Bias.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        residual (bool):  If True, use residual strage to connect conv.\n    \"\"\"\n    def __init__(self, dim, num_heads, residual):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "Spatial_Attention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class Spatial_Attention(nn.Module):\n    \"\"\"Spatial Window Self-Attention.\n    It supports rectangle window (containing square window).\n    Args:\n        dim (int): Number of input channels.\n        idx (int): The indentix of window. (0/1)\n        split_size (tuple(int)): Height and Width of spatial window.\n        dim_out (int | None): The dimension of the attention output. Default: None\n        num_heads (int): Number of attention heads. Default: 6\n        attn_drop (float): Dropout ratio of attention weight. Default: 0.0",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "Adaptive_Spatial_Attention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class Adaptive_Spatial_Attention(nn.Module):\n    # The implementation builds on CAT code https://github.com/Zhengchen1999/CAT\n    \"\"\"Adaptive Spatial Self-Attention\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads. Default: 6\n        split_size (tuple(int)): Height and Width of spatial window.\n        shift_size (tuple(int)): Shift size for spatial window.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "Adaptive_Channel_Attention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class Adaptive_Channel_Attention(nn.Module):\n    # The implementation builds on XCiT code https://github.com/facebookresearch/xcit\n    \"\"\"Adaptive Channel Self-Attention\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads. Default: 6\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set.\n        attn_drop (float): Attention dropout rate. Default: 0.0\n        drop_path (float): Stochastic depth rate. Default: 0.0",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "DATB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class DATB(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        reso=64,\n        split_size=[2, 4],\n        shift_size=[1, 2],\n        expansion_factor=4.0,\n        qkv_bias=False,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "ResidualGroup",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class ResidualGroup(nn.Module):\n    \"\"\"ResidualGroup\n    Args:\n        dim (int): Number of input channels.\n        reso (int): Input resolution.\n        num_heads (int): Number of attention heads.\n        split_size (tuple(int)): Height and Width of spatial window.\n        expansion_factor (float): Ratio of ffn hidden dim to embedding dim.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set. Default: None",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "UpsampleOneStep",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "DAT",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "class DAT(nn.Module):\n    \"\"\"Dual Aggregation Transformer\n    Args:\n        img_size (int): Input image size. Default: 64\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 180\n        depths (tuple(int)): Depth of each residual group (number of DATB in each RG).\n        split_size (tuple(int)): Height and Width of spatial window.\n        num_heads (tuple(int)): Number of attention heads in different residual groups.\n        expansion_factor (float): Ratio of ffn hidden dim to embedding dim. Default: 4",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "img2windows",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "def img2windows(img, H_sp, W_sp):\n    \"\"\"\n    Input: Image (B, C, H, W)\n    Output: Window Partition (B', N, C)\n    \"\"\"\n    B, C, H, W = img.shape\n    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n    img_perm = (\n        img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp * W_sp, C)\n    )",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "windows2img",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "peekOfCode": "def windows2img(img_splits_hw, H_sp, W_sp, H, W):\n    \"\"\"\n    Input: Window Partition (B', N, C)\n    Output: Image (B, H, W, C)\n    \"\"\"\n    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)\n    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return img\nclass SpatialGate(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.DAT",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)  # type: ignore\nclass ChannelAttention(nn.Module):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "ChannelAttention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class ChannelAttention(nn.Module):\n    \"\"\"Channel attention used in RCAN.\n    Args:\n        num_feat (int): Channel number of intermediate features.\n        squeeze_factor (int): Channel squeeze factor. Default: 16.\n    \"\"\"\n    def __init__(self, num_feat, squeeze_factor=16):\n        super(ChannelAttention, self).__init__()\n        self.attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "CAB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class CAB(nn.Module):\n    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n        super(CAB, self).__init__()\n        self.cab = nn.Sequential(\n            nn.Conv2d(num_feat, num_feat // compress_ratio, 3, 1, 1),\n            nn.GELU(),\n            nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),\n            ChannelAttention(num_feat, squeeze_factor),\n        )\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class WindowAttention(nn.Module):\n    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "HAB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class HAB(nn.Module):\n    r\"\"\"Hybrid Attention Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class PatchMerging(nn.Module):\n    r\"\"\"Patch Merging Layer.\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "OCAB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class OCAB(nn.Module):\n    # overlapping cross-attention block\n    def __init__(\n        self,\n        dim,\n        input_resolution,\n        window_size,\n        overlap_ratio,\n        num_heads,\n        qkv_bias=True,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "AttenBlocks",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class AttenBlocks(nn.Module):\n    \"\"\"A series of attention blocks for one RHAG.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "RHAG",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class RHAG(nn.Module):\n    \"\"\"Residual Hybrid Attention Group (RHAG).\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    r\"\"\"Image to Patch Embedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "PatchUnEmbed",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class PatchUnEmbed(nn.Module):\n    r\"\"\"Image to Patch Unembedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "HAT",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "class HAT(nn.Module):\n    r\"\"\"Hybrid Attention Transformer\n        A PyTorch implementation of : `Activating More Pixels in Image Super-Resolution Transformer`.\n        Some codes are based on SwinIR.\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (b, h, w, c)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*b, window_size, window_size, c)\n    \"\"\"\n    b, h, w, c = x.shape\n    x = x.view(b, h // window_size, window_size, w // window_size, window_size, c)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "peekOfCode": "def window_reverse(windows, window_size, h, w):\n    \"\"\"\n    Args:\n        windows: (num_windows*b, window_size, window_size, c)\n        window_size (int): Window size\n        h (int): Height of image\n        w (int): Width of image\n    Returns:\n        x: (b, h, w, c)\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.HAT",
        "documentation": {}
    },
    {
        "label": "LearnableSpatialTransformWrapper",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class LearnableSpatialTransformWrapper(nn.Module):\n    def __init__(self, impl, pad_coef=0.5, angle_init_range=80, train_angle=True):\n        super().__init__()\n        self.impl = impl\n        self.angle = torch.rand(1) * angle_init_range\n        if train_angle:\n            self.angle = nn.Parameter(self.angle, requires_grad=True)\n        self.pad_coef = pad_coef\n    def forward(self, x):\n        if torch.is_tensor(x):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "SELayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid(),\n        )",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "FourierUnit",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class FourierUnit(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        groups=1,\n        spatial_scale_factor=None,\n        spatial_scale_mode=\"bilinear\",\n        spectral_pos_encoding=False,\n        use_se=False,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "SpectralTransform",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class SpectralTransform(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride=1,\n        groups=1,\n        enable_lfu=True,\n        separable_fu=False,\n        **fu_kwargs,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "FFC",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class FFC(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        ratio_gin,\n        ratio_gout,\n        stride=1,\n        padding=0,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "FFC_BN_ACT",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class FFC_BN_ACT(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        ratio_gin,\n        ratio_gout,\n        stride=1,\n        padding=0,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "FFCResnetBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class FFCResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        padding_type,\n        norm_layer,\n        activation_layer=nn.ReLU,\n        dilation=1,\n        spatial_transform_kwargs=None,\n        inline=False,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "ConcatTupleLayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class ConcatTupleLayer(nn.Module):\n    def forward(self, x):\n        assert isinstance(x, tuple)\n        x_l, x_g = x\n        assert torch.is_tensor(x_l) or torch.is_tensor(x_g)\n        if not torch.is_tensor(x_g):\n            return x_l\n        return torch.cat(x, dim=1)\nclass FFCResNetGenerator(nn.Module):\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "FFCResNetGenerator",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class FFCResNetGenerator(nn.Module):\n    def __init__(\n        self,\n        input_nc,\n        output_nc,\n        ngf=64,\n        n_downsampling=3,\n        n_blocks=18,\n        norm_layer=nn.BatchNorm2d,\n        padding_type=\"reflect\",",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "LaMa",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "peekOfCode": "class LaMa(nn.Module):\n    def __init__(self, state_dict) -> None:\n        super(LaMa, self).__init__()\n        self.model_arch = \"LaMa\"\n        self.sub_type = \"Inpaint\"\n        self.in_nc = 4\n        self.out_nc = 3\n        self.scale = 1\n        self.min_size = None\n        self.pad_mod = 8",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.LaMa",
        "documentation": {}
    },
    {
        "label": "RRDBNet",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.RRDB",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.RRDB",
        "peekOfCode": "class RRDBNet(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        norm=None,\n        act: str = \"leakyrelu\",\n        upsampler: str = \"upconv\",\n        mode: B.ConvMode = \"CNA\",\n    ) -> None:\n        \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.RRDB",
        "documentation": {}
    },
    {
        "label": "WMSA",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "peekOfCode": "class WMSA(nn.Module):\n    \"\"\"Self-attention module in Swin Transformer\"\"\"\n    def __init__(self, input_dim, output_dim, head_dim, window_size, type):\n        super(WMSA, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.head_dim = head_dim\n        self.scale = self.head_dim**-0.5\n        self.n_heads = input_dim // head_dim\n        self.window_size = window_size",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        head_dim,\n        window_size,\n        drop_path,\n        type=\"W\",\n        input_resolution=None,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "documentation": {}
    },
    {
        "label": "ConvTransBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "peekOfCode": "class ConvTransBlock(nn.Module):\n    def __init__(\n        self,\n        conv_dim,\n        trans_dim,\n        head_dim,\n        window_size,\n        drop_path,\n        type=\"W\",\n        input_resolution=None,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "documentation": {}
    },
    {
        "label": "SCUNet",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "peekOfCode": "class SCUNet(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        in_nc=3,\n        config=[4, 4, 4, 4, 4, 4, 4],\n        dim=64,\n        drop_path_rate=0.0,\n        input_resolution=256,\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SCUNet",
        "documentation": {}
    },
    {
        "label": "Get_gradient_nopadding",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SPSR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SPSR",
        "peekOfCode": "class Get_gradient_nopadding(nn.Module):\n    def __init__(self):\n        super(Get_gradient_nopadding, self).__init__()\n        kernel_v = [[0, -1, 0], [0, 0, 0], [0, 1, 0]]\n        kernel_h = [[0, 0, 0], [-1, 0, 1], [0, 0, 0]]\n        kernel_h = torch.FloatTensor(kernel_h).unsqueeze(0).unsqueeze(0)\n        kernel_v = torch.FloatTensor(kernel_v).unsqueeze(0).unsqueeze(0)\n        self.weight_h = nn.Parameter(data=kernel_h, requires_grad=False)  # type: ignore\n        self.weight_v = nn.Parameter(data=kernel_v, requires_grad=False)  # type: ignore\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SPSR",
        "documentation": {}
    },
    {
        "label": "SPSRNet",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SPSR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SPSR",
        "peekOfCode": "class SPSRNet(nn.Module):\n    def __init__(\n        self,\n        state_dict,\n        norm=None,\n        act: str = \"leakyrelu\",\n        upsampler: str = \"upconv\",\n        mode: B.ConvMode = \"CNA\",\n    ):\n        super(SPSRNet, self).__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SPSR",
        "documentation": {}
    },
    {
        "label": "SRVGGNetCompact",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SRVGG",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SRVGG",
        "peekOfCode": "class SRVGGNetCompact(nn.Module):\n    \"\"\"A compact VGG-style network structure for super-resolution.\n    It is a compact network structure, which performs upsampling in the last layer and no convolution is\n    conducted on the HR feature space.\n    Args:\n        num_in_ch (int): Channel number of inputs. Default: 3.\n        num_out_ch (int): Channel number of outputs. Default: 3.\n        num_feat (int): Channel number of intermediate features. Default: 64.\n        num_conv (int): Number of convolution layers in the body network. Default: 16.\n        upscale (int): Upsampling factor. Default: 4.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SRVGG",
        "documentation": {}
    },
    {
        "label": "SeperableConv2d",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "peekOfCode": "class SeperableConv2d(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, padding=1, bias=True\n    ):\n        super(SeperableConv2d, self).__init__()\n        self.depthwise = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=kernel_size,\n            stride=stride,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "peekOfCode": "class ConvBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        use_act=True,\n        use_bn=True,\n        discriminator=False,\n        **kwargs,\n    ):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "documentation": {}
    },
    {
        "label": "UpsampleBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "peekOfCode": "class UpsampleBlock(nn.Module):\n    def __init__(self, in_channels, scale_factor):\n        super(UpsampleBlock, self).__init__()\n        self.conv = SeperableConv2d(\n            in_channels,\n            in_channels * scale_factor**2,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n        )",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n        self.block1 = ConvBlock(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.block2 = ConvBlock(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1, use_act=False\n        )\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "documentation": {}
    },
    {
        "label": "Generator",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "peekOfCode": "class Generator(nn.Module):\n    \"\"\"Swift-SRGAN Generator\n    Args:\n        in_channels (int): number of input image channels.\n        num_channels (int): number of hidden channels.\n        num_blocks (int): number of residual blocks.\n        upscale_factor (int): factor to upscale the image [2x, 4x, 8x].\n    Returns:\n        torch.Tensor: super resolution image\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwiftSRGAN",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class WindowAttention(nn.Module):\n    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    r\"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class PatchMerging(nn.Module):\n    r\"\"\"Patch Merging Layer.\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    r\"\"\"Image to Patch Embedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "RSTB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class RSTB(nn.Module):\n    \"\"\"Residual Swin Transformer Block (RSTB).\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "PatchUnEmbed",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class PatchUnEmbed(nn.Module):\n    r\"\"\"Image to Patch Unembedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "Upsample_hf",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class Upsample_hf(nn.Sequential):\n    \"\"\"Upsample module.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "UpsampleOneStep",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "Swin2SR",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "class Swin2SR(nn.Module):\n    r\"\"\"Swin2SR\n        A PyTorch impl of : `Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration`.\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.Swin2SR",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class WindowAttention(nn.Module):\n    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    r\"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class PatchMerging(nn.Module):\n    r\"\"\"Patch Merging Layer.\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "RSTB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class RSTB(nn.Module):\n    \"\"\"Residual Swin Transformer Block (RSTB).\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    r\"\"\"Image to Patch Embedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "PatchUnEmbed",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class PatchUnEmbed(nn.Module):\n    r\"\"\"Image to Patch Unembedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "UpsampleOneStep",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class UpsampleOneStep(nn.Sequential):\n    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n       Used in lightweight SR to save parameters.\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n        self.num_feat = num_feat\n        self.input_resolution = input_resolution",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "SwinIR",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "class SwinIR(nn.Module):\n    r\"\"\"SwinIR\n        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 64\n        patch_size (int | tuple(int)): Patch size. Default: 1\n        in_chans (int): Number of input image channels. Default: 3\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.SwinIR",
        "documentation": {}
    },
    {
        "label": "ConcatBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "class ConcatBlock(nn.Module):\n    # Concat the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ConcatBlock, self).__init__()\n        self.sub = submodule\n    def forward(self, x):\n        output = torch.cat((x, self.sub(x)), dim=1)\n        return output\n    def __repr__(self):\n        tmpstr = \"Identity .. \\n|\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "ShortcutBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "class ShortcutBlock(nn.Module):\n    # Elementwise sum the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ShortcutBlock, self).__init__()\n        self.sub = submodule\n    def forward(self, x):\n        output = x + self.sub(x)\n        return output\n    def __repr__(self):\n        tmpstr = \"Identity + \\n|\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "ShortcutBlockSPSR",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "class ShortcutBlockSPSR(nn.Module):\n    # Elementwise sum the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ShortcutBlockSPSR, self).__init__()\n        self.sub = submodule\n    def forward(self, x):\n        return x, self.sub\n    def __repr__(self):\n        tmpstr = \"Identity + \\n|\"\n        modstr = self.sub.__repr__().replace(\"\\n\", \"\\n|\")",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "ResNetBlock",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "class ResNetBlock(nn.Module):\n    \"\"\"\n    ResNet Block, 3-3 style\n    with extra residual scaling used in EDSR\n    (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW 17)\n    \"\"\"\n    def __init__(\n        self,\n        in_nc,\n        mid_nc,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "RRDB",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "class RRDB(nn.Module):\n    \"\"\"\n    Residual in Residual Dense Block\n    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n    \"\"\"\n    def __init__(\n        self,\n        nf,\n        kernel_size=3,\n        gc=32,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "ResidualDenseBlock_5C",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "class ResidualDenseBlock_5C(nn.Module):\n    \"\"\"\n    Residual Dense Block\n    style: 5 convs\n    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n    Modified options that can be used:\n        - \"Partial Convolution based Padding\" arXiv:1811.11718\n        - \"Spectral normalization\" arXiv:1802.05957\n        - \"ICASSP 2020 - ESRGAN+ : Further Improving ESRGAN\" N. C.\n            {Rakotonirina} and A. {Rasoanaivo}",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "act",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def act(act_type: str, inplace=True, neg_slope=0.2, n_prelu=1):\n    # helper selecting activation\n    # neg_slope: for leakyrelu and init of prelu\n    # n_prelu: for p_relu num_parameters\n    act_type = act_type.lower()\n    if act_type == \"relu\":\n        layer = nn.ReLU(inplace)\n    elif act_type == \"leakyrelu\":\n        layer = nn.LeakyReLU(neg_slope, inplace)\n    elif act_type == \"prelu\":",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "norm",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def norm(norm_type: str, nc: int):\n    # helper selecting normalization layer\n    norm_type = norm_type.lower()\n    if norm_type == \"batch\":\n        layer = nn.BatchNorm2d(nc, affine=True)\n    elif norm_type == \"instance\":\n        layer = nn.InstanceNorm2d(nc, affine=False)\n    else:\n        raise NotImplementedError(\n            \"normalization layer [{:s}] is not found\".format(norm_type)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def pad(pad_type: str, padding):\n    # helper selecting padding layer\n    # if padding is 'zero', do by conv layers\n    pad_type = pad_type.lower()\n    if padding == 0:\n        return None\n    if pad_type == \"reflect\":\n        layer = nn.ReflectionPad2d(padding)\n    elif pad_type == \"replicate\":\n        layer = nn.ReplicationPad2d(padding)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "get_valid_padding",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def get_valid_padding(kernel_size, dilation):\n    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n    padding = (kernel_size - 1) // 2\n    return padding\nclass ConcatBlock(nn.Module):\n    # Concat the output of a submodule to its input\n    def __init__(self, submodule):\n        super(ConcatBlock, self).__init__()\n        self.sub = submodule\n    def forward(self, x):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "sequential",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def sequential(*args):\n    # Flatten Sequential. It unwraps nn.Sequential.\n    if len(args) == 1:\n        if isinstance(args[0], OrderedDict):\n            raise NotImplementedError(\"sequential does not support OrderedDict input.\")\n        return args[0]  # No sequential is needed.\n    modules = []\n    for module in args:\n        if isinstance(module, nn.Sequential):\n            for submodule in module.children():",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "conv_block_2c2",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def conv_block_2c2(\n    in_nc,\n    out_nc,\n    act_type=\"relu\",\n):\n    return sequential(\n        nn.Conv2d(in_nc, out_nc, kernel_size=2, padding=1),\n        nn.Conv2d(out_nc, out_nc, kernel_size=2, padding=0),\n        act(act_type) if act_type else None,\n    )",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "conv_block",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def conv_block(\n    in_nc: int,\n    out_nc: int,\n    kernel_size,\n    stride=1,\n    dilation=1,\n    groups=1,\n    bias=True,\n    pad_type=\"zero\",\n    norm_type: str | None = None,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n####################\n# Upsampler\n####################\ndef pixelshuffle_block(\n    in_nc: int,\n    out_nc: int,\n    upscale_factor=2,\n    kernel_size=3,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "pixelshuffle_block",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def pixelshuffle_block(\n    in_nc: int,\n    out_nc: int,\n    upscale_factor=2,\n    kernel_size=3,\n    stride=1,\n    bias=True,\n    pad_type=\"zero\",\n    norm_type: str | None = None,\n    act_type=\"relu\",",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "upconv_block",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "def upconv_block(\n    in_nc: int,\n    out_nc: int,\n    upscale_factor=2,\n    kernel_size=3,\n    stride=1,\n    bias=True,\n    pad_type=\"zero\",\n    norm_type: str | None = None,\n    act_type=\"relu\",",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "ConvMode",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "peekOfCode": "ConvMode = Literal[\"CNA\", \"NAC\", \"CNAC\"]\n# 2x2x2 Conv Block\ndef conv_block_2c2(\n    in_nc,\n    out_nc,\n    act_type=\"relu\",\n):\n    return sequential(\n        nn.Conv2d(in_nc, out_nc, kernel_size=2, padding=1),\n        nn.Conv2d(out_nc, out_nc, kernel_size=2, padding=0),",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.architecture.block",
        "documentation": {}
    },
    {
        "label": "UnsupportedModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.model_loading",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.model_loading",
        "peekOfCode": "class UnsupportedModel(Exception):\n    pass\ndef load_state_dict(state_dict) -> PyTorchModel:\n    logger.debug(f\"Loading state dict into pytorch model arch\")\n    state_dict_keys = list(state_dict.keys())\n    if \"params_ema\" in state_dict_keys:\n        state_dict = state_dict[\"params_ema\"]\n    elif \"params-ema\" in state_dict_keys:\n        state_dict = state_dict[\"params-ema\"]\n    elif \"params\" in state_dict_keys:",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.model_loading",
        "documentation": {}
    },
    {
        "label": "load_state_dict",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.model_loading",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.model_loading",
        "peekOfCode": "def load_state_dict(state_dict) -> PyTorchModel:\n    logger.debug(f\"Loading state dict into pytorch model arch\")\n    state_dict_keys = list(state_dict.keys())\n    if \"params_ema\" in state_dict_keys:\n        state_dict = state_dict[\"params_ema\"]\n    elif \"params-ema\" in state_dict_keys:\n        state_dict = state_dict[\"params-ema\"]\n    elif \"params\" in state_dict_keys:\n        state_dict = state_dict[\"params\"]\n    state_dict_keys = list(state_dict.keys())",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.model_loading",
        "documentation": {}
    },
    {
        "label": "is_pytorch_sr_model",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "def is_pytorch_sr_model(model: object):\n    return isinstance(model, PyTorchSRModels)\nPyTorchFaceModels = (GFPGANv1Clean, RestoreFormer, CodeFormer)\nPyTorchFaceModel = Union[GFPGANv1Clean, RestoreFormer, CodeFormer]\ndef is_pytorch_face_model(model: object):\n    return isinstance(model, PyTorchFaceModels)\nPyTorchInpaintModels = (LaMa,)\nPyTorchInpaintModel = Union[LaMa]\ndef is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "is_pytorch_face_model",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "def is_pytorch_face_model(model: object):\n    return isinstance(model, PyTorchFaceModels)\nPyTorchInpaintModels = (LaMa,)\nPyTorchInpaintModel = Union[LaMa]\ndef is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)\nPyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\ndef is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "is_pytorch_inpaint_model",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "def is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)\nPyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\ndef is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "is_pytorch_model",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "def is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchSRModels",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchSRModels = (\n    RealESRGANv2,\n    SPSR,\n    SwiftSRGAN,\n    ESRGAN,\n    SwinIR,\n    Swin2SR,\n    HAT,\n    OmniSR,\n    SCUNet,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchSRModel",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchSRModel = Union[\n    RealESRGANv2,\n    SPSR,\n    SwiftSRGAN,\n    ESRGAN,\n    SwinIR,\n    Swin2SR,\n    HAT,\n    OmniSR,\n    SCUNet,",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchFaceModels",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchFaceModels = (GFPGANv1Clean, RestoreFormer, CodeFormer)\nPyTorchFaceModel = Union[GFPGANv1Clean, RestoreFormer, CodeFormer]\ndef is_pytorch_face_model(model: object):\n    return isinstance(model, PyTorchFaceModels)\nPyTorchInpaintModels = (LaMa,)\nPyTorchInpaintModel = Union[LaMa]\ndef is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)\nPyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchFaceModel",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchFaceModel = Union[GFPGANv1Clean, RestoreFormer, CodeFormer]\ndef is_pytorch_face_model(model: object):\n    return isinstance(model, PyTorchFaceModels)\nPyTorchInpaintModels = (LaMa,)\nPyTorchInpaintModel = Union[LaMa]\ndef is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)\nPyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\ndef is_pytorch_model(model: object):",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchInpaintModels",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchInpaintModels = (LaMa,)\nPyTorchInpaintModel = Union[LaMa]\ndef is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)\nPyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\ndef is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchInpaintModel",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchInpaintModel = Union[LaMa]\ndef is_pytorch_inpaint_model(model: object):\n    return isinstance(model, PyTorchInpaintModels)\nPyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\ndef is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchModels",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchModels = (*PyTorchSRModels, *PyTorchFaceModels, *PyTorchInpaintModels)\nPyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\ndef is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "PyTorchModel",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "description": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "peekOfCode": "PyTorchModel = Union[PyTorchSRModel, PyTorchFaceModel, PyTorchInpaintModel]\ndef is_pytorch_model(model: object):\n    return isinstance(model, PyTorchModels)",
        "detail": "Fooocus.backend.headless.fcbh_extras.chainner_models.types",
        "documentation": {}
    },
    {
        "label": "Canny",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "class Canny:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"image\": (\"IMAGE\",),\n                                \"low_threshold\": (\"FLOAT\", {\"default\": 0.4, \"min\": 0.01, \"max\": 0.99, \"step\": 0.01}),\n                                \"high_threshold\": (\"FLOAT\", {\"default\": 0.8, \"min\": 0.01, \"max\": 0.99, \"step\": 0.01})\n                                }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"detect_edge\"\n    CATEGORY = \"image/preprocessors\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "get_canny_nms_kernel",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "def get_canny_nms_kernel(device=None, dtype=None):\n    \"\"\"Utility function that returns 3x3 kernels for the Canny Non-maximal suppression.\"\"\"\n    return torch.tensor(\n        [\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, -1.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, -1.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [-1.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [-1.0, 1.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[-1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]],",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "get_hysteresis_kernel",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "def get_hysteresis_kernel(device=None, dtype=None):\n    \"\"\"Utility function that returns the 3x3 kernels for the Canny hysteresis.\"\"\"\n    return torch.tensor(\n        [\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 1.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [1.0, 0.0, 0.0]]],\n            [[[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 0.0]]],\n            [[[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]],",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "gaussian_blur_2d",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "def gaussian_blur_2d(img, kernel_size, sigma):\n    ksize_half = (kernel_size - 1) * 0.5\n    x = torch.linspace(-ksize_half, ksize_half, steps=kernel_size)\n    pdf = torch.exp(-0.5 * (x / sigma).pow(2))\n    x_kernel = pdf / pdf.sum()\n    x_kernel = x_kernel.to(device=img.device, dtype=img.dtype)\n    kernel2d = torch.mm(x_kernel[:, None], x_kernel[None, :])\n    kernel2d = kernel2d.expand(img.shape[-3], 1, kernel2d.shape[0], kernel2d.shape[1])\n    padding = [kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2]\n    img = torch.nn.functional.pad(img, padding, mode=\"reflect\")",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "get_sobel_kernel2d",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "def get_sobel_kernel2d(device=None, dtype=None):\n    kernel_x = torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 0.0, 2.0], [-1.0, 0.0, 1.0]], device=device, dtype=dtype)\n    kernel_y = kernel_x.transpose(0, 1)\n    return torch.stack([kernel_x, kernel_y])\ndef spatial_gradient(input, normalized: bool = True):\n    r\"\"\"Compute the first order image derivative in both x and y using a Sobel operator.\n    .. image:: _static/img/spatial_gradient.png\n    Args:\n        input: input image tensor with shape :math:`(B, C, H, W)`.\n        mode: derivatives modality, can be: `sobel` or `diff`.",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "spatial_gradient",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "def spatial_gradient(input, normalized: bool = True):\n    r\"\"\"Compute the first order image derivative in both x and y using a Sobel operator.\n    .. image:: _static/img/spatial_gradient.png\n    Args:\n        input: input image tensor with shape :math:`(B, C, H, W)`.\n        mode: derivatives modality, can be: `sobel` or `diff`.\n        order: the order of the derivatives.\n        normalized: whether the output is normalized.\n    Return:\n        the derivatives of the input feature map. with shape :math:`(B, C, 2, H, W)`.",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "rgb_to_grayscale",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "def rgb_to_grayscale(image, rgb_weights = None):\n    r\"\"\"Convert a RGB image to grayscale version of image.\n    .. image:: _static/img/rgb_to_grayscale.png\n    The image data is assumed to be in the range of (0, 1).\n    Args:\n        image: RGB image to be converted to grayscale with shape :math:`(*,3,H,W)`.\n        rgb_weights: Weights that will be applied on each channel (RGB).\n            The sum of the weights should add up to one.\n    Returns:\n        grayscale version of the image with shape :math:`(*,1,H,W)`.",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "canny",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "def canny(\n    input,\n    low_threshold = 0.1,\n    high_threshold = 0.2,\n    kernel_size  = 5,\n    sigma = 1,\n    hysteresis = True,\n    eps = 1e-6,\n):\n    r\"\"\"Find edges of the input image and filters them using the Canny algorithm.",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"Canny\": Canny,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_canny",
        "documentation": {}
    },
    {
        "label": "CLIPTextEncodeSDXLRefiner",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "peekOfCode": "class CLIPTextEncodeSDXLRefiner:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"ascore\": (\"FLOAT\", {\"default\": 6.0, \"min\": 0.0, \"max\": 1000.0, \"step\": 0.01}),\n            \"width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"text\": (\"STRING\", {\"multiline\": True}), \"clip\": (\"CLIP\", ),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "documentation": {}
    },
    {
        "label": "CLIPTextEncodeSDXL",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "peekOfCode": "class CLIPTextEncodeSDXL:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"crop_w\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"crop_h\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"target_width\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),\n            \"target_height\": (\"INT\", {\"default\": 1024.0, \"min\": 0, \"max\": MAX_RESOLUTION}),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"CLIPTextEncodeSDXLRefiner\": CLIPTextEncodeSDXLRefiner,\n    \"CLIPTextEncodeSDXL\": CLIPTextEncodeSDXL,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_clip_sdxl",
        "documentation": {}
    },
    {
        "label": "PorterDuffMode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "class PorterDuffMode(Enum):\n    ADD = 0\n    CLEAR = 1\n    DARKEN = 2\n    DST = 3\n    DST_ATOP = 4\n    DST_IN = 5\n    DST_OUT = 6\n    DST_OVER = 7\n    LIGHTEN = 8",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "PorterDuffImageComposite",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "class PorterDuffImageComposite:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"source\": (\"IMAGE\",),\n                \"source_alpha\": (\"MASK\",),\n                \"destination\": (\"IMAGE\",),\n                \"destination_alpha\": (\"MASK\",),\n                \"mode\": ([mode.name for mode in PorterDuffMode], {\"default\": PorterDuffMode.DST.name}),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "SplitImageWithAlpha",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "class SplitImageWithAlpha:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                }\n        }\n    CATEGORY = \"mask/compositing\"\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "JoinImageWithAlpha",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "class JoinImageWithAlpha:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"alpha\": (\"MASK\",),\n                }\n        }\n    CATEGORY = \"mask/compositing\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "resize_mask",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "def resize_mask(mask, shape):\n    return torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(shape[0], shape[1]), mode=\"bilinear\").squeeze(1)\nclass PorterDuffMode(Enum):\n    ADD = 0\n    CLEAR = 1\n    DARKEN = 2\n    DST = 3\n    DST_ATOP = 4\n    DST_IN = 5\n    DST_OUT = 6",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "porter_duff_composite",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "def porter_duff_composite(src_image: torch.Tensor, src_alpha: torch.Tensor, dst_image: torch.Tensor, dst_alpha: torch.Tensor, mode: PorterDuffMode):\n    if mode == PorterDuffMode.ADD:\n        out_alpha = torch.clamp(src_alpha + dst_alpha, 0, 1)\n        out_image = torch.clamp(src_image + dst_image, 0, 1)\n    elif mode == PorterDuffMode.CLEAR:\n        out_alpha = torch.zeros_like(dst_alpha)\n        out_image = torch.zeros_like(dst_image)\n    elif mode == PorterDuffMode.DARKEN:\n        out_alpha = src_alpha + dst_alpha  - src_alpha * dst_alpha\n        out_image = (1 - dst_alpha) * src_image + (1 - src_alpha) * dst_image + torch.min(src_image, dst_image)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"PorterDuffImageComposite\": PorterDuffImageComposite,\n    \"SplitImageWithAlpha\": SplitImageWithAlpha,\n    \"JoinImageWithAlpha\": JoinImageWithAlpha,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"PorterDuffImageComposite\": \"Porter-Duff Image Composite\",\n    \"SplitImageWithAlpha\": \"Split Image with Alpha\",\n    \"JoinImageWithAlpha\": \"Join Image with Alpha\",\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "NODE_DISPLAY_NAME_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "peekOfCode": "NODE_DISPLAY_NAME_MAPPINGS = {\n    \"PorterDuffImageComposite\": \"Porter-Duff Image Composite\",\n    \"SplitImageWithAlpha\": \"Split Image with Alpha\",\n    \"JoinImageWithAlpha\": \"Join Image with Alpha\",\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_compositing",
        "documentation": {}
    },
    {
        "label": "BasicScheduler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class BasicScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                     \"scheduler\": (fcbh.samplers.SCHEDULER_NAMES, ),\n                     \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "KarrasScheduler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class KarrasScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"rho\": (\"FLOAT\", {\"default\": 7.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                    }\n               }",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "ExponentialScheduler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class ExponentialScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                    }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "PolyexponentialScheduler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class PolyexponentialScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"sigma_max\": (\"FLOAT\", {\"default\": 14.614642, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"sigma_min\": (\"FLOAT\", {\"default\": 0.0291675, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"rho\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                    }\n               }",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "VPScheduler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class VPScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"beta_d\": (\"FLOAT\", {\"default\": 19.9, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}), #TODO: fix default values\n                     \"beta_min\": (\"FLOAT\", {\"default\": 0.1, \"min\": 0.0, \"max\": 1000.0, \"step\":0.01, \"round\": False}),\n                     \"eps_s\": (\"FLOAT\", {\"default\": 0.001, \"min\": 0.0, \"max\": 1.0, \"step\":0.0001, \"round\": False}),\n                    }\n               }",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "SplitSigmas",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class SplitSigmas:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sigmas\": (\"SIGMAS\", ),\n                    \"step\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10000}),\n                     }\n                }\n    RETURN_TYPES = (\"SIGMAS\",\"SIGMAS\")\n    CATEGORY = \"sampling/custom_sampling/sigmas\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "FlipSigmas",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class FlipSigmas:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sigmas\": (\"SIGMAS\", ),\n                     }\n                }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/sigmas\"\n    FUNCTION = \"get_sigmas\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "KSamplerSelect",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class KSamplerSelect:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sampler_name\": (fcbh.samplers.SAMPLER_NAMES, ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n    FUNCTION = \"get_sampler\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "SamplerDPMPP_2M_SDE",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class SamplerDPMPP_2M_SDE:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"solver_type\": (['midpoint', 'heun'], ),\n                     \"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"noise_device\": (['gpu', 'cpu'], ),\n                      }\n               }",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "SamplerDPMPP_SDE",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class SamplerDPMPP_SDE:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"eta\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"s_noise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"r\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 100.0, \"step\":0.01, \"round\": False}),\n                     \"noise_device\": (['gpu', 'cpu'], ),\n                      }\n               }",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "SamplerCustom",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "class SamplerCustom:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"add_noise\": (\"BOOLEAN\", {\"default\": True}),\n                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"positive\": (\"CONDITIONING\", ),\n                    \"negative\": (\"CONDITIONING\", ),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"SamplerCustom\": SamplerCustom,\n    \"BasicScheduler\": BasicScheduler,\n    \"KarrasScheduler\": KarrasScheduler,\n    \"ExponentialScheduler\": ExponentialScheduler,\n    \"PolyexponentialScheduler\": PolyexponentialScheduler,\n    \"VPScheduler\": VPScheduler,\n    \"KSamplerSelect\": KSamplerSelect,\n    \"SamplerDPMPP_2M_SDE\": SamplerDPMPP_2M_SDE,\n    \"SamplerDPMPP_SDE\": SamplerDPMPP_SDE,",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_custom_sampler",
        "documentation": {}
    },
    {
        "label": "FreeU",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "peekOfCode": "class FreeU:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"b1\": (\"FLOAT\", {\"default\": 1.1, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"b2\": (\"FLOAT\", {\"default\": 1.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s1\": (\"FLOAT\", {\"default\": 0.9, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s2\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "documentation": {}
    },
    {
        "label": "FreeU_V2",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "peekOfCode": "class FreeU_V2:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"b1\": (\"FLOAT\", {\"default\": 1.3, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"b2\": (\"FLOAT\", {\"default\": 1.4, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s1\": (\"FLOAT\", {\"default\": 0.9, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"s2\": (\"FLOAT\", {\"default\": 0.2, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "documentation": {}
    },
    {
        "label": "Fourier_filter",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "peekOfCode": "def Fourier_filter(x, threshold, scale):\n    # FFT\n    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))\n    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))\n    B, C, H, W = x_freq.shape\n    mask = torch.ones((B, C, H, W), device=x.device)\n    crow, ccol = H // 2, W //2\n    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale\n    x_freq = x_freq * mask\n    # IFFT",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"FreeU\": FreeU,\n    \"FreeU_V2\": FreeU_V2,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_freelunch",
        "documentation": {}
    },
    {
        "label": "HypernetworkLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "peekOfCode": "class HypernetworkLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"hypernetwork_name\": (folder_paths.get_filename_list(\"hypernetworks\"), ),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_hypernetwork\"\n    CATEGORY = \"loaders\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "documentation": {}
    },
    {
        "label": "load_hypernetwork_patch",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "peekOfCode": "def load_hypernetwork_patch(path, strength):\n    sd = fcbh.utils.load_torch_file(path, safe_load=True)\n    activation_func = sd.get('activation_func', 'linear')\n    is_layer_norm = sd.get('is_layer_norm', False)\n    use_dropout = sd.get('use_dropout', False)\n    activate_output = sd.get('activate_output', False)\n    last_layer_dropout = sd.get('last_layer_dropout', False)\n    valid_activation = {\n        \"linear\": torch.nn.Identity,\n        \"relu\": torch.nn.ReLU,",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"HypernetworkLoader\": HypernetworkLoader\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_hypernetwork",
        "documentation": {}
    },
    {
        "label": "HyperTile",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "peekOfCode": "class HyperTile:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                             \"tile_size\": (\"INT\", {\"default\": 256, \"min\": 1, \"max\": 2048}),\n                             \"swap_size\": (\"INT\", {\"default\": 2, \"min\": 1, \"max\": 128}),\n                             \"max_depth\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10}),\n                             \"scale_depth\": (\"BOOLEAN\", {\"default\": False}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "documentation": {}
    },
    {
        "label": "random_divisor",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "peekOfCode": "def random_divisor(value: int, min_value: int, /, max_options: int = 1, counter = 0) -> int:\n    min_value = min(min_value, value)\n    # All big divisors of value (inclusive)\n    divisors = [i for i in range(min_value, value + 1) if value % i == 0]\n    ns = [value // i for i in divisors[:max_options]]  # has at least 1 element\n    random.seed(counter)\n    idx = random.randint(0, len(ns) - 1)\n    return ns[idx]\nclass HyperTile:\n    @classmethod",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"HyperTile\": HyperTile,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_hypertile",
        "documentation": {}
    },
    {
        "label": "ImageCrop",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "peekOfCode": "class ImageCrop:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "documentation": {}
    },
    {
        "label": "RepeatImageBatch",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "peekOfCode": "class RepeatImageBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"amount\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"repeat\"\n    CATEGORY = \"image/batch\"\n    def repeat(self, image, amount):",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "documentation": {}
    },
    {
        "label": "SaveAnimatedWEBP",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "peekOfCode": "class SaveAnimatedWEBP:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n    methods = {\"default\": 4, \"fastest\": 0, \"slowest\": 6}\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "documentation": {}
    },
    {
        "label": "MAX_RESOLUTION",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "peekOfCode": "MAX_RESOLUTION = nodes.MAX_RESOLUTION\nclass ImageCrop:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              }}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"ImageCrop\": ImageCrop,\n    \"RepeatImageBatch\": RepeatImageBatch,\n    \"SaveAnimatedWEBP\": SaveAnimatedWEBP,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_images",
        "documentation": {}
    },
    {
        "label": "LatentAdd",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "peekOfCode": "class LatentAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n    CATEGORY = \"latent/advanced\"\n    def op(self, samples1, samples2):\n        samples_out = samples1.copy()\n        s1 = samples1[\"samples\"]",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "documentation": {}
    },
    {
        "label": "LatentSubtract",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "peekOfCode": "class LatentSubtract:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n    CATEGORY = \"latent/advanced\"\n    def op(self, samples1, samples2):\n        samples_out = samples1.copy()\n        s1 = samples1[\"samples\"]",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "documentation": {}
    },
    {
        "label": "LatentMultiply",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "peekOfCode": "class LatentMultiply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n    CATEGORY = \"latent/advanced\"\n    def op(self, samples, multiplier):",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "documentation": {}
    },
    {
        "label": "LatentInterpolate",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "peekOfCode": "class LatentInterpolate:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",),\n                              \"samples2\": (\"LATENT\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n    CATEGORY = \"latent/advanced\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "documentation": {}
    },
    {
        "label": "reshape_latent_to",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "peekOfCode": "def reshape_latent_to(target_shape, latent):\n    if latent.shape[1:] != target_shape[1:]:\n        latent.movedim(1, -1)\n        latent = fcbh.utils.common_upscale(latent, target_shape[3], target_shape[2], \"bilinear\", \"center\")\n        latent.movedim(-1, 1)\n    return fcbh.utils.repeat_to_batch_size(latent, target_shape[0])\nclass LatentAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples1\": (\"LATENT\",), \"samples2\": (\"LATENT\",)}}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"LatentAdd\": LatentAdd,\n    \"LatentSubtract\": LatentSubtract,\n    \"LatentMultiply\": LatentMultiply,\n    \"LatentInterpolate\": LatentInterpolate,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_latent",
        "documentation": {}
    },
    {
        "label": "LatentCompositeMasked",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class LatentCompositeMasked:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"destination\": (\"LATENT\",),\n                \"source\": (\"LATENT\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"resize_source\": (\"BOOLEAN\", {\"default\": False}),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "ImageCompositeMasked",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class ImageCompositeMasked:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"destination\": (\"IMAGE\",),\n                \"source\": (\"IMAGE\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"resize_source\": (\"BOOLEAN\", {\"default\": False}),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "MaskToImage",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class MaskToImage:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"mask\": (\"MASK\",),\n                }\n        }\n    CATEGORY = \"mask\"\n    RETURN_TYPES = (\"IMAGE\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "ImageToMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class ImageToMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"channel\": ([\"red\", \"green\", \"blue\", \"alpha\"],),\n                }\n        }\n    CATEGORY = \"mask\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "ImageColorToMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class ImageColorToMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n                \"required\": {\n                    \"image\": (\"IMAGE\",),\n                    \"color\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xFFFFFF, \"step\": 1, \"display\": \"color\"}),\n                }\n        }\n    CATEGORY = \"mask\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "SolidMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class SolidMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"value\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n            }\n        }",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "InvertMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class InvertMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n            }\n        }\n    CATEGORY = \"mask\"\n    RETURN_TYPES = (\"MASK\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "CropMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class CropMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "MaskComposite",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class MaskComposite:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"destination\": (\"MASK\",),\n                \"source\": (\"MASK\",),\n                \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"operation\": ([\"multiply\", \"add\", \"subtract\", \"and\", \"or\", \"xor\"],),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "FeatherMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class FeatherMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"left\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"top\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"right\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"bottom\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "GrowMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "class GrowMask:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"expand\": (\"INT\", {\"default\": 0, \"min\": -MAX_RESOLUTION, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                \"tapered_corners\": (\"BOOLEAN\", {\"default\": True}),\n            },\n        }",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "composite",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "def composite(destination, source, x, y, mask = None, multiplier = 8, resize_source = False):\n    if resize_source:\n        source = torch.nn.functional.interpolate(source, size=(destination.shape[2], destination.shape[3]), mode=\"bilinear\")\n    source = fcbh.utils.repeat_to_batch_size(source, destination.shape[0])\n    x = max(-source.shape[3] * multiplier, min(x, destination.shape[3] * multiplier))\n    y = max(-source.shape[2] * multiplier, min(y, destination.shape[2] * multiplier))\n    left, top = (x // multiplier, y // multiplier)\n    right, bottom = (left + source.shape[3], top + source.shape[2],)\n    if mask is None:\n        mask = torch.ones_like(source)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"LatentCompositeMasked\": LatentCompositeMasked,\n    \"ImageCompositeMasked\": ImageCompositeMasked,\n    \"MaskToImage\": MaskToImage,\n    \"ImageToMask\": ImageToMask,\n    \"ImageColorToMask\": ImageColorToMask,\n    \"SolidMask\": SolidMask,\n    \"InvertMask\": InvertMask,\n    \"CropMask\": CropMask,\n    \"MaskComposite\": MaskComposite,",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "NODE_DISPLAY_NAME_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "peekOfCode": "NODE_DISPLAY_NAME_MAPPINGS = {\n    \"ImageToMask\": \"Convert Image to Mask\",\n    \"MaskToImage\": \"Convert Mask to Image\",\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_mask",
        "documentation": {}
    },
    {
        "label": "LCM",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "peekOfCode": "class LCM(fcbh.model_sampling.EPS):\n    def calculate_denoised(self, sigma, model_output, model_input):\n        timestep = self.timestep(sigma).view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        sigma = sigma.view(sigma.shape[:1] + (1,) * (model_output.ndim - 1))\n        x0 = model_input - model_output * sigma\n        sigma_data = 0.5\n        scaled_timestep = timestep * 10.0 #timestep_scaling\n        c_skip = sigma_data**2 / (scaled_timestep**2 + sigma_data**2)\n        c_out = scaled_timestep / (scaled_timestep**2 + sigma_data**2) ** 0.5\n        return c_out * x0 + c_skip * model_input",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "documentation": {}
    },
    {
        "label": "ModelSamplingDiscreteLCM",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "peekOfCode": "class ModelSamplingDiscreteLCM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sigma_data = 1.0\n        timesteps = 1000\n        beta_start = 0.00085\n        beta_end = 0.012\n        betas = torch.linspace(beta_start**0.5, beta_end**0.5, timesteps, dtype=torch.float32) ** 2\n        alphas = 1.0 - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "documentation": {}
    },
    {
        "label": "ModelSamplingDiscrete",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "peekOfCode": "class ModelSamplingDiscrete:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"sampling\": ([\"eps\", \"v_prediction\", \"lcm\"],),\n                              \"zsnr\": (\"BOOLEAN\", {\"default\": False}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n    CATEGORY = \"advanced/model\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "documentation": {}
    },
    {
        "label": "RescaleCFG",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "peekOfCode": "class RescaleCFG:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 0.7, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n    CATEGORY = \"advanced/model\"\n    def patch(self, model, multiplier):",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "documentation": {}
    },
    {
        "label": "rescale_zero_terminal_snr_sigmas",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "peekOfCode": "def rescale_zero_terminal_snr_sigmas(sigmas):\n    alphas_cumprod = 1 / ((sigmas * sigmas) + 1)\n    alphas_bar_sqrt = alphas_cumprod.sqrt()\n    # Store old values.\n    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\n    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\n    # Shift so the last timestep is zero.\n    alphas_bar_sqrt -= (alphas_bar_sqrt_T)\n    # Scale so the first timestep is back to the old value.\n    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"ModelSamplingDiscrete\": ModelSamplingDiscrete,\n    \"RescaleCFG\": RescaleCFG,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_advanced",
        "documentation": {}
    },
    {
        "label": "PatchModelAddDownscale",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "peekOfCode": "class PatchModelAddDownscale:\n    upscale_methods = [\"bicubic\", \"nearest-exact\", \"bilinear\", \"area\", \"bislerp\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"block_number\": (\"INT\", {\"default\": 3, \"min\": 1, \"max\": 32, \"step\": 1}),\n                              \"downscale_factor\": (\"FLOAT\", {\"default\": 2.0, \"min\": 0.1, \"max\": 9.0, \"step\": 0.001}),\n                              \"start_percent\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                              \"end_percent\": (\"FLOAT\", {\"default\": 0.35, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                              \"downscale_after_skip\": (\"BOOLEAN\", {\"default\": True}),",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"PatchModelAddDownscale\": PatchModelAddDownscale,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    # Sampling\n    \"PatchModelAddDownscale\": \"PatchModelAddDownscale (Kohya Deep Shrink)\",\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "documentation": {}
    },
    {
        "label": "NODE_DISPLAY_NAME_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "peekOfCode": "NODE_DISPLAY_NAME_MAPPINGS = {\n    # Sampling\n    \"PatchModelAddDownscale\": \"PatchModelAddDownscale (Kohya Deep Shrink)\",\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_downscale",
        "documentation": {}
    },
    {
        "label": "ModelMergeSimple",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class ModelMergeSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n    CATEGORY = \"advanced/model_merging\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "ModelSubtract",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class ModelSubtract:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"multiplier\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n    CATEGORY = \"advanced/model_merging\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "ModelAdd",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class ModelAdd:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"merge\"\n    CATEGORY = \"advanced/model_merging\"\n    def merge(self, model1, model2):",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "CLIPMergeSimple",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class CLIPMergeSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip1\": (\"CLIP\",),\n                              \"clip2\": (\"CLIP\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"merge\"\n    CATEGORY = \"advanced/model_merging\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "ModelMergeBlocks",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class ModelMergeBlocks:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model1\": (\"MODEL\",),\n                              \"model2\": (\"MODEL\",),\n                              \"input\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              \"middle\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              \"out\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n                              }}\n    RETURN_TYPES = (\"MODEL\",)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "CheckpointSave",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class CheckpointSave:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip\": (\"CLIP\",),\n                              \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"checkpoints/fcbh_backend\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "CLIPSave",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class CLIPSave:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip\": (\"CLIP\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"clip/fcbh_backend\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "VAESave",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "class VAESave:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"vae\": (\"VAE\",),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"vae/fcbh_backend_vae\"}),},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},}\n    RETURN_TYPES = ()\n    FUNCTION = \"save\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"ModelMergeSimple\": ModelMergeSimple,\n    \"ModelMergeBlocks\": ModelMergeBlocks,\n    \"ModelMergeSubtract\": ModelSubtract,\n    \"ModelMergeAdd\": ModelAdd,\n    \"CheckpointSave\": CheckpointSave,\n    \"CLIPMergeSimple\": CLIPMergeSimple,\n    \"CLIPSave\": CLIPSave,\n    \"VAESave\": VAESave,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_model_merging",
        "documentation": {}
    },
    {
        "label": "Blend",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "peekOfCode": "class Blend:\n    def __init__(self):\n        pass\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image1\": (\"IMAGE\",),\n                \"image2\": (\"IMAGE\",),\n                \"blend_factor\": (\"FLOAT\", {",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "documentation": {}
    },
    {
        "label": "Blur",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "peekOfCode": "class Blur:\n    def __init__(self):\n        pass\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"blur_radius\": (\"INT\", {\n                    \"default\": 1,",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "documentation": {}
    },
    {
        "label": "Quantize",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "peekOfCode": "class Quantize:\n    def __init__(self):\n        pass\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"colors\": (\"INT\", {\n                    \"default\": 256,",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "documentation": {}
    },
    {
        "label": "Sharpen",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "peekOfCode": "class Sharpen:\n    def __init__(self):\n        pass\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"sharpen_radius\": (\"INT\", {\n                    \"default\": 1,",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "documentation": {}
    },
    {
        "label": "ImageScaleToTotalPixels",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "peekOfCode": "class ImageScaleToTotalPixels:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n    crop_methods = [\"disabled\", \"center\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"megapixels\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.01, \"max\": 16.0, \"step\": 0.01}),\n                            }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "documentation": {}
    },
    {
        "label": "gaussian_kernel",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "peekOfCode": "def gaussian_kernel(kernel_size: int, sigma: float, device=None):\n    x, y = torch.meshgrid(torch.linspace(-1, 1, kernel_size, device=device), torch.linspace(-1, 1, kernel_size, device=device), indexing=\"ij\")\n    d = torch.sqrt(x * x + y * y)\n    g = torch.exp(-(d * d) / (2.0 * sigma * sigma))\n    return g / g.sum()\nclass Blur:\n    def __init__(self):\n        pass\n    @classmethod\n    def INPUT_TYPES(s):",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"ImageBlend\": Blend,\n    \"ImageBlur\": Blur,\n    \"ImageQuantize\": Quantize,\n    \"ImageSharpen\": Sharpen,\n    \"ImageScaleToTotalPixels\": ImageScaleToTotalPixels,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_post_processing",
        "documentation": {}
    },
    {
        "label": "LatentRebatch",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "peekOfCode": "class LatentRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"latents\": (\"LATENT\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n    FUNCTION = \"rebatch\"",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"RebatchLatents\": LatentRebatch,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"RebatchLatents\": \"Rebatch Latents\",\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "documentation": {}
    },
    {
        "label": "NODE_DISPLAY_NAME_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "peekOfCode": "NODE_DISPLAY_NAME_MAPPINGS = {\n    \"RebatchLatents\": \"Rebatch Latents\",\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_rebatch",
        "documentation": {}
    },
    {
        "label": "TomePatchModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "peekOfCode": "class TomePatchModel:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"ratio\": (\"FLOAT\", {\"default\": 0.3, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"patch\"\n    CATEGORY = \"_for_testing\"\n    def patch(self, model, ratio):",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "documentation": {}
    },
    {
        "label": "do_nothing",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "peekOfCode": "def do_nothing(x: torch.Tensor, mode:str=None):\n    return x\ndef mps_gather_workaround(input, dim, index):\n    if input.shape[-1] == 1:\n        return torch.gather(\n            input.unsqueeze(-1),\n            dim - 1 if dim < 0 else dim,\n            index.unsqueeze(-1)\n        ).squeeze(-1)\n    else:",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "documentation": {}
    },
    {
        "label": "mps_gather_workaround",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "peekOfCode": "def mps_gather_workaround(input, dim, index):\n    if input.shape[-1] == 1:\n        return torch.gather(\n            input.unsqueeze(-1),\n            dim - 1 if dim < 0 else dim,\n            index.unsqueeze(-1)\n        ).squeeze(-1)\n    else:\n        return torch.gather(input, dim, index)\ndef bipartite_soft_matching_random2d(metric: torch.Tensor,",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "documentation": {}
    },
    {
        "label": "bipartite_soft_matching_random2d",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "peekOfCode": "def bipartite_soft_matching_random2d(metric: torch.Tensor,\n                                     w: int, h: int, sx: int, sy: int, r: int,\n                                     no_rand: bool = False) -> Tuple[Callable, Callable]:\n    \"\"\"\n    Partitions the tokens into src and dst and merges r tokens from src to dst.\n    Dst tokens are partitioned by choosing one randomy in each (sx, sy) region.\n    Args:\n     - metric [B, N, C]: metric to use for similarity\n     - w: image width in tokens\n     - h: image height in tokens",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "documentation": {}
    },
    {
        "label": "get_functions",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "peekOfCode": "def get_functions(x, ratio, original_shape):\n    b, c, original_h, original_w = original_shape\n    original_tokens = original_h * original_w\n    downsample = int(math.ceil(math.sqrt(original_tokens // x.shape[1])))\n    stride_x = 2\n    stride_y = 2\n    max_downsample = 1\n    if downsample <= max_downsample:\n        w = int(math.ceil(original_w / downsample))\n        h = int(math.ceil(original_h / downsample))",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"TomePatchModel\": TomePatchModel,\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_tomesd",
        "documentation": {}
    },
    {
        "label": "UpscaleModelLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "peekOfCode": "class UpscaleModelLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model_name\": (folder_paths.get_filename_list(\"upscale_models\"), ),\n                             }}\n    RETURN_TYPES = (\"UPSCALE_MODEL\",)\n    FUNCTION = \"load_model\"\n    CATEGORY = \"loaders\"\n    def load_model(self, model_name):\n        model_path = folder_paths.get_full_path(\"upscale_models\", model_name)",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "documentation": {}
    },
    {
        "label": "ImageUpscaleWithModel",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "peekOfCode": "class ImageUpscaleWithModel:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"upscale_model\": (\"UPSCALE_MODEL\",),\n                              \"image\": (\"IMAGE\",),\n                              }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n    CATEGORY = \"image/upscaling\"\n    def upscale(self, upscale_model, image):",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "description": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"UpscaleModelLoader\": UpscaleModelLoader,\n    \"ImageUpscaleWithModel\": ImageUpscaleWithModel\n}",
        "detail": "Fooocus.backend.headless.fcbh_extras.nodes_upscale_model",
        "documentation": {}
    },
    {
        "label": "set_output_directory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir\ndef set_temp_directory(temp_dir):\n    global temp_directory\n    temp_directory = temp_dir\ndef set_input_directory(input_dir):\n    global input_directory\n    input_directory = input_dir\ndef get_output_directory():",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "set_temp_directory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def set_temp_directory(temp_dir):\n    global temp_directory\n    temp_directory = temp_dir\ndef set_input_directory(input_dir):\n    global input_directory\n    input_directory = input_dir\ndef get_output_directory():\n    global output_directory\n    return output_directory\ndef get_temp_directory():",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "set_input_directory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def set_input_directory(input_dir):\n    global input_directory\n    input_directory = input_dir\ndef get_output_directory():\n    global output_directory\n    return output_directory\ndef get_temp_directory():\n    global temp_directory\n    return temp_directory\ndef get_input_directory():",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_output_directory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_output_directory():\n    global output_directory\n    return output_directory\ndef get_temp_directory():\n    global temp_directory\n    return temp_directory\ndef get_input_directory():\n    global input_directory\n    return input_directory\n#NOTE: used in http server so don't put folders that should not be accessed remotely",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_temp_directory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_temp_directory():\n    global temp_directory\n    return temp_directory\ndef get_input_directory():\n    global input_directory\n    return input_directory\n#NOTE: used in http server so don't put folders that should not be accessed remotely\ndef get_directory_by_type(type_name):\n    if type_name == \"output\":\n        return get_output_directory()",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_input_directory",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_input_directory():\n    global input_directory\n    return input_directory\n#NOTE: used in http server so don't put folders that should not be accessed remotely\ndef get_directory_by_type(type_name):\n    if type_name == \"output\":\n        return get_output_directory()\n    if type_name == \"temp\":\n        return get_temp_directory()\n    if type_name == \"input\":",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_directory_by_type",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_directory_by_type(type_name):\n    if type_name == \"output\":\n        return get_output_directory()\n    if type_name == \"temp\":\n        return get_temp_directory()\n    if type_name == \"input\":\n        return get_input_directory()\n    return None\n# determine base_dir rely on annotation if name is 'filename.ext [annotation]' format\n# otherwise use default_path as base_dir",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "annotated_filepath",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def annotated_filepath(name):\n    if name.endswith(\"[output]\"):\n        base_dir = get_output_directory()\n        name = name[:-9]\n    elif name.endswith(\"[input]\"):\n        base_dir = get_input_directory()\n        name = name[:-8]\n    elif name.endswith(\"[temp]\"):\n        base_dir = get_temp_directory()\n        name = name[:-7]",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_annotated_filepath",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_annotated_filepath(name, default_dir=None):\n    name, base_dir = annotated_filepath(name)\n    if base_dir is None:\n        if default_dir is not None:\n            base_dir = default_dir\n        else:\n            base_dir = get_input_directory()  # fallback path\n    return os.path.join(base_dir, name)\ndef exists_annotated_filepath(name):\n    name, base_dir = annotated_filepath(name)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "exists_annotated_filepath",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def exists_annotated_filepath(name):\n    name, base_dir = annotated_filepath(name)\n    if base_dir is None:\n        base_dir = get_input_directory()  # fallback path\n    filepath = os.path.join(base_dir, name)\n    return os.path.exists(filepath)\ndef add_model_folder_path(folder_name, full_folder_path):\n    global folder_names_and_paths\n    if folder_name in folder_names_and_paths:\n        folder_names_and_paths[folder_name][0].append(full_folder_path)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "add_model_folder_path",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def add_model_folder_path(folder_name, full_folder_path):\n    global folder_names_and_paths\n    if folder_name in folder_names_and_paths:\n        folder_names_and_paths[folder_name][0].append(full_folder_path)\n    else:\n        folder_names_and_paths[folder_name] = ([full_folder_path], set())\ndef get_folder_paths(folder_name):\n    return folder_names_and_paths[folder_name][0][:]\ndef recursive_search(directory, excluded_dir_names=None):\n    if not os.path.isdir(directory):",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_folder_paths",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_folder_paths(folder_name):\n    return folder_names_and_paths[folder_name][0][:]\ndef recursive_search(directory, excluded_dir_names=None):\n    if not os.path.isdir(directory):\n        return [], {}\n    if excluded_dir_names is None:\n        excluded_dir_names = []\n    result = []\n    dirs = {directory: os.path.getmtime(directory)}\n    for dirpath, subdirs, filenames in os.walk(directory, followlinks=True, topdown=True):",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "recursive_search",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def recursive_search(directory, excluded_dir_names=None):\n    if not os.path.isdir(directory):\n        return [], {}\n    if excluded_dir_names is None:\n        excluded_dir_names = []\n    result = []\n    dirs = {directory: os.path.getmtime(directory)}\n    for dirpath, subdirs, filenames in os.walk(directory, followlinks=True, topdown=True):\n        subdirs[:] = [d for d in subdirs if d not in excluded_dir_names]\n        for file_name in filenames:",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "filter_files_extensions",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def filter_files_extensions(files, extensions):\n    return sorted(list(filter(lambda a: os.path.splitext(a)[-1].lower() in extensions or len(extensions) == 0, files)))\ndef get_full_path(folder_name, filename):\n    global folder_names_and_paths\n    if folder_name not in folder_names_and_paths:\n        return None\n    folders = folder_names_and_paths[folder_name]\n    filename = os.path.relpath(os.path.join(\"/\", filename), \"/\")\n    for x in folders[0]:\n        full_path = os.path.join(x, filename)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_full_path",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_full_path(folder_name, filename):\n    global folder_names_and_paths\n    if folder_name not in folder_names_and_paths:\n        return None\n    folders = folder_names_and_paths[folder_name]\n    filename = os.path.relpath(os.path.join(\"/\", filename), \"/\")\n    for x in folders[0]:\n        full_path = os.path.join(x, filename)\n        if os.path.isfile(full_path):\n            return full_path",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_filename_list_",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_filename_list_(folder_name):\n    global folder_names_and_paths\n    output_list = set()\n    folders = folder_names_and_paths[folder_name]\n    output_folders = {}\n    for x in folders[0]:\n        files, folders_all = recursive_search(x, excluded_dir_names=[\".git\"])\n        output_list.update(filter_files_extensions(files, folders[1]))\n        output_folders = {**output_folders, **folders_all}\n    return (sorted(list(output_list)), output_folders, time.perf_counter())",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "cached_filename_list_",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def cached_filename_list_(folder_name):\n    global filename_list_cache\n    global folder_names_and_paths\n    if folder_name not in filename_list_cache:\n        return None\n    out = filename_list_cache[folder_name]\n    if time.perf_counter() < (out[2] + 0.5):\n        return out\n    for x in out[1]:\n        time_modified = out[1][x]",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_filename_list",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_filename_list(folder_name):\n    out = cached_filename_list_(folder_name)\n    if out is None:\n        out = get_filename_list_(folder_name)\n        global filename_list_cache\n        filename_list_cache[folder_name] = out\n    return list(out[0])\ndef get_save_image_path(filename_prefix, output_dir, image_width=0, image_height=0):\n    def map_filename(filename):\n        prefix_len = len(os.path.basename(filename_prefix))",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "get_save_image_path",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "def get_save_image_path(filename_prefix, output_dir, image_width=0, image_height=0):\n    def map_filename(filename):\n        prefix_len = len(os.path.basename(filename_prefix))\n        prefix = filename[:prefix_len + 1]\n        try:\n            digits = int(filename[prefix_len + 1:].split('_')[0])\n        except:\n            digits = 0\n        return (digits, prefix)\n    def compute_vars(input, image_width, image_height):",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "supported_pt_extensions",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "supported_pt_extensions = set(['.ckpt', '.pt', '.bin', '.pth', '.safetensors'])\nfolder_names_and_paths = {}\nbase_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nmodels_dir = os.path.join(base_path, \"models\")\nfolder_names_and_paths[\"checkpoints\"] = ([os.path.join(models_dir, \"checkpoints\")], supported_pt_extensions)\nfolder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths = {}\nbase_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nmodels_dir = os.path.join(base_path, \"models\")\nfolder_names_and_paths[\"checkpoints\"] = ([os.path.join(models_dir, \"checkpoints\")], supported_pt_extensions)\nfolder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "base_path",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nmodels_dir = os.path.join(base_path, \"models\")\nfolder_names_and_paths[\"checkpoints\"] = ([os.path.join(models_dir, \"checkpoints\")], supported_pt_extensions)\nfolder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "models_dir",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "models_dir = os.path.join(base_path, \"models\")\nfolder_names_and_paths[\"checkpoints\"] = ([os.path.join(models_dir, \"checkpoints\")], supported_pt_extensions)\nfolder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"checkpoints\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"checkpoints\"] = ([os.path.join(models_dir, \"checkpoints\")], supported_pt_extensions)\nfolder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"configs\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"configs\"] = ([os.path.join(models_dir, \"configs\")], [\".yaml\"])\nfolder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"loras\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"loras\"] = ([os.path.join(models_dir, \"loras\")], supported_pt_extensions)\nfolder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"vae\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"vae\"] = ([os.path.join(models_dir, \"vae\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"clip\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"clip\"] = ([os.path.join(models_dir, \"clip\")], supported_pt_extensions)\nfolder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"unet\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"unet\"] = ([os.path.join(models_dir, \"unet\")], supported_pt_extensions)\nfolder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"clip_vision\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"clip_vision\"] = ([os.path.join(models_dir, \"clip_vision\")], supported_pt_extensions)\nfolder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"style_models\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"style_models\"] = ([os.path.join(models_dir, \"style_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"embeddings\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"embeddings\"] = ([os.path.join(models_dir, \"embeddings\")], supported_pt_extensions)\nfolder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"diffusers\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"diffusers\"] = ([os.path.join(models_dir, \"diffusers\")], [\"folder\"])\nfolder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"vae_approx\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"vae_approx\"] = ([os.path.join(models_dir, \"vae_approx\")], supported_pt_extensions)\nfolder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"controlnet\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"controlnet\"] = ([os.path.join(models_dir, \"controlnet\"), os.path.join(models_dir, \"t2i_adapter\")], supported_pt_extensions)\nfolder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"gligen\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"gligen\"] = ([os.path.join(models_dir, \"gligen\")], supported_pt_extensions)\nfolder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"upscale_models\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"upscale_models\"] = ([os.path.join(models_dir, \"upscale_models\")], supported_pt_extensions)\nfolder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"custom_nodes\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"custom_nodes\"] = ([os.path.join(base_path, \"custom_nodes\")], [])\nfolder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)\ndef set_output_directory(output_dir):",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"hypernetworks\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"hypernetworks\"] = ([os.path.join(models_dir, \"hypernetworks\")], supported_pt_extensions)\nfolder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)\ndef set_output_directory(output_dir):\n    global output_directory",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "folder_names_and_paths[\"classifiers\"]",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "folder_names_and_paths[\"classifiers\"] = ([os.path.join(models_dir, \"classifiers\")], {\"\"})\noutput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)\ndef set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "output_directory",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "output_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"output\")\ntemp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)\ndef set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir\ndef set_temp_directory(temp_dir):",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "temp_directory",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "temp_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"temp\")\ninput_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)\ndef set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir\ndef set_temp_directory(temp_dir):\n    global temp_directory",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "input_directory",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "input_directory = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), \"input\")\nfilename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)\ndef set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir\ndef set_temp_directory(temp_dir):\n    global temp_directory\n    temp_directory = temp_dir",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "filename_list_cache",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.folder_paths",
        "description": "Fooocus.backend.headless.folder_paths",
        "peekOfCode": "filename_list_cache = {}\nif not os.path.exists(input_directory):\n    os.makedirs(input_directory)\ndef set_output_directory(output_dir):\n    global output_directory\n    output_directory = output_dir\ndef set_temp_directory(temp_dir):\n    global temp_directory\n    temp_directory = temp_dir\ndef set_input_directory(input_dir):",
        "detail": "Fooocus.backend.headless.folder_paths",
        "documentation": {}
    },
    {
        "label": "LatentPreviewer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.latent_preview",
        "description": "Fooocus.backend.headless.latent_preview",
        "peekOfCode": "class LatentPreviewer:\n    def decode_latent_to_preview(self, x0):\n        pass\n    def decode_latent_to_preview_image(self, preview_format, x0):\n        preview_image = self.decode_latent_to_preview(x0)\n        return (\"JPEG\", preview_image, MAX_PREVIEW_RESOLUTION)\nclass TAESDPreviewerImpl(LatentPreviewer):\n    def __init__(self, taesd):\n        self.taesd = taesd\n    def decode_latent_to_preview(self, x0):",
        "detail": "Fooocus.backend.headless.latent_preview",
        "documentation": {}
    },
    {
        "label": "TAESDPreviewerImpl",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.latent_preview",
        "description": "Fooocus.backend.headless.latent_preview",
        "peekOfCode": "class TAESDPreviewerImpl(LatentPreviewer):\n    def __init__(self, taesd):\n        self.taesd = taesd\n    def decode_latent_to_preview(self, x0):\n        x_sample = self.taesd.decode(x0[:1])[0].detach()\n        x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)\n        x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n        x_sample = x_sample.astype(np.uint8)\n        preview_image = Image.fromarray(x_sample)\n        return preview_image",
        "detail": "Fooocus.backend.headless.latent_preview",
        "documentation": {}
    },
    {
        "label": "Latent2RGBPreviewer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.latent_preview",
        "description": "Fooocus.backend.headless.latent_preview",
        "peekOfCode": "class Latent2RGBPreviewer(LatentPreviewer):\n    def __init__(self, latent_rgb_factors):\n        self.latent_rgb_factors = torch.tensor(latent_rgb_factors, device=\"cpu\")\n    def decode_latent_to_preview(self, x0):\n        latent_image = x0[0].permute(1, 2, 0).cpu() @ self.latent_rgb_factors\n        latents_ubyte = (((latent_image + 1) / 2)\n                            .clamp(0, 1)  # change scale from -1..1 to 0..1\n                            .mul(0xFF)  # to 0..255\n                            .byte()).cpu()\n        return Image.fromarray(latents_ubyte.numpy())",
        "detail": "Fooocus.backend.headless.latent_preview",
        "documentation": {}
    },
    {
        "label": "get_previewer",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.latent_preview",
        "description": "Fooocus.backend.headless.latent_preview",
        "peekOfCode": "def get_previewer(device, latent_format):\n    previewer = None\n    method = args.preview_method\n    if method != LatentPreviewMethod.NoPreviews:\n        # TODO previewer methods\n        taesd_decoder_path = None\n        if latent_format.taesd_decoder_name is not None:\n            taesd_decoder_path = next(\n                (fn for fn in folder_paths.get_filename_list(\"vae_approx\")\n                    if fn.startswith(latent_format.taesd_decoder_name)),",
        "detail": "Fooocus.backend.headless.latent_preview",
        "documentation": {}
    },
    {
        "label": "prepare_callback",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.latent_preview",
        "description": "Fooocus.backend.headless.latent_preview",
        "peekOfCode": "def prepare_callback(model, steps, x0_output_dict=None):\n    preview_format = \"JPEG\"\n    if preview_format not in [\"JPEG\", \"PNG\"]:\n        preview_format = \"JPEG\"\n    previewer = get_previewer(model.load_device, model.model.latent_format)\n    pbar = fcbh.utils.ProgressBar(steps)\n    def callback(step, x0, x, total_steps):\n        if x0_output_dict is not None:\n            x0_output_dict[\"x0\"] = x0\n        preview_bytes = None",
        "detail": "Fooocus.backend.headless.latent_preview",
        "documentation": {}
    },
    {
        "label": "MAX_PREVIEW_RESOLUTION",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.latent_preview",
        "description": "Fooocus.backend.headless.latent_preview",
        "peekOfCode": "MAX_PREVIEW_RESOLUTION = 512\nclass LatentPreviewer:\n    def decode_latent_to_preview(self, x0):\n        pass\n    def decode_latent_to_preview_image(self, preview_format, x0):\n        preview_image = self.decode_latent_to_preview(x0)\n        return (\"JPEG\", preview_image, MAX_PREVIEW_RESOLUTION)\nclass TAESDPreviewerImpl(LatentPreviewer):\n    def __init__(self, taesd):\n        self.taesd = taesd",
        "detail": "Fooocus.backend.headless.latent_preview",
        "documentation": {}
    },
    {
        "label": "CLIPTextEncode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class CLIPTextEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"text\": (\"STRING\", {\"multiline\": True}), \"clip\": (\"CLIP\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n    CATEGORY = \"conditioning\"\n    def encode(self, clip, text):\n        tokens = clip.tokenize(text)\n        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningCombine",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningCombine:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_1\": (\"CONDITIONING\", ), \"conditioning_2\": (\"CONDITIONING\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"combine\"\n    CATEGORY = \"conditioning\"\n    def combine(self, conditioning_1, conditioning_2):\n        return (conditioning_1 + conditioning_2, )\nclass ConditioningAverage :",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningAverag",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningAverage :\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ), \"conditioning_from\": (\"CONDITIONING\", ),\n                              \"conditioning_to_strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"addWeighted\"\n    CATEGORY = \"conditioning\"\n    def addWeighted(self, conditioning_to, conditioning_from, conditioning_to_strength):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningConcat",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningConcat:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"conditioning_to\": (\"CONDITIONING\",),\n            \"conditioning_from\": (\"CONDITIONING\",),\n            }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"concat\"\n    CATEGORY = \"conditioning\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningSetArea",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningSetArea:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"width\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningSetAreaPercentage",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningSetAreaPercentage:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"width\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"height\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"x\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"y\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             }}",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningSetMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningSetMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                              \"mask\": (\"MASK\", ),\n                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                              \"set_cond_area\": ([\"default\", \"mask bounds\"],),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningZeroOut",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningZeroOut:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"zero_out\"\n    CATEGORY = \"advanced/conditioning\"\n    def zero_out(self, conditioning):\n        c = []\n        for t in conditioning:",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ConditioningSetTimestepRange",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ConditioningSetTimestepRange:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"start\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             \"end\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"set_range\"\n    CATEGORY = \"advanced/conditioning\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "VAEDecode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class VAEDecode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"decode\"\n    CATEGORY = \"latent\"\n    def decode(self, vae, samples):\n        return (vae.decode(samples[\"samples\"]), )\nclass VAEDecodeTiled:",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "VAEDecodeTiled",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class VAEDecodeTiled:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", ),\n                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n                            }}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"decode\"\n    CATEGORY = \"_for_testing\"\n    def decode(self, vae, samples, tile_size):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "VAEEncode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class VAEEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", )}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n    CATEGORY = \"latent\"\n    @staticmethod\n    def vae_encode_crop_pixels(pixels):\n        x = (pixels.shape[1] // 8) * 8",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "VAEEncodeTiled",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class VAEEncodeTiled:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ),\n                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n                            }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n    CATEGORY = \"_for_testing\"\n    def encode(self, vae, pixels, tile_size):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "VAEEncodeForInpaint",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class VAEEncodeForInpaint:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ), \"mask\": (\"MASK\", ), \"grow_mask_by\": (\"INT\", {\"default\": 6, \"min\": 0, \"max\": 64, \"step\": 1}),}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n    CATEGORY = \"latent/inpaint\"\n    def encode(self, vae, pixels, mask, grow_mask_by=6):\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "SaveLatent",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class SaveLatent:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\", ),\n                              \"filename_prefix\": (\"STRING\", {\"default\": \"latents/fcbh_backend\"})},\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n                }\n    RETURN_TYPES = ()",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LoadLatent",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LoadLatent:\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f)) and f.endswith(\".latent\")]\n        return {\"required\": {\"latent\": [sorted(files), ]}, }\n    CATEGORY = \"_for_testing\"\n    RETURN_TYPES = (\"LATENT\", )\n    FUNCTION = \"load\"\n    def load(self, latent):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "CheckpointLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class CheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"config_name\": (folder_paths.get_filename_list(\"configs\"), ),\n                              \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), )}}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n    CATEGORY = \"advanced/loaders\"\n    def load_checkpoint(self, config_name, ckpt_name, output_vae=True, output_clip=True):\n        config_path = folder_paths.get_full_path(\"configs\", config_name)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "CheckpointLoaderSimple",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class CheckpointLoaderSimple:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n    FUNCTION = \"load_checkpoint\"\n    CATEGORY = \"loaders\"\n    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "DiffusersLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class DiffusersLoader:\n    @classmethod\n    def INPUT_TYPES(cls):\n        paths = []\n        for search_path in folder_paths.get_folder_paths(\"diffusers\"):\n            if os.path.exists(search_path):\n                for root, subdir, files in os.walk(search_path, followlinks=True):\n                    if \"model_index.json\" in files:\n                        paths.append(os.path.relpath(root, start=search_path))\n        return {\"required\": {\"model_path\": (paths,), }}",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "unCLIPCheckpointLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class unCLIPCheckpointLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\", \"CLIP_VISION\")\n    FUNCTION = \"load_checkpoint\"\n    CATEGORY = \"loaders\"\n    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "CLIPSetLastLayer",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class CLIPSetLastLayer:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip\": (\"CLIP\", ),\n                              \"stop_at_clip_layer\": (\"INT\", {\"default\": -1, \"min\": -24, \"max\": -1, \"step\": 1}),\n                              }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"set_last_layer\"\n    CATEGORY = \"conditioning\"\n    def set_last_layer(self, clip, stop_at_clip_layer):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LoraLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LoraLoader:\n    def __init__(self):\n        self.loaded_lora = None\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"clip\": (\"CLIP\", ),\n                              \"lora_name\": (folder_paths.get_filename_list(\"loras\"), ),\n                              \"strength_model\": (\"FLOAT\", {\"default\": 1.0, \"min\": -20.0, \"max\": 20.0, \"step\": 0.01}),\n                              \"strength_clip\": (\"FLOAT\", {\"default\": 1.0, \"min\": -20.0, \"max\": 20.0, \"step\": 0.01}),",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "VAELoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class VAELoader:\n    @staticmethod\n    def vae_list():\n        vaes = folder_paths.get_filename_list(\"vae\")\n        approx_vaes = folder_paths.get_filename_list(\"vae_approx\")\n        sdxl_taesd_enc = False\n        sdxl_taesd_dec = False\n        sd1_taesd_enc = False\n        sd1_taesd_dec = False\n        for v in approx_vaes:",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ControlNetLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ControlNetLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"control_net_name\": (folder_paths.get_filename_list(\"controlnet\"), )}}\n    RETURN_TYPES = (\"CONTROL_NET\",)\n    FUNCTION = \"load_controlnet\"\n    CATEGORY = \"loaders\"\n    def load_controlnet(self, control_net_name):\n        controlnet_path = folder_paths.get_full_path(\"controlnet\", control_net_name)\n        controlnet = fcbh.controlnet.load_controlnet(controlnet_path)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "DiffControlNetLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class DiffControlNetLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"control_net_name\": (folder_paths.get_filename_list(\"controlnet\"), )}}\n    RETURN_TYPES = (\"CONTROL_NET\",)\n    FUNCTION = \"load_controlnet\"\n    CATEGORY = \"loaders\"\n    def load_controlnet(self, model, control_net_name):\n        controlnet_path = folder_paths.get_full_path(\"controlnet\", control_net_name)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ControlNetApply",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ControlNetApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"control_net\": (\"CONTROL_NET\", ),\n                             \"image\": (\"IMAGE\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01})\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_controlnet\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ControlNetApplyAdvanced",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ControlNetApplyAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"control_net\": (\"CONTROL_NET\", ),\n                             \"image\": (\"IMAGE\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"start_percent\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n                             \"end_percent\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "UNETLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class UNETLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"unet_name\": (folder_paths.get_filename_list(\"unet\"), ),\n                             }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_unet\"\n    CATEGORY = \"advanced/loaders\"\n    def load_unet(self, unet_name):\n        unet_path = folder_paths.get_full_path(\"unet\", unet_name)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "CLIPLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class CLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name\": (folder_paths.get_filename_list(\"clip\"), ),\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n    CATEGORY = \"advanced/loaders\"\n    def load_clip(self, clip_name):\n        clip_path = folder_paths.get_full_path(\"clip\", clip_name)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "DualCLIPLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class DualCLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name1\": (folder_paths.get_filename_list(\"clip\"), ), \"clip_name2\": (folder_paths.get_filename_list(\"clip\"), ),\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n    CATEGORY = \"advanced/loaders\"\n    def load_clip(self, clip_name1, clip_name2):\n        clip_path1 = folder_paths.get_full_path(\"clip\", clip_name1)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "CLIPVisionLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class CLIPVisionLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name\": (folder_paths.get_filename_list(\"clip_vision\"), ),\n                             }}\n    RETURN_TYPES = (\"CLIP_VISION\",)\n    FUNCTION = \"load_clip\"\n    CATEGORY = \"loaders\"\n    def load_clip(self, clip_name):\n        clip_path = folder_paths.get_full_path(\"clip_vision\", clip_name)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "CLIPVisionEncode",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class CLIPVisionEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n                              \"image\": (\"IMAGE\",)\n                             }}\n    RETURN_TYPES = (\"CLIP_VISION_OUTPUT\",)\n    FUNCTION = \"encode\"\n    CATEGORY = \"conditioning\"\n    def encode(self, clip_vision, image):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "StyleModelLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class StyleModelLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"style_model_name\": (folder_paths.get_filename_list(\"style_models\"), )}}\n    RETURN_TYPES = (\"STYLE_MODEL\",)\n    FUNCTION = \"load_style_model\"\n    CATEGORY = \"loaders\"\n    def load_style_model(self, style_model_name):\n        style_model_path = folder_paths.get_full_path(\"style_models\", style_model_name)\n        style_model = fcbh.sd.load_style_model(style_model_path)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "StyleModelApply",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class StyleModelApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"style_model\": (\"STYLE_MODEL\", ),\n                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_stylemodel\"\n    CATEGORY = \"conditioning/style_model\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "unCLIPConditioning",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class unCLIPConditioning:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n                             \"noise_augmentation\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                             }}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"apply_adm\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "GLIGENLoader",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class GLIGENLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"gligen_name\": (folder_paths.get_filename_list(\"gligen\"), )}}\n    RETURN_TYPES = (\"GLIGEN\",)\n    FUNCTION = \"load_gligen\"\n    CATEGORY = \"loaders\"\n    def load_gligen(self, gligen_name):\n        gligen_path = folder_paths.get_full_path(\"gligen\", gligen_name)\n        gligen = fcbh.sd.load_gligen(gligen_path)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "GLIGENTextBoxApply",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class GLIGENTextBoxApply:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ),\n                              \"clip\": (\"CLIP\", ),\n                              \"gligen_textbox_model\": (\"GLIGEN\", ),\n                              \"text\": (\"STRING\", {\"multiline\": True}),\n                              \"width\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "EmptyLatentImage",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class EmptyLatentImage:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096})}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"generate\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentFromBatch",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentFromBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"batch_index\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 63}),\n                              \"length\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"frombatch\"\n    CATEGORY = \"latent/batch\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "RepeatLatentBatch",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class RepeatLatentBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"amount\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"repeat\"\n    CATEGORY = \"latent/batch\"\n    def repeat(self, samples, amount):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentUpscale",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentUpscale:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n    crop_methods = [\"disabled\", \"center\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"crop\": (s.crop_methods,)}}\n    RETURN_TYPES = (\"LATENT\",)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentUpscaleBy",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentUpscaleBy:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n                              \"scale_by\": (\"FLOAT\", {\"default\": 1.5, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"upscale\"\n    CATEGORY = \"latent\"\n    def upscale(self, samples, upscale_method, scale_by):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentRotate",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentRotate:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"rotation\": ([\"none\", \"90 degrees\", \"180 degrees\", \"270 degrees\"],),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"rotate\"\n    CATEGORY = \"latent/transform\"\n    def rotate(self, samples, rotation):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentFlip",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentFlip:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"flip_method\": ([\"x-axis: vertically\", \"y-axis: horizontally\"],),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"flip\"\n    CATEGORY = \"latent/transform\"\n    def flip(self, samples, flip_method):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentComposite",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentComposite:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples_to\": (\"LATENT\",),\n                              \"samples_from\": (\"LATENT\",),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"feather\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentBlend",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentBlend:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"samples1\": (\"LATENT\",),\n            \"samples2\": (\"LATENT\",),\n            \"blend_factor\": (\"FLOAT\", {\n                \"default\": 0.5,\n                \"min\": 0,\n                \"max\": 1,",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LatentCrop",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LatentCrop:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "SetLatentNoiseMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class SetLatentNoiseMask:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"mask\": (\"MASK\",),\n                              }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"set_mask\"\n    CATEGORY = \"latent/inpaint\"\n    def set_mask(self, samples, mask):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "KSampler",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class KSampler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"sampler_name\": (fcbh.samplers.KSampler.SAMPLERS, ),\n                    \"scheduler\": (fcbh.samplers.KSampler.SCHEDULERS, ),",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "KSamplerAdvanced",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class KSamplerAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"add_noise\": ([\"enable\", \"disable\"], ),\n                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n                    \"sampler_name\": (fcbh.samplers.KSampler.SAMPLERS, ),",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "SaveImage",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class SaveImage:\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": \n                    {\"images\": (\"IMAGE\", ),\n                     \"filename_prefix\": (\"STRING\", {\"default\": \"fcbh_backend\"})},",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "PreviewImage",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class PreviewImage(SaveImage):\n    def __init__(self):\n        self.output_dir = folder_paths.get_temp_directory()\n        self.type = \"temp\"\n        self.prefix_append = \"_temp_\" + ''.join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for x in range(5))\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"images\": (\"IMAGE\", ), },\n                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LoadImage",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LoadImage:\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n        return {\"required\":\n                    {\"image\": (sorted(files), {\"image_upload\": True})},\n                }\n    CATEGORY = \"image\"\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "LoadImageMask",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class LoadImageMask:\n    _color_channels = [\"alpha\", \"red\", \"green\", \"blue\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n        return {\"required\":\n                    {\"image\": (sorted(files), {\"image_upload\": True}),\n                     \"channel\": (s._color_channels, ), }\n                }",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ImageScale",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ImageScale:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n    crop_methods = [\"disabled\", \"center\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"crop\": (s.crop_methods,)}}\n    RETURN_TYPES = (\"IMAGE\",)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ImageScaleBy",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ImageScaleBy:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n                              \"scale_by\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"upscale\"\n    CATEGORY = \"image/upscaling\"\n    def upscale(self, image, upscale_method, scale_by):",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ImageInvert",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ImageInvert:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image\": (\"IMAGE\",)}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"invert\"\n    CATEGORY = \"image\"\n    def invert(self, image):\n        s = 1.0 - image\n        return (s,)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ImageBatch",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ImageBatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"image1\": (\"IMAGE\",), \"image2\": (\"IMAGE\",)}}\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"batch\"\n    CATEGORY = \"image\"\n    def batch(self, image1, image2):\n        if image1.shape[1:] != image2.shape[1:]:\n            image2 = fcbh.utils.common_upscale(image2.movedim(-1,1), image1.shape[2], image1.shape[1], \"bilinear\", \"center\").movedim(1,-1)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "EmptyImage",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class EmptyImage:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                              \"color\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xFFFFFF, \"step\": 1, \"display\": \"color\"}),\n                              }}",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "ImagePadForOutpaint",
        "kind": 6,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "class ImagePadForOutpaint:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"left\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"top\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"right\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n                \"bottom\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "before_node_execution",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "def before_node_execution():\n    fcbh.model_management.throw_exception_if_processing_interrupted()\ndef interrupt_processing(value=True):\n    fcbh.model_management.interrupt_current_processing(value)\nMAX_RESOLUTION=8192\nclass CLIPTextEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"text\": (\"STRING\", {\"multiline\": True}), \"clip\": (\"CLIP\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "interrupt_processing",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "def interrupt_processing(value=True):\n    fcbh.model_management.interrupt_current_processing(value)\nMAX_RESOLUTION=8192\nclass CLIPTextEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"text\": (\"STRING\", {\"multiline\": True}), \"clip\": (\"CLIP\", )}}\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"encode\"\n    CATEGORY = \"conditioning\"",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "common_ksampler",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "def common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    latent_image = latent[\"samples\"]\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = fcbh.sample.prepare_noise(latent_image, seed, batch_inds)\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "load_custom_node",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "def load_custom_node(module_path, ignore=set()):\n    module_name = os.path.basename(module_path)\n    if os.path.isfile(module_path):\n        sp = os.path.splitext(module_path)\n        module_name = sp[0]\n    try:\n        if os.path.isfile(module_path):\n            module_spec = importlib.util.spec_from_file_location(module_name, module_path)\n            module_dir = os.path.split(module_path)[0]\n        else:",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "load_custom_nodes",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "def load_custom_nodes():\n    base_node_names = set(NODE_CLASS_MAPPINGS.keys())\n    node_paths = folder_paths.get_folder_paths(\"custom_nodes\")\n    node_import_times = []\n    for custom_node_path in node_paths:\n        possible_modules = os.listdir(custom_node_path)\n        if \"__pycache__\" in possible_modules:\n            possible_modules.remove(\"__pycache__\")\n        for possible_module in possible_modules:\n            module_path = os.path.join(custom_node_path, possible_module)",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "init_custom_nodes",
        "kind": 2,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "def init_custom_nodes():\n    extras_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"fcbh_extras\")\n    extras_files = [\n        \"nodes_latent.py\",\n        \"nodes_hypernetwork.py\",\n        \"nodes_upscale_model.py\",\n        \"nodes_post_processing.py\",\n        \"nodes_mask.py\",\n        \"nodes_compositing.py\",\n        \"nodes_rebatch.py\",",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "NODE_CLASS_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "NODE_CLASS_MAPPINGS = {\n    \"KSampler\": KSampler,\n    \"CheckpointLoaderSimple\": CheckpointLoaderSimple,\n    \"CLIPTextEncode\": CLIPTextEncode,\n    \"CLIPSetLastLayer\": CLIPSetLastLayer,\n    \"VAEDecode\": VAEDecode,\n    \"VAEEncode\": VAEEncode,\n    \"VAEEncodeForInpaint\": VAEEncodeForInpaint,\n    \"VAELoader\": VAELoader,\n    \"EmptyLatentImage\": EmptyLatentImage,",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "NODE_DISPLAY_NAME_MAPPINGS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "NODE_DISPLAY_NAME_MAPPINGS = {\n    # Sampling\n    \"KSampler\": \"KSampler\",\n    \"KSamplerAdvanced\": \"KSampler (Advanced)\",\n    # Loaders\n    \"CheckpointLoader\": \"Load Checkpoint With Config (DEPRECATED)\",\n    \"CheckpointLoaderSimple\": \"Load Checkpoint\",\n    \"VAELoader\": \"Load VAE\",\n    \"LoraLoader\": \"Load LoRA\",\n    \"CLIPLoader\": \"Load CLIP\",",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "EXTENSION_WEB_DIRS",
        "kind": 5,
        "importPath": "Fooocus.backend.headless.nodes",
        "description": "Fooocus.backend.headless.nodes",
        "peekOfCode": "EXTENSION_WEB_DIRS = {}\ndef load_custom_node(module_path, ignore=set()):\n    module_name = os.path.basename(module_path)\n    if os.path.isfile(module_path):\n        sp = os.path.splitext(module_path)\n        module_name = sp[0]\n    try:\n        if os.path.isfile(module_path):\n            module_spec = importlib.util.spec_from_file_location(module_name, module_path)\n            module_dir = os.path.split(module_path)[0]",
        "detail": "Fooocus.backend.headless.nodes",
        "documentation": {}
    },
    {
        "label": "FaceWarpException",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "description": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "peekOfCode": "class FaceWarpException(Exception):\n    def __str__(self):\n        return 'In File {}:{}'.format(__file__, super.__str__(self))\ndef get_reference_facial_points(output_size=None, inner_padding_factor=0.0, outer_padding=(0, 0), default_square=False):\n    \"\"\"\n    Function:\n    ----------\n        get reference 5 key points according to crop settings:\n        0. Set default crop_size:\n            if default_square:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "get_reference_facial_points",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "description": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "peekOfCode": "def get_reference_facial_points(output_size=None, inner_padding_factor=0.0, outer_padding=(0, 0), default_square=False):\n    \"\"\"\n    Function:\n    ----------\n        get reference 5 key points according to crop settings:\n        0. Set default crop_size:\n            if default_square:\n                crop_size = (112, 112)\n            else:\n                crop_size = (96, 112)",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "get_affine_transform_matrix",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "description": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "peekOfCode": "def get_affine_transform_matrix(src_pts, dst_pts):\n    \"\"\"\n    Function:\n    ----------\n        get affine transform matrix 'tfm' from src_pts to dst_pts\n    Parameters:\n    ----------\n        @src_pts: Kx2 np.array\n            source points matrix, each row is a pair of coordinates (x, y)\n        @dst_pts: Kx2 np.array",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "warp_and_crop_face",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "description": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "peekOfCode": "def warp_and_crop_face(src_img, facial_pts, reference_pts=None, crop_size=(96, 112), align_type='smilarity'):\n    \"\"\"\n    Function:\n    ----------\n        apply affine transform 'trans' to uv\n    Parameters:\n    ----------\n        @src_img: 3x3 np.array\n            input image\n        @facial_pts: could be",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "REFERENCE_FACIAL_POINTS",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "description": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "peekOfCode": "REFERENCE_FACIAL_POINTS = [[30.29459953, 51.69630051], [65.53179932, 51.50139999], [48.02519989, 71.73660278],\n                           [33.54930115, 92.3655014], [62.72990036, 92.20410156]]\nDEFAULT_CROP_SIZE = (96, 112)\nclass FaceWarpException(Exception):\n    def __str__(self):\n        return 'In File {}:{}'.format(__file__, super.__str__(self))\ndef get_reference_facial_points(output_size=None, inner_padding_factor=0.0, outer_padding=(0, 0), default_square=False):\n    \"\"\"\n    Function:\n    ----------",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CROP_SIZE",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "description": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "peekOfCode": "DEFAULT_CROP_SIZE = (96, 112)\nclass FaceWarpException(Exception):\n    def __str__(self):\n        return 'In File {}:{}'.format(__file__, super.__str__(self))\ndef get_reference_facial_points(output_size=None, inner_padding_factor=0.0, outer_padding=(0, 0), default_square=False):\n    \"\"\"\n    Function:\n    ----------\n        get reference 5 key points according to crop settings:\n        0. Set default crop_size:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.align_trans",
        "documentation": {}
    },
    {
        "label": "MatlabCp2tormException",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "class MatlabCp2tormException(Exception):\n    def __str__(self):\n        return 'In File {}:{}'.format(__file__, super.__str__(self))\ndef tformfwd(trans, uv):\n    \"\"\"\n    Function:\n    ----------\n        apply affine transform 'trans' to uv\n    Parameters:\n    ----------",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "tformfwd",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "def tformfwd(trans, uv):\n    \"\"\"\n    Function:\n    ----------\n        apply affine transform 'trans' to uv\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "tforminv",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "def tforminv(trans, uv):\n    \"\"\"\n    Function:\n    ----------\n        apply the inverse of affine transform 'trans' to uv\n    Parameters:\n    ----------\n        @trans: 3x3 np.array\n            transform matrix\n        @uv: Kx2 np.array",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "findNonreflectiveSimilarity",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "def findNonreflectiveSimilarity(uv, xy, options=None):\n    options = {'K': 2}\n    K = options['K']\n    M = xy.shape[0]\n    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\n    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\n    X = np.vstack((tmp1, tmp2))\n    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "findSimilarity",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "def findSimilarity(uv, xy, options=None):\n    options = {'K': 2}\n    #    uv = np.array(uv)\n    #    xy = np.array(xy)\n    # Solve for trans1\n    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\n    # Solve for trans2\n    # manually reflect the xy data across the Y-axis\n    xyR = xy\n    xyR[:, 0] = -1 * xyR[:, 0]",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "get_similarity_transform",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "def get_similarity_transform(src_pts, dst_pts, reflective=True):\n    \"\"\"\n    Function:\n    ----------\n        Find Similarity Transform Matrix 'trans':\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]\n            [x, y, 1] = [u, v, 1] * trans",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "cvt_tform_mat_for_cv2",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "def cvt_tform_mat_for_cv2(trans):\n    \"\"\"\n    Function:\n    ----------\n        Convert Transform Matrix 'trans' into 'cv2_trans' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "get_similarity_transform_for_cv2",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "description": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "peekOfCode": "def get_similarity_transform_for_cv2(src_pts, dst_pts, reflective=True):\n    \"\"\"\n    Function:\n    ----------\n        Find Similarity Transform Matrix 'cv2_trans' which could be\n        directly used by cv2.warpAffine():\n            u = src_pts[:, 0]\n            v = src_pts[:, 1]\n            x = dst_pts[:, 0]\n            y = dst_pts[:, 1]",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.matlab_cp2tform",
        "documentation": {}
    },
    {
        "label": "RetinaFace",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface",
        "peekOfCode": "class RetinaFace(nn.Module):\n    def __init__(self, network_name='resnet50', half=False, phase='test', device=None):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n        super(RetinaFace, self).__init__()\n        self.half_inference = half\n        cfg = generate_config(network_name)\n        self.backbone = cfg['name']\n        self.model_name = f'retinaface_{network_name}'\n        self.cfg = cfg\n        self.phase = phase",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface",
        "documentation": {}
    },
    {
        "label": "generate_config",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface",
        "peekOfCode": "def generate_config(network_name):\n    cfg_mnet = {\n        'name': 'mobilenet0.25',\n        'min_sizes': [[16, 32], [64, 128], [256, 512]],\n        'steps': [8, 16, 32],\n        'variance': [0.1, 0.2],\n        'clip': False,\n        'loc_weight': 2.0,\n        'gpu_train': True,\n        'batch_size': 32,",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface",
        "documentation": {}
    },
    {
        "label": "SSH",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "class SSH(nn.Module):\n    def __init__(self, in_channel, out_channel):\n        super(SSH, self).__init__()\n        assert out_channel % 4 == 0\n        leaky = 0\n        if (out_channel <= 64):\n            leaky = 0.1\n        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel // 2, stride=1)\n        self.conv5X5_1 = conv_bn(in_channel, out_channel // 4, stride=1, leaky=leaky)\n        self.conv5X5_2 = conv_bn_no_relu(out_channel // 4, out_channel // 4, stride=1)",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "FPN",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "class FPN(nn.Module):\n    def __init__(self, in_channels_list, out_channels):\n        super(FPN, self).__init__()\n        leaky = 0\n        if (out_channels <= 64):\n            leaky = 0.1\n        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride=1, leaky=leaky)\n        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride=1, leaky=leaky)\n        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride=1, leaky=leaky)\n        self.merge1 = conv_bn(out_channels, out_channels, leaky=leaky)",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "MobileNetV1",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "class MobileNetV1(nn.Module):\n    def __init__(self):\n        super(MobileNetV1, self).__init__()\n        self.stage1 = nn.Sequential(\n            conv_bn(3, 8, 2, leaky=0.1),  # 3\n            conv_dw(8, 16, 1),  # 7\n            conv_dw(16, 32, 2),  # 11\n            conv_dw(32, 32, 1),  # 19\n            conv_dw(32, 64, 2),  # 27\n            conv_dw(64, 64, 1),  # 43",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "ClassHead",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "class ClassHead(nn.Module):\n    def __init__(self, inchannels=512, num_anchors=3):\n        super(ClassHead, self).__init__()\n        self.num_anchors = num_anchors\n        self.conv1x1 = nn.Conv2d(inchannels, self.num_anchors * 2, kernel_size=(1, 1), stride=1, padding=0)\n    def forward(self, x):\n        out = self.conv1x1(x)\n        out = out.permute(0, 2, 3, 1).contiguous()\n        return out.view(out.shape[0], -1, 2)\nclass BboxHead(nn.Module):",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "BboxHead",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "class BboxHead(nn.Module):\n    def __init__(self, inchannels=512, num_anchors=3):\n        super(BboxHead, self).__init__()\n        self.conv1x1 = nn.Conv2d(inchannels, num_anchors * 4, kernel_size=(1, 1), stride=1, padding=0)\n    def forward(self, x):\n        out = self.conv1x1(x)\n        out = out.permute(0, 2, 3, 1).contiguous()\n        return out.view(out.shape[0], -1, 4)\nclass LandmarkHead(nn.Module):\n    def __init__(self, inchannels=512, num_anchors=3):",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "LandmarkHead",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "class LandmarkHead(nn.Module):\n    def __init__(self, inchannels=512, num_anchors=3):\n        super(LandmarkHead, self).__init__()\n        self.conv1x1 = nn.Conv2d(inchannels, num_anchors * 10, kernel_size=(1, 1), stride=1, padding=0)\n    def forward(self, x):\n        out = self.conv1x1(x)\n        out = out.permute(0, 2, 3, 1).contiguous()\n        return out.view(out.shape[0], -1, 10)\ndef make_class_head(fpn_num=3, inchannels=64, anchor_num=2):\n    classhead = nn.ModuleList()",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "conv_bn",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "def conv_bn(inp, oup, stride=1, leaky=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True))\ndef conv_bn_no_relu(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n    )\ndef conv_bn1X1(inp, oup, stride, leaky=0):",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "conv_bn_no_relu",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "def conv_bn_no_relu(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n    )\ndef conv_bn1X1(inp, oup, stride, leaky=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False), nn.BatchNorm2d(oup),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True))\ndef conv_dw(inp, oup, stride, leaky=0.1):",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "conv_bn1X1",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "def conv_bn1X1(inp, oup, stride, leaky=0):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False), nn.BatchNorm2d(oup),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True))\ndef conv_dw(inp, oup, stride, leaky=0.1):\n    return nn.Sequential(\n        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n        nn.BatchNorm2d(inp),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True),\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "conv_dw",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "def conv_dw(inp, oup, stride, leaky=0.1):\n    return nn.Sequential(\n        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n        nn.BatchNorm2d(inp),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True),\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.LeakyReLU(negative_slope=leaky, inplace=True),\n    )\nclass SSH(nn.Module):",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "make_class_head",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "def make_class_head(fpn_num=3, inchannels=64, anchor_num=2):\n    classhead = nn.ModuleList()\n    for i in range(fpn_num):\n        classhead.append(ClassHead(inchannels, anchor_num))\n    return classhead\ndef make_bbox_head(fpn_num=3, inchannels=64, anchor_num=2):\n    bboxhead = nn.ModuleList()\n    for i in range(fpn_num):\n        bboxhead.append(BboxHead(inchannels, anchor_num))\n    return bboxhead",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "make_bbox_head",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "def make_bbox_head(fpn_num=3, inchannels=64, anchor_num=2):\n    bboxhead = nn.ModuleList()\n    for i in range(fpn_num):\n        bboxhead.append(BboxHead(inchannels, anchor_num))\n    return bboxhead\ndef make_landmark_head(fpn_num=3, inchannels=64, anchor_num=2):\n    landmarkhead = nn.ModuleList()\n    for i in range(fpn_num):\n        landmarkhead.append(LandmarkHead(inchannels, anchor_num))\n    return landmarkhead",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "make_landmark_head",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "peekOfCode": "def make_landmark_head(fpn_num=3, inchannels=64, anchor_num=2):\n    landmarkhead = nn.ModuleList()\n    for i in range(fpn_num):\n        landmarkhead.append(LandmarkHead(inchannels, anchor_num))\n    return landmarkhead",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_net",
        "documentation": {}
    },
    {
        "label": "PriorBox",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "class PriorBox(object):\n    def __init__(self, cfg, image_size=None, phase='train'):\n        super(PriorBox, self).__init__()\n        self.min_sizes = cfg['min_sizes']\n        self.steps = cfg['steps']\n        self.clip = cfg['clip']\n        self.image_size = image_size\n        self.feature_maps = [[ceil(self.image_size[0] / step), ceil(self.image_size[1] / step)] for step in self.steps]\n        self.name = 's'\n    def forward(self):",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "py_cpu_nms",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def py_cpu_nms(dets, thresh):\n    \"\"\"Pure Python NMS baseline.\"\"\"\n    keep = torchvision.ops.nms(\n        boxes=torch.Tensor(dets[:, :4]),\n        scores=torch.Tensor(dets[:, 4]),\n        iou_threshold=thresh,\n    )\n    return list(keep)\ndef point_form(boxes):\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "point_form",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def point_form(boxes):\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat(\n        (",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "center_size",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def center_size(boxes):\n    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat(\n        (boxes[:, 2:] + boxes[:, :2]) / 2,  # cx, cy",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "intersect",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def intersect(box_a, box_b):\n    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "jaccard",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A  B / A  B = A  B / (area(A) + area(B) - A  B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "matrix_iou",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def matrix_iou(a, b):\n    \"\"\"\n    return iou of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "matrix_iof",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def matrix_iof(a, b):\n    \"\"\"\n    return iof of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    return area_i / np.maximum(area_a[:, np.newaxis], 1)\ndef match(threshold, truths, priors, variances, labels, landms, loc_t, conf_t, landm_t, idx):",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "match",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def match(threshold, truths, priors, variances, labels, landms, loc_t, conf_t, landm_t, idx):\n    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when matching boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, 4].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "encode",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def encode(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "encode_landm",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def encode_landm(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 10].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "decode",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def decode(loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "decode_landm",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def decode_landm(pre, priors, variances):\n    \"\"\"Decode landm from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): landm predictions for loc layers,\n            Shape: [num_priors,10]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "batched_decode",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def batched_decode(b_loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        b_loc (tensor): location predictions for loc layers,\n            Shape: [num_batches,num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [1,num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "batched_decode_landm",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def batched_decode_landm(pre, priors, variances):\n    \"\"\"Decode landm from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): landm predictions for loc layers,\n            Shape: [num_batches,num_priors,10]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [1,num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "log_sum_exp",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def log_sum_exp(x):\n    \"\"\"Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    \"\"\"\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x - x_max), 1, keepdim=True)) + x_max\n# Original author: Francisco Massa:",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "nms",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "description": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "peekOfCode": "def nms(boxes, scores, overlap=0.5, top_k=200):\n    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.",
        "detail": "Fooocus.fooocus_extras.facexlib.detection.retinaface_utils",
        "documentation": {}
    },
    {
        "label": "ConvBNReLU",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "peekOfCode": "class ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size=ks, stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_chan)\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(self.bn(x))\n        return x\nclass BiSeNetOutput(nn.Module):",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "documentation": {}
    },
    {
        "label": "BiSeNetOutput",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "peekOfCode": "class BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, num_class):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, num_class, kernel_size=1, bias=False)\n    def forward(self, x):\n        feat = self.conv(x)\n        out = self.conv_out(feat)\n        return out, feat\nclass AttentionRefinementModule(nn.Module):",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "documentation": {}
    },
    {
        "label": "AttentionRefinementModule",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "peekOfCode": "class AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size=1, bias=False)\n        self.bn_atten = nn.BatchNorm2d(out_chan)\n        self.sigmoid_atten = nn.Sigmoid()\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "documentation": {}
    },
    {
        "label": "ContextPath",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "peekOfCode": "class ContextPath(nn.Module):\n    def __init__(self):\n        super(ContextPath, self).__init__()\n        self.resnet = ResNet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n    def forward(self, x):",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "documentation": {}
    },
    {
        "label": "FeatureFusionModule",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "peekOfCode": "class FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan, out_chan // 4, kernel_size=1, stride=1, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(out_chan // 4, out_chan, kernel_size=1, stride=1, padding=0, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "documentation": {}
    },
    {
        "label": "BiSeNet",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "peekOfCode": "class BiSeNet(nn.Module):\n    def __init__(self, num_class):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, num_class)\n        self.conv_out16 = BiSeNetOutput(128, 64, num_class)\n        self.conv_out32 = BiSeNetOutput(128, 64, num_class)\n    def forward(self, x, return_feat=False):\n        h, w = x.size()[2:]",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.bisenet",
        "documentation": {}
    },
    {
        "label": "NormLayer",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "peekOfCode": "class NormLayer(nn.Module):\n    \"\"\"Normalization Layers.\n    Args:\n        channels: input channels, for batch norm and instance norm.\n        input_size: input shape without batch size, for layer norm.\n    \"\"\"\n    def __init__(self, channels, normalize_shape=None, norm_type='bn'):\n        super(NormLayer, self).__init__()\n        norm_type = norm_type.lower()\n        self.norm_type = norm_type",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "documentation": {}
    },
    {
        "label": "ReluLayer",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "peekOfCode": "class ReluLayer(nn.Module):\n    \"\"\"Relu Layer.\n    Args:\n        relu type: type of relu layer, candidates are\n            - ReLU\n            - LeakyReLU: default relu slope 0.2\n            - PRelu\n            - SELU\n            - none: direct pass\n    \"\"\"",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "documentation": {}
    },
    {
        "label": "ConvLayer",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "peekOfCode": "class ConvLayer(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 scale='none',\n                 norm_type='none',\n                 relu_type='none',\n                 use_pad=True,\n                 bias=True):",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    \"\"\"\n    Residual block recommended in: http://torch.ch/blog/2016/02/04/resnets.html\n    \"\"\"\n    def __init__(self, c_in, c_out, relu_type='prelu', norm_type='bn', scale='none'):\n        super(ResidualBlock, self).__init__()\n        if scale == 'none' and c_in == c_out:\n            self.shortcut_func = lambda x: x\n        else:\n            self.shortcut_func = ConvLayer(c_in, c_out, 3, scale)",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "documentation": {}
    },
    {
        "label": "ParseNet",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "peekOfCode": "class ParseNet(nn.Module):\n    def __init__(self,\n                 in_size=128,\n                 out_size=128,\n                 min_feat_size=32,\n                 base_ch=64,\n                 parsing_ch=19,\n                 res_depth=10,\n                 relu_type='LeakyReLU',\n                 norm_type='bn',",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.parsenet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "peekOfCode": "class BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = nn.BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "documentation": {}
    },
    {
        "label": "ResNet18",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "peekOfCode": "class ResNet18(nn.Module):\n    def __init__(self):\n        super(ResNet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\nclass BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = nn.BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = nn.BatchNorm2d(out_chan)",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "documentation": {}
    },
    {
        "label": "create_layer_basic",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "description": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "peekOfCode": "def create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum - 1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\nclass ResNet18(nn.Module):\n    def __init__(self):\n        super(ResNet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)",
        "detail": "Fooocus.fooocus_extras.facexlib.parsing.resnet",
        "documentation": {}
    },
    {
        "label": "FaceRestoreHelper",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "description": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "peekOfCode": "class FaceRestoreHelper(object):\n    \"\"\"Helper for the face restoration pipeline (base class).\"\"\"\n    def __init__(self,\n                 upscale_factor,\n                 face_size=512,\n                 crop_ratio=(1, 1),\n                 det_model='retinaface_resnet50',\n                 save_ext='png',\n                 template_3points=False,\n                 pad_blur=False,",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "documentation": {}
    },
    {
        "label": "get_largest_face",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "description": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "peekOfCode": "def get_largest_face(det_faces, h, w):\n    def get_location(val, length):\n        if val < 0:\n            return 0\n        elif val > length:\n            return length\n        else:\n            return val\n    face_areas = []\n    for det_face in det_faces:",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "documentation": {}
    },
    {
        "label": "get_center_face",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "description": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "peekOfCode": "def get_center_face(det_faces, h=0, w=0, center=None):\n    if center is not None:\n        center = np.array(center)\n    else:\n        center = np.array([w / 2, h / 2])\n    center_dist = []\n    for det_face in det_faces:\n        face_center = np.array([(det_face[0] + det_face[2]) / 2, (det_face[1] + det_face[3]) / 2])\n        dist = np.linalg.norm(face_center - center)\n        center_dist.append(dist)",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.face_restoration_helper",
        "documentation": {}
    },
    {
        "label": "compute_increased_bbox",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "description": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "peekOfCode": "def compute_increased_bbox(bbox, increase_area, preserve_aspect=True):\n    left, top, right, bot = bbox\n    width = right - left\n    height = bot - top\n    if preserve_aspect:\n        width_increase = max(increase_area, ((1 + 2 * increase_area) * height - width) / (2 * width))\n        height_increase = max(increase_area, ((1 + 2 * increase_area) * width - height) / (2 * height))\n    else:\n        width_increase = height_increase = increase_area\n    left = int(left - width_increase * width)",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "documentation": {}
    },
    {
        "label": "get_valid_bboxes",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "description": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "peekOfCode": "def get_valid_bboxes(bboxes, h, w):\n    left = max(bboxes[0], 0)\n    top = max(bboxes[1], 0)\n    right = min(bboxes[2], w)\n    bottom = min(bboxes[3], h)\n    return (left, top, right, bottom)\ndef align_crop_face_landmarks(img,\n                              landmarks,\n                              output_size,\n                              transform_size=None,",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "documentation": {}
    },
    {
        "label": "align_crop_face_landmarks",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "description": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "peekOfCode": "def align_crop_face_landmarks(img,\n                              landmarks,\n                              output_size,\n                              transform_size=None,\n                              enable_padding=True,\n                              return_inverse_affine=False,\n                              shrink_ratio=(1, 1)):\n    \"\"\"Align and crop face with landmarks.\n    The output_size and transform_size are based on width. The height is\n    adjusted based on shrink_ratio_h/shring_ration_w.",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "documentation": {}
    },
    {
        "label": "paste_face_back",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "description": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "peekOfCode": "def paste_face_back(img, face, inverse_affine):\n    h, w = img.shape[0:2]\n    face_h, face_w = face.shape[0:2]\n    inv_restored = cv2.warpAffine(face, inverse_affine, (w, h))\n    mask = np.ones((face_h, face_w, 3), dtype=np.float32)\n    inv_mask = cv2.warpAffine(mask, inverse_affine, (w, h))\n    # remove the black borders\n    inv_mask_erosion = cv2.erode(inv_mask, np.ones((2, 2), np.uint8))\n    inv_restored_remove_border = inv_mask_erosion * inv_restored\n    total_face_area = np.sum(inv_mask_erosion) // 3",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.face_utils",
        "documentation": {}
    },
    {
        "label": "imwrite",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "description": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "peekOfCode": "def imwrite(img, file_path, params=None, auto_mkdir=True):\n    \"\"\"Write image to file.\n    Args:\n        img (ndarray): Image array to be written.\n        file_path (str): Image file path.\n        params (None or list): Same as opencv's :func:`imwrite` interface.\n        auto_mkdir (bool): If the parent folder of `file_path` does not exist,\n            whether to create it automatically.\n    Returns:\n        bool: Successful or not.",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "documentation": {}
    },
    {
        "label": "img2tensor",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "description": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "peekOfCode": "def img2tensor(imgs, bgr2rgb=True, float32=True):\n    \"\"\"Numpy array to tensor.\n    Args:\n        imgs (list[ndarray] | ndarray): Input images.\n        bgr2rgb (bool): Whether to change bgr to rgb.\n        float32 (bool): Whether to change to float32.\n    Returns:\n        list[tensor] | tensor: Tensor images. If returned results only have\n            one element, just return tensor.\n    \"\"\"",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "documentation": {}
    },
    {
        "label": "load_file_from_url",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "description": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "peekOfCode": "def load_file_from_url(url, model_dir=None, progress=True, file_name=None, save_dir=None):\n    \"\"\"Ref:https://github.com/1adrianb/face-alignment/blob/master/face_alignment/utils.py\n    \"\"\"\n    if model_dir is None:\n        hub_dir = get_dir()\n        model_dir = os.path.join(hub_dir, 'checkpoints')\n    if save_dir is None:\n        save_dir = os.path.join(ROOT_DIR, model_dir)\n    os.makedirs(save_dir, exist_ok=True)\n    parts = urlparse(url)",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "documentation": {}
    },
    {
        "label": "scandir",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "description": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "peekOfCode": "def scandir(dir_path, suffix=None, recursive=False, full_path=False):\n    \"\"\"Scan a directory to find the interested files.\n    Args:\n        dir_path (str): Path of the directory.\n        suffix (str | tuple(str), optional): File suffix that we are\n            interested in. Default: None.\n        recursive (bool, optional): If set to True, recursively scan the\n            directory. Default: False.\n        full_path (bool, optional): If set to True, include the dir_path.\n            Default: False.",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "documentation": {}
    },
    {
        "label": "ROOT_DIR",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "description": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "peekOfCode": "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\ndef imwrite(img, file_path, params=None, auto_mkdir=True):\n    \"\"\"Write image to file.\n    Args:\n        img (ndarray): Image array to be written.\n        file_path (str): Image file path.\n        params (None or list): Same as opencv's :func:`imwrite` interface.\n        auto_mkdir (bool): If the parent folder of `file_path` does not exist,\n            whether to create it automatically.\n    Returns:",
        "detail": "Fooocus.fooocus_extras.facexlib.utils.misc",
        "documentation": {}
    },
    {
        "label": "align_warp_face",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.face_crop",
        "description": "Fooocus.fooocus_extras.face_crop",
        "peekOfCode": "def align_warp_face(self, landmark, border_mode='constant'):\n    affine_matrix = cv2.estimateAffinePartial2D(landmark, self.face_template, method=cv2.LMEDS)[0]\n    self.affine_matrices.append(affine_matrix)\n    if border_mode == 'constant':\n        border_mode = cv2.BORDER_CONSTANT\n    elif border_mode == 'reflect101':\n        border_mode = cv2.BORDER_REFLECT101\n    elif border_mode == 'reflect':\n        border_mode = cv2.BORDER_REFLECT\n    input_img = self.input_img",
        "detail": "Fooocus.fooocus_extras.face_crop",
        "documentation": {}
    },
    {
        "label": "crop_image",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.face_crop",
        "description": "Fooocus.fooocus_extras.face_crop",
        "peekOfCode": "def crop_image(img_rgb):\n    global faceRestoreHelper\n    if faceRestoreHelper is None:\n        from fooocus_extras.facexlib.utils.face_restoration_helper import FaceRestoreHelper\n        faceRestoreHelper = FaceRestoreHelper(\n            upscale_factor=1,\n            model_rootpath=modules.config.path_controlnet,\n            device='cpu'  # use cpu is safer since we are out of fcbh management\n        )\n    faceRestoreHelper.clean_all()",
        "detail": "Fooocus.fooocus_extras.face_crop",
        "documentation": {}
    },
    {
        "label": "faceRestoreHelper",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.face_crop",
        "description": "Fooocus.fooocus_extras.face_crop",
        "peekOfCode": "faceRestoreHelper = None\ndef align_warp_face(self, landmark, border_mode='constant'):\n    affine_matrix = cv2.estimateAffinePartial2D(landmark, self.face_template, method=cv2.LMEDS)[0]\n    self.affine_matrices.append(affine_matrix)\n    if border_mode == 'constant':\n        border_mode = cv2.BORDER_CONSTANT\n    elif border_mode == 'reflect101':\n        border_mode = cv2.BORDER_REFLECT101\n    elif border_mode == 'reflect':\n        border_mode = cv2.BORDER_REFLECT",
        "detail": "Fooocus.fooocus_extras.face_crop",
        "documentation": {}
    },
    {
        "label": "ImageProjModel",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "class ImageProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n        super().__init__()\n        self.cross_attention_dim = cross_attention_dim\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n    def forward(self, image_embeds):\n        embeds = image_embeds\n        clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.clip_extra_context_tokens,",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "To_KV",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "class To_KV(torch.nn.Module):\n    def __init__(self, cross_attention_dim):\n        super().__init__()\n        channels = SD_XL_CHANNELS if cross_attention_dim == 2048 else SD_V12_CHANNELS\n        self.to_kvs = torch.nn.ModuleList(\n            [torch.nn.Linear(cross_attention_dim, channel, bias=False) for channel in channels])\n    def load_state_dict_ordered(self, sd):\n        state_dict = []\n        for i in range(4096):\n            for k in ['k', 'v']:",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "IPAdapterModel",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "class IPAdapterModel(torch.nn.Module):\n    def __init__(self, state_dict, plus, cross_attention_dim=768, clip_embeddings_dim=1024, clip_extra_context_tokens=4,\n                 sdxl_plus=False):\n        super().__init__()\n        self.plus = plus\n        if self.plus:\n            self.image_proj_model = Resampler(\n                dim=1280 if sdxl_plus else cross_attention_dim,\n                depth=4,\n                dim_head=64,",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "sdp",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "def sdp(q, k, v, extra_options):\n    return attention.optimized_attention(q, k, v, heads=extra_options[\"n_heads\"], mask=None)\nclass ImageProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n        super().__init__()\n        self.cross_attention_dim = cross_attention_dim\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n    def forward(self, image_embeds):",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "load_ip_adapter",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "def load_ip_adapter(clip_vision_path, ip_negative_path, ip_adapter_path):\n    global clip_vision, ip_negative, ip_adapters\n    if clip_vision is None and isinstance(clip_vision_path, str):\n        clip_vision = fcbh.clip_vision.load(clip_vision_path)\n    if ip_negative is None and isinstance(ip_negative_path, str):\n        ip_negative = sf.load_file(ip_negative_path)['data']\n    if not isinstance(ip_adapter_path, str) or ip_adapter_path in ip_adapters:\n        return\n    load_device = model_management.get_torch_device()\n    offload_device = torch.device('cpu')",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "clip_preprocess",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "def clip_preprocess(image):\n    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=image.device, dtype=image.dtype).view([1, 3, 1, 1])\n    std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=image.device, dtype=image.dtype).view([1, 3, 1, 1])\n    image = image.movedim(-1, 1)\n    # https://github.com/tencent-ailab/IP-Adapter/blob/d580c50a291566bbf9fc7ac0f760506607297e6d/README.md?plain=1#L75\n    B, C, H, W = image.shape\n    assert H == 224 and W == 224\n    return (image - mean) / std\n@torch.no_grad()\n@torch.inference_mode()",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "def preprocess(img, ip_adapter_path):\n    global ip_adapters\n    entry = ip_adapters[ip_adapter_path]\n    fcbh.model_management.load_model_gpu(clip_vision.patcher)\n    pixel_values = clip_preprocess(numpy_to_pytorch(img).to(clip_vision.load_device))\n    if clip_vision.dtype != torch.float32:\n        precision_scope = torch.autocast\n    else:\n        precision_scope = lambda a, b: contextlib.nullcontext(a)\n    with precision_scope(fcbh.model_management.get_autocast_device(clip_vision.load_device), torch.float32):",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "patch_model",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "def patch_model(model, tasks):\n    new_model = model.clone()\n    def make_attn_patcher(ip_index):\n        def patcher(n, context_attn2, value_attn2, extra_options):\n            org_dtype = n.dtype\n            current_step = float(model.model.diffusion_model.current_step.detach().cpu().numpy()[0])\n            cond_or_uncond = extra_options['cond_or_uncond']\n            q = n\n            k = [context_attn2]\n            v = [value_attn2]",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "SD_V12_CHANNELS",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "SD_V12_CHANNELS = [320] * 4 + [640] * 4 + [1280] * 4 + [1280] * 6 + [640] * 6 + [320] * 6 + [1280] * 2\nSD_XL_CHANNELS = [640] * 8 + [1280] * 40 + [1280] * 60 + [640] * 12 + [1280] * 20\ndef sdp(q, k, v, extra_options):\n    return attention.optimized_attention(q, k, v, heads=extra_options[\"n_heads\"], mask=None)\nclass ImageProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n        super().__init__()\n        self.cross_attention_dim = cross_attention_dim\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "SD_XL_CHANNELS",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.ip_adapter",
        "description": "Fooocus.fooocus_extras.ip_adapter",
        "peekOfCode": "SD_XL_CHANNELS = [640] * 8 + [1280] * 40 + [1280] * 60 + [640] * 12 + [1280] * 20\ndef sdp(q, k, v, extra_options):\n    return attention.optimized_attention(q, k, v, heads=extra_options[\"n_heads\"], mask=None)\nclass ImageProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n        super().__init__()\n        self.cross_attention_dim = cross_attention_dim\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)",
        "detail": "Fooocus.fooocus_extras.ip_adapter",
        "documentation": {}
    },
    {
        "label": "centered_canny",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.preprocessors",
        "description": "Fooocus.fooocus_extras.preprocessors",
        "peekOfCode": "def centered_canny(x: np.ndarray):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 2 and x.dtype == np.uint8\n    y = cv2.Canny(x, int(advanced_parameters.canny_low_threshold), int(advanced_parameters.canny_high_threshold))\n    y = y.astype(np.float32) / 255.0\n    return y\ndef centered_canny_color(x: np.ndarray):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 3 and x.shape[2] == 3\n    result = [centered_canny(x[..., i]) for i in range(3)]",
        "detail": "Fooocus.fooocus_extras.preprocessors",
        "documentation": {}
    },
    {
        "label": "centered_canny_color",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.preprocessors",
        "description": "Fooocus.fooocus_extras.preprocessors",
        "peekOfCode": "def centered_canny_color(x: np.ndarray):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 3 and x.shape[2] == 3\n    result = [centered_canny(x[..., i]) for i in range(3)]\n    result = np.stack(result, axis=2)\n    return result\ndef pyramid_canny_color(x: np.ndarray):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 3 and x.shape[2] == 3\n    H, W, C = x.shape",
        "detail": "Fooocus.fooocus_extras.preprocessors",
        "documentation": {}
    },
    {
        "label": "pyramid_canny_color",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.preprocessors",
        "description": "Fooocus.fooocus_extras.preprocessors",
        "peekOfCode": "def pyramid_canny_color(x: np.ndarray):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 3 and x.shape[2] == 3\n    H, W, C = x.shape\n    acc_edge = None\n    for k in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n        Hs, Ws = int(H * k), int(W * k)\n        small = cv2.resize(x, (Ws, Hs), interpolation=cv2.INTER_AREA)\n        edge = centered_canny_color(small)\n        if acc_edge is None:",
        "detail": "Fooocus.fooocus_extras.preprocessors",
        "documentation": {}
    },
    {
        "label": "norm255",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.preprocessors",
        "description": "Fooocus.fooocus_extras.preprocessors",
        "peekOfCode": "def norm255(x, low=4, high=96):\n    assert isinstance(x, np.ndarray)\n    assert x.ndim == 2 and x.dtype == np.float32\n    v_min = np.percentile(x, low)\n    v_max = np.percentile(x, high)\n    x -= v_min\n    x /= v_max - v_min\n    return x * 255.0\ndef canny_pyramid(x):\n    # For some reasons, SAI's Control-lora Canny seems to be trained on canny maps with non-standard resolutions.",
        "detail": "Fooocus.fooocus_extras.preprocessors",
        "documentation": {}
    },
    {
        "label": "canny_pyramid",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.preprocessors",
        "description": "Fooocus.fooocus_extras.preprocessors",
        "peekOfCode": "def canny_pyramid(x):\n    # For some reasons, SAI's Control-lora Canny seems to be trained on canny maps with non-standard resolutions.\n    # Then we use pyramid to use all resolutions to avoid missing any structure in specific resolutions.\n    color_canny = pyramid_canny_color(x)\n    result = np.sum(color_canny, axis=2)\n    return norm255(result, low=1, high=99).clip(0, 255).astype(np.uint8)\ndef cpds(x):\n    # cv2.decolor is not \"decolor\", it is Cewu Lu's method\n    # See http://www.cse.cuhk.edu.hk/leojia/projects/color2gray/index.html\n    # See https://docs.opencv.org/3.0-beta/modules/photo/doc/decolor.html",
        "detail": "Fooocus.fooocus_extras.preprocessors",
        "documentation": {}
    },
    {
        "label": "cpds",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.preprocessors",
        "description": "Fooocus.fooocus_extras.preprocessors",
        "peekOfCode": "def cpds(x):\n    # cv2.decolor is not \"decolor\", it is Cewu Lu's method\n    # See http://www.cse.cuhk.edu.hk/leojia/projects/color2gray/index.html\n    # See https://docs.opencv.org/3.0-beta/modules/photo/doc/decolor.html\n    raw = cv2.GaussianBlur(x, (0, 0), 0.8)\n    density, boost = cv2.decolor(raw)\n    raw = raw.astype(np.float32)\n    density = density.astype(np.float32)\n    boost = boost.astype(np.float32)\n    offset = np.sum((raw - boost) ** 2.0, axis=2) ** 0.5",
        "detail": "Fooocus.fooocus_extras.preprocessors",
        "documentation": {}
    },
    {
        "label": "PerceiverAttention",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.resampler",
        "description": "Fooocus.fooocus_extras.resampler",
        "peekOfCode": "class PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.dim_head = dim_head\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)",
        "detail": "Fooocus.fooocus_extras.resampler",
        "documentation": {}
    },
    {
        "label": "Resampler",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.resampler",
        "description": "Fooocus.fooocus_extras.resampler",
        "peekOfCode": "class Resampler(nn.Module):\n    def __init__(\n        self,\n        dim=1024,\n        depth=8,\n        dim_head=64,\n        heads=16,\n        num_queries=8,\n        embedding_dim=768,\n        output_dim=1024,",
        "detail": "Fooocus.fooocus_extras.resampler",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.resampler",
        "description": "Fooocus.fooocus_extras.resampler",
        "peekOfCode": "def FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )\ndef reshape_tensor(x, heads):\n    bs, length, width = x.shape",
        "detail": "Fooocus.fooocus_extras.resampler",
        "documentation": {}
    },
    {
        "label": "reshape_tensor",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.resampler",
        "description": "Fooocus.fooocus_extras.resampler",
        "peekOfCode": "def reshape_tensor(x, heads):\n    bs, length, width = x.shape\n    #(bs, length, width) --> (bs, length, n_heads, dim_per_head)\n    x = x.view(bs, length, heads, -1)\n    # (bs, length, n_heads, dim_per_head) --> (bs, n_heads, length, dim_per_head)\n    x = x.transpose(1, 2)\n    # (bs, n_heads, length, dim_per_head) --> (bs*n_heads, length, dim_per_head)\n    x = x.reshape(bs, heads, length, -1)\n    return x\nclass PerceiverAttention(nn.Module):",
        "detail": "Fooocus.fooocus_extras.resampler",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.vae_interpose",
        "description": "Fooocus.fooocus_extras.vae_interpose",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.join = nn.ReLU()\n        self.long = nn.Sequential(\n            nn.Conv2d(size, size, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(size, size, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(size, size, kernel_size=3, stride=1, padding=1),",
        "detail": "Fooocus.fooocus_extras.vae_interpose",
        "documentation": {}
    },
    {
        "label": "Interposer",
        "kind": 6,
        "importPath": "Fooocus.fooocus_extras.vae_interpose",
        "description": "Fooocus.fooocus_extras.vae_interpose",
        "peekOfCode": "class Interposer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.chan = 4\n        self.hid = 128\n        self.head_join = nn.ReLU()\n        self.head_short = nn.Conv2d(self.chan, self.hid, kernel_size=3, stride=1, padding=1)\n        self.head_long = nn.Sequential(\n            nn.Conv2d(self.chan, self.hid, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),",
        "detail": "Fooocus.fooocus_extras.vae_interpose",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "Fooocus.fooocus_extras.vae_interpose",
        "description": "Fooocus.fooocus_extras.vae_interpose",
        "peekOfCode": "def parse(x):\n    global vae_approx_model\n    x_origin = x.clone()\n    if vae_approx_model is None:\n        model = Interposer()\n        model.eval()\n        sd = sf.load_file(vae_approx_filename)\n        model.load_state_dict(sd)\n        fp16 = fcbh.model_management.should_use_fp16()\n        if fp16:",
        "detail": "Fooocus.fooocus_extras.vae_interpose",
        "documentation": {}
    },
    {
        "label": "vae_approx_model",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.vae_interpose",
        "description": "Fooocus.fooocus_extras.vae_interpose",
        "peekOfCode": "vae_approx_model = None\nvae_approx_filename = os.path.join(path_vae_approx, 'xl-to-v1_interposer-v3.1.safetensors')\ndef parse(x):\n    global vae_approx_model\n    x_origin = x.clone()\n    if vae_approx_model is None:\n        model = Interposer()\n        model.eval()\n        sd = sf.load_file(vae_approx_filename)\n        model.load_state_dict(sd)",
        "detail": "Fooocus.fooocus_extras.vae_interpose",
        "documentation": {}
    },
    {
        "label": "vae_approx_filename",
        "kind": 5,
        "importPath": "Fooocus.fooocus_extras.vae_interpose",
        "description": "Fooocus.fooocus_extras.vae_interpose",
        "peekOfCode": "vae_approx_filename = os.path.join(path_vae_approx, 'xl-to-v1_interposer-v3.1.safetensors')\ndef parse(x):\n    global vae_approx_model\n    x_origin = x.clone()\n    if vae_approx_model is None:\n        model = Interposer()\n        model.eval()\n        sd = sf.load_file(vae_approx_filename)\n        model.load_state_dict(sd)\n        fp16 = fcbh.model_management.should_use_fp16()",
        "detail": "Fooocus.fooocus_extras.vae_interpose",
        "documentation": {}
    },
    {
        "label": "set_all_advanced_parameters",
        "kind": 2,
        "importPath": "Fooocus.modules.advanced_parameters",
        "description": "Fooocus.modules.advanced_parameters",
        "peekOfCode": "def set_all_advanced_parameters(*args):\n    global disable_preview, adm_scaler_positive, adm_scaler_negative, adm_scaler_end, adaptive_cfg, sampler_name, \\\n        scheduler_name, generate_image_grid, overwrite_step, overwrite_switch, overwrite_width, overwrite_height, \\\n        overwrite_vary_strength, overwrite_upscale_strength, \\\n        mixing_image_prompt_and_vary_upscale, mixing_image_prompt_and_inpaint, \\\n        debugging_cn_preprocessor, skipping_cn_preprocessor, controlnet_softness, canny_low_threshold, canny_high_threshold, \\\n        refiner_swap_method, \\\n        freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2, \\\n        debugging_inpaint_preprocessor, inpaint_disable_initial_latent, inpaint_engine, inpaint_strength, inpaint_respective_field\n    disable_preview, adm_scaler_positive, adm_scaler_negative, adm_scaler_end, adaptive_cfg, sampler_name, \\",
        "detail": "Fooocus.modules.advanced_parameters",
        "documentation": {}
    },
    {
        "label": "_BilateralBlur",
        "kind": 6,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "class _BilateralBlur(torch.nn.Module):\n    def __init__(\n        self,\n        kernel_size: tuple[int, int] | int,\n        sigma_color: float | Tensor,\n        sigma_space: tuple[float, float] | Tensor,\n        border_type: str = 'reflect',\n        color_distance_type: str = \"l1\",\n    ) -> None:\n        super().__init__()",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "BilateralBlur",
        "kind": 6,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "class BilateralBlur(_BilateralBlur):\n    def forward(self, input: Tensor) -> Tensor:\n        return bilateral_blur(\n            input, self.kernel_size, self.sigma_color, self.sigma_space, self.border_type, self.color_distance_type\n        )\nclass JointBilateralBlur(_BilateralBlur):\n    def forward(self, input: Tensor, guidance: Tensor) -> Tensor:\n        return joint_bilateral_blur(\n            input,\n            guidance,",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "JointBilateralBlur",
        "kind": 6,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "class JointBilateralBlur(_BilateralBlur):\n    def forward(self, input: Tensor, guidance: Tensor) -> Tensor:\n        return joint_bilateral_blur(\n            input,\n            guidance,\n            self.kernel_size,\n            self.sigma_color,\n            self.sigma_space,\n            self.border_type,\n            self.color_distance_type,",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "gaussian",
        "kind": 2,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "def gaussian(\n    window_size: int, sigma: Tensor | float, *, device: Device | None = None, dtype: Dtype | None = None\n) -> Tensor:\n    batch_size = sigma.shape[0]\n    x = (torch.arange(window_size, device=sigma.device, dtype=sigma.dtype) - window_size // 2).expand(batch_size, -1)\n    if window_size % 2 == 0:\n        x = x + 0.5\n    gauss = torch.exp(-x.pow(2.0) / (2 * sigma.pow(2.0)))\n    return gauss / gauss.sum(-1, keepdim=True)\ndef get_gaussian_kernel1d(",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "get_gaussian_kernel1d",
        "kind": 2,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "def get_gaussian_kernel1d(\n    kernel_size: int,\n    sigma: float | Tensor,\n    force_even: bool = False,\n    *,\n    device: Device | None = None,\n    dtype: Dtype | None = None,\n) -> Tensor:\n    return gaussian(kernel_size, sigma, device=device, dtype=dtype)\ndef get_gaussian_kernel2d(",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "get_gaussian_kernel2d",
        "kind": 2,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "def get_gaussian_kernel2d(\n    kernel_size: tuple[int, int] | int,\n    sigma: tuple[float, float] | Tensor,\n    force_even: bool = False,\n    *,\n    device: Device | None = None,\n    dtype: Dtype | None = None,\n) -> Tensor:\n    sigma = torch.Tensor([[sigma, sigma]]).to(device=device, dtype=dtype)\n    ksize_y, ksize_x = _unpack_2d_ks(kernel_size)",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "bilateral_blur",
        "kind": 2,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "def bilateral_blur(\n    input: Tensor,\n    kernel_size: tuple[int, int] | int = (13, 13),\n    sigma_color: float | Tensor = 3.0,\n    sigma_space: tuple[float, float] | Tensor = 3.0,\n    border_type: str = 'reflect',\n    color_distance_type: str = 'l1',\n) -> Tensor:\n    return _bilateral_blur(input, None, kernel_size, sigma_color, sigma_space, border_type, color_distance_type)\ndef adaptive_anisotropic_filter(x, g=None):",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "adaptive_anisotropic_filter",
        "kind": 2,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "def adaptive_anisotropic_filter(x, g=None):\n    if g is None:\n        g = x\n    s, m = torch.std_mean(g, dim=(1, 2, 3), keepdim=True)\n    s = s + 1e-5\n    guidance = (g - m) / s\n    y = _bilateral_blur(x, guidance,\n                        kernel_size=(13, 13),\n                        sigma_color=3.0,\n                        sigma_space=3.0,",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "joint_bilateral_blur",
        "kind": 2,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "def joint_bilateral_blur(\n    input: Tensor,\n    guidance: Tensor,\n    kernel_size: tuple[int, int] | int,\n    sigma_color: float | Tensor,\n    sigma_space: tuple[float, float] | Tensor,\n    border_type: str = 'reflect',\n    color_distance_type: str = 'l1',\n) -> Tensor:\n    return _bilateral_blur(input, guidance, kernel_size, sigma_color, sigma_space, border_type, color_distance_type)",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "kind": 5,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "Tensor = torch.Tensor\nDevice = torch.DeviceObjType\nDtype = torch.Type\npad = torch.nn.functional.pad\ndef _compute_zero_padding(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    ky, kx = _unpack_2d_ks(kernel_size)\n    return (ky - 1) // 2, (kx - 1) // 2\ndef _unpack_2d_ks(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    if isinstance(kernel_size, int):\n        ky = kx = kernel_size",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "Device",
        "kind": 5,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "Device = torch.DeviceObjType\nDtype = torch.Type\npad = torch.nn.functional.pad\ndef _compute_zero_padding(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    ky, kx = _unpack_2d_ks(kernel_size)\n    return (ky - 1) // 2, (kx - 1) // 2\ndef _unpack_2d_ks(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    if isinstance(kernel_size, int):\n        ky = kx = kernel_size\n    else:",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "Dtype",
        "kind": 5,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "Dtype = torch.Type\npad = torch.nn.functional.pad\ndef _compute_zero_padding(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    ky, kx = _unpack_2d_ks(kernel_size)\n    return (ky - 1) // 2, (kx - 1) // 2\ndef _unpack_2d_ks(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    if isinstance(kernel_size, int):\n        ky = kx = kernel_size\n    else:\n        assert len(kernel_size) == 2, '2D Kernel size should have a length of 2.'",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 5,
        "importPath": "Fooocus.modules.anisotropic",
        "description": "Fooocus.modules.anisotropic",
        "peekOfCode": "pad = torch.nn.functional.pad\ndef _compute_zero_padding(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    ky, kx = _unpack_2d_ks(kernel_size)\n    return (ky - 1) // 2, (kx - 1) // 2\ndef _unpack_2d_ks(kernel_size: tuple[int, int] | int) -> tuple[int, int]:\n    if isinstance(kernel_size, int):\n        ky = kx = kernel_size\n    else:\n        assert len(kernel_size) == 2, '2D Kernel size should have a length of 2.'\n        ky, kx = kernel_size",
        "detail": "Fooocus.modules.anisotropic",
        "documentation": {}
    },
    {
        "label": "AsyncTask",
        "kind": 6,
        "importPath": "Fooocus.modules.async_worker",
        "description": "Fooocus.modules.async_worker",
        "peekOfCode": "class AsyncTask:\n    def __init__(self, args):\n        self.args = args\n        self.yields = []\n        self.results = []\nasync_tasks = []\ndef worker():\n    global async_tasks\n    import traceback\n    import math",
        "detail": "Fooocus.modules.async_worker",
        "documentation": {}
    },
    {
        "label": "worker",
        "kind": 2,
        "importPath": "Fooocus.modules.async_worker",
        "description": "Fooocus.modules.async_worker",
        "peekOfCode": "def worker():\n    global async_tasks\n    import traceback\n    import math\n    import numpy as np\n    import torch\n    import time\n    import shared\n    import random\n    import copy",
        "detail": "Fooocus.modules.async_worker",
        "documentation": {}
    },
    {
        "label": "async_tasks",
        "kind": 5,
        "importPath": "Fooocus.modules.async_worker",
        "description": "Fooocus.modules.async_worker",
        "peekOfCode": "async_tasks = []\ndef worker():\n    global async_tasks\n    import traceback\n    import math\n    import numpy as np\n    import torch\n    import time\n    import shared\n    import random",
        "detail": "Fooocus.modules.async_worker",
        "documentation": {}
    },
    {
        "label": "auth_list_to_dict",
        "kind": 2,
        "importPath": "Fooocus.modules.auth",
        "description": "Fooocus.modules.auth",
        "peekOfCode": "def auth_list_to_dict(auth_list):\n    auth_dict = {}\n    for auth_data in auth_list:\n        if 'user' in auth_data:\n            if 'hash' in auth_data:\n                auth_dict |= {auth_data['user']: auth_data['hash']}\n            elif 'pass' in auth_data:\n                auth_dict |= {auth_data['user']: hashlib.sha256(bytes(auth_data['pass'], encoding='utf-8')).hexdigest()}\n    return auth_dict\ndef load_auth_data(filename=None):",
        "detail": "Fooocus.modules.auth",
        "documentation": {}
    },
    {
        "label": "load_auth_data",
        "kind": 2,
        "importPath": "Fooocus.modules.auth",
        "description": "Fooocus.modules.auth",
        "peekOfCode": "def load_auth_data(filename=None):\n    auth_dict = None\n    if filename != None and exists(filename):\n        with open(filename, encoding='utf-8') as auth_file:\n            try:\n                auth_obj = json.load(auth_file)\n                if isinstance(auth_obj, list) and len(auth_obj) > 0:\n                    auth_dict = auth_list_to_dict(auth_obj)\n            except Exception as e:\n                print('load_auth_data, e: ' + str(e))",
        "detail": "Fooocus.modules.auth",
        "documentation": {}
    },
    {
        "label": "check_auth",
        "kind": 2,
        "importPath": "Fooocus.modules.auth",
        "description": "Fooocus.modules.auth",
        "peekOfCode": "def check_auth(user, password):\n    if user not in auth_dict:\n        return False\n    else:   \n        return hashlib.sha256(bytes(password, encoding='utf-8')).hexdigest() == auth_dict[user]",
        "detail": "Fooocus.modules.auth",
        "documentation": {}
    },
    {
        "label": "auth_dict",
        "kind": 5,
        "importPath": "Fooocus.modules.auth",
        "description": "Fooocus.modules.auth",
        "peekOfCode": "auth_dict = load_auth_data(constants.AUTH_FILENAME)\nauth_enabled = auth_dict != None\ndef check_auth(user, password):\n    if user not in auth_dict:\n        return False\n    else:   \n        return hashlib.sha256(bytes(password, encoding='utf-8')).hexdigest() == auth_dict[user]",
        "detail": "Fooocus.modules.auth",
        "documentation": {}
    },
    {
        "label": "auth_enabled",
        "kind": 5,
        "importPath": "Fooocus.modules.auth",
        "description": "Fooocus.modules.auth",
        "peekOfCode": "auth_enabled = auth_dict != None\ndef check_auth(user, password):\n    if user not in auth_dict:\n        return False\n    else:   \n        return hashlib.sha256(bytes(password, encoding='utf-8')).hexdigest() == auth_dict[user]",
        "detail": "Fooocus.modules.auth",
        "documentation": {}
    },
    {
        "label": "try_load_deprecated_user_path_config",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def try_load_deprecated_user_path_config():\n    global config_dict\n    if not os.path.exists('user_path_config.txt'):\n        return\n    try:\n        deprecated_config_dict = json.load(open('user_path_config.txt', \"r\", encoding=\"utf-8\"))\n        def replace_config(old_key, new_key):\n            if old_key in deprecated_config_dict:\n                config_dict[new_key] = deprecated_config_dict[old_key]\n                del deprecated_config_dict[old_key]",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "get_dir_or_set_default",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def get_dir_or_set_default(key, default_value):\n    global config_dict, visited_keys, always_save_keys\n    if key not in visited_keys:\n        visited_keys.append(key)\n    if key not in always_save_keys:\n        always_save_keys.append(key)\n    v = config_dict.get(key, None)\n    if isinstance(v, str) and os.path.exists(v) and os.path.isdir(v):\n        return v\n    else:",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "get_config_item_or_set_default",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:\n        visited_keys.append(key)\n    if key not in config_dict:\n        config_dict[key] = default_value\n        return default_value\n    v = config_dict.get(key, None)\n    if not disable_empty_as_none:\n        if v is None or v == '':",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "add_ratio",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def add_ratio(x):\n    a, b = x.replace('*', ' ').split(' ')[:2]\n    a, b = int(a), int(b)\n    g = math.gcd(a, b)\n    return f'{a}{b} <span style=\"color: grey;\"> \\U00002223 {a // g}:{b // g}</span>'\ndefault_aspect_ratio = add_ratio(default_aspect_ratio)\navailable_aspect_ratios = [add_ratio(x) for x in available_aspect_ratios]\n# Only write config in the first launch.\nif not os.path.exists(config_path):\n    with open(config_path, \"w\", encoding=\"utf-8\") as json_file:",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "get_model_filenames",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def get_model_filenames(folder_path, name_filter=None):\n    return get_files_from_folder(folder_path, ['.pth', '.ckpt', '.bin', '.safetensors', '.fooocus.patch'], name_filter)\ndef update_all_model_names():\n    global model_filenames, lora_filenames\n    model_filenames = get_model_filenames(path_checkpoints)\n    lora_filenames = get_model_filenames(path_loras)\n    return\ndef downloading_inpaint_models(v):\n    assert v in modules.flags.inpaint_engine_versions\n    load_file_from_url(",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "update_all_model_names",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def update_all_model_names():\n    global model_filenames, lora_filenames\n    model_filenames = get_model_filenames(path_checkpoints)\n    lora_filenames = get_model_filenames(path_loras)\n    return\ndef downloading_inpaint_models(v):\n    assert v in modules.flags.inpaint_engine_versions\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/fooocus_inpaint_head.pth',\n        model_dir=path_inpaint,",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "downloading_inpaint_models",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def downloading_inpaint_models(v):\n    assert v in modules.flags.inpaint_engine_versions\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/fooocus_inpaint_head.pth',\n        model_dir=path_inpaint,\n        file_name='fooocus_inpaint_head.pth'\n    )\n    head_file = os.path.join(path_inpaint, 'fooocus_inpaint_head.pth')\n    patch_file = None\n    if v == 'v1':",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "downloading_sdxl_lcm_lora",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def downloading_sdxl_lcm_lora():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/sdxl_lcm_lora.safetensors',\n        model_dir=path_loras,\n        file_name='sdxl_lcm_lora.safetensors'\n    )\n    return 'sdxl_lcm_lora.safetensors'\ndef downloading_controlnet_canny():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/control-lora-canny-rank128.safetensors',",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "downloading_controlnet_canny",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def downloading_controlnet_canny():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/control-lora-canny-rank128.safetensors',\n        model_dir=path_controlnet,\n        file_name='control-lora-canny-rank128.safetensors'\n    )\n    return os.path.join(path_controlnet, 'control-lora-canny-rank128.safetensors')\ndef downloading_controlnet_cpds():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_xl_cpds_128.safetensors',",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "downloading_controlnet_cpds",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def downloading_controlnet_cpds():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_xl_cpds_128.safetensors',\n        model_dir=path_controlnet,\n        file_name='fooocus_xl_cpds_128.safetensors'\n    )\n    return os.path.join(path_controlnet, 'fooocus_xl_cpds_128.safetensors')\ndef downloading_ip_adapters(v):\n    assert v in ['ip', 'face']\n    results = []",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "downloading_ip_adapters",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def downloading_ip_adapters(v):\n    assert v in ['ip', 'face']\n    results = []\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/clip_vision_vit_h.safetensors',\n        model_dir=path_clip_vision,\n        file_name='clip_vision_vit_h.safetensors'\n    )\n    results += [os.path.join(path_clip_vision, 'clip_vision_vit_h.safetensors')]\n    load_file_from_url(",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "downloading_upscale_model",
        "kind": 2,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "def downloading_upscale_model():\n    load_file_from_url(\n        url='https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_upscaler_s409985e5.bin',\n        model_dir=path_upscale_models,\n        file_name='fooocus_upscaler_s409985e5.bin'\n    )\n    return os.path.join(path_upscale_models, 'fooocus_upscaler_s409985e5.bin')\nupdate_all_model_names()",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "config_path = os.path.abspath(\"./config.txt\")\nconfig_example_path = os.path.abspath(\"config_modification_tutorial.txt\")\nconfig_dict = {}\nalways_save_keys = []\nvisited_keys = []\ntry:\n    if os.path.exists(config_path):\n        with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict = json.load(json_file)\n            always_save_keys = list(config_dict.keys())",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "config_example_path",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "config_example_path = os.path.abspath(\"config_modification_tutorial.txt\")\nconfig_dict = {}\nalways_save_keys = []\nvisited_keys = []\ntry:\n    if os.path.exists(config_path):\n        with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict = json.load(json_file)\n            always_save_keys = list(config_dict.keys())\nexcept Exception as e:",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "config_dict",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "config_dict = {}\nalways_save_keys = []\nvisited_keys = []\ntry:\n    if os.path.exists(config_path):\n        with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict = json.load(json_file)\n            always_save_keys = list(config_dict.keys())\nexcept Exception as e:\n    print(f'Failed to load config file \"{config_path}\" . The reason is: {str(e)}')",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "always_save_keys",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "always_save_keys = []\nvisited_keys = []\ntry:\n    if os.path.exists(config_path):\n        with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict = json.load(json_file)\n            always_save_keys = list(config_dict.keys())\nexcept Exception as e:\n    print(f'Failed to load config file \"{config_path}\" . The reason is: {str(e)}')\n    print('Please make sure that:')",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "visited_keys",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "visited_keys = []\ntry:\n    if os.path.exists(config_path):\n        with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict = json.load(json_file)\n            always_save_keys = list(config_dict.keys())\nexcept Exception as e:\n    print(f'Failed to load config file \"{config_path}\" . The reason is: {str(e)}')\n    print('Please make sure that:')\n    print(f'1. The file \"{config_path}\" is a valid text file, and you have access to read it.')",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "preset",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "preset = args_manager.args.preset\nif isinstance(preset, str):\n    preset_path = os.path.abspath(f'./presets/{preset}.json')\n    try:\n        if os.path.exists(preset_path):\n            with open(preset_path, \"r\", encoding=\"utf-8\") as json_file:\n                config_dict.update(json.load(json_file))\n                print(f'Loaded preset: {preset_path}')\n        else:\n            raise FileNotFoundError",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_checkpoints",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_checkpoints = get_dir_or_set_default('path_checkpoints', '../models/checkpoints/')\npath_loras = get_dir_or_set_default('path_loras', '../models/loras/')\npath_embeddings = get_dir_or_set_default('path_embeddings', '../models/embeddings/')\npath_vae_approx = get_dir_or_set_default('path_vae_approx', '../models/vae_approx/')\npath_upscale_models = get_dir_or_set_default('path_upscale_models', '../models/upscale_models/')\npath_inpaint = get_dir_or_set_default('path_inpaint', '../models/inpaint/')\npath_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_loras",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_loras = get_dir_or_set_default('path_loras', '../models/loras/')\npath_embeddings = get_dir_or_set_default('path_embeddings', '../models/embeddings/')\npath_vae_approx = get_dir_or_set_default('path_vae_approx', '../models/vae_approx/')\npath_upscale_models = get_dir_or_set_default('path_upscale_models', '../models/upscale_models/')\npath_inpaint = get_dir_or_set_default('path_inpaint', '../models/inpaint/')\npath_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_embeddings",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_embeddings = get_dir_or_set_default('path_embeddings', '../models/embeddings/')\npath_vae_approx = get_dir_or_set_default('path_vae_approx', '../models/vae_approx/')\npath_upscale_models = get_dir_or_set_default('path_upscale_models', '../models/upscale_models/')\npath_inpaint = get_dir_or_set_default('path_inpaint', '../models/inpaint/')\npath_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_vae_approx",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_vae_approx = get_dir_or_set_default('path_vae_approx', '../models/vae_approx/')\npath_upscale_models = get_dir_or_set_default('path_upscale_models', '../models/upscale_models/')\npath_inpaint = get_dir_or_set_default('path_inpaint', '../models/inpaint/')\npath_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_upscale_models",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_upscale_models = get_dir_or_set_default('path_upscale_models', '../models/upscale_models/')\npath_inpaint = get_dir_or_set_default('path_inpaint', '../models/inpaint/')\npath_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:\n        visited_keys.append(key)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_inpaint",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_inpaint = get_dir_or_set_default('path_inpaint', '../models/inpaint/')\npath_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:\n        visited_keys.append(key)\n    if key not in config_dict:",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_controlnet",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_controlnet = get_dir_or_set_default('path_controlnet', '../models/controlnet/')\npath_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:\n        visited_keys.append(key)\n    if key not in config_dict:\n        config_dict[key] = default_value",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_clip_vision",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_clip_vision = get_dir_or_set_default('path_clip_vision', '../models/clip_vision/')\npath_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:\n        visited_keys.append(key)\n    if key not in config_dict:\n        config_dict[key] = default_value\n        return default_value",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_fooocus_expansion",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_fooocus_expansion = get_dir_or_set_default('path_fooocus_expansion', '../models/prompt_expansion/fooocus_expansion')\npath_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:\n        visited_keys.append(key)\n    if key not in config_dict:\n        config_dict[key] = default_value\n        return default_value\n    v = config_dict.get(key, None)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "path_outputs",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "path_outputs = get_dir_or_set_default('path_outputs', '../outputs/')\ndef get_config_item_or_set_default(key, default_value, validator, disable_empty_as_none=False):\n    global config_dict, visited_keys\n    if key not in visited_keys:\n        visited_keys.append(key)\n    if key not in config_dict:\n        config_dict[key] = default_value\n        return default_value\n    v = config_dict.get(key, None)\n    if not disable_empty_as_none:",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_base_model_name",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_base_model_name = get_config_item_or_set_default(\n    key='default_model',\n    default_value='juggernautXL_version6Rundiffusion.safetensors',\n    validator=lambda x: isinstance(x, str)\n)\ndefault_refiner_model_name = get_config_item_or_set_default(\n    key='default_refiner',\n    default_value='None',\n    validator=lambda x: isinstance(x, str)\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_refiner_model_name",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_refiner_model_name = get_config_item_or_set_default(\n    key='default_refiner',\n    default_value='None',\n    validator=lambda x: isinstance(x, str)\n)\ndefault_refiner_switch = get_config_item_or_set_default(\n    key='default_refiner_switch',\n    default_value=0.5,\n    validator=lambda x: isinstance(x, numbers.Number) and 0 <= x <= 1\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_refiner_switch",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_refiner_switch = get_config_item_or_set_default(\n    key='default_refiner_switch',\n    default_value=0.5,\n    validator=lambda x: isinstance(x, numbers.Number) and 0 <= x <= 1\n)\ndefault_loras = get_config_item_or_set_default(\n    key='default_loras',\n    default_value=[\n        [\n            \"sd_xl_offset_example-lora_1.0.safetensors\",",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_loras",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_loras = get_config_item_or_set_default(\n    key='default_loras',\n    default_value=[\n        [\n            \"sd_xl_offset_example-lora_1.0.safetensors\",\n            0.1\n        ],\n        [\n            \"None\",\n            1.0",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_cfg_scale",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_cfg_scale = get_config_item_or_set_default(\n    key='default_cfg_scale',\n    default_value=4.0,\n    validator=lambda x: isinstance(x, numbers.Number)\n)\ndefault_sample_sharpness = get_config_item_or_set_default(\n    key='default_sample_sharpness',\n    default_value=2.0,\n    validator=lambda x: isinstance(x, numbers.Number)\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_sample_sharpness",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_sample_sharpness = get_config_item_or_set_default(\n    key='default_sample_sharpness',\n    default_value=2.0,\n    validator=lambda x: isinstance(x, numbers.Number)\n)\ndefault_sampler = get_config_item_or_set_default(\n    key='default_sampler',\n    default_value='dpmpp_2m_sde_gpu',\n    validator=lambda x: x in modules.flags.sampler_list\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_sampler",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_sampler = get_config_item_or_set_default(\n    key='default_sampler',\n    default_value='dpmpp_2m_sde_gpu',\n    validator=lambda x: x in modules.flags.sampler_list\n)\ndefault_scheduler = get_config_item_or_set_default(\n    key='default_scheduler',\n    default_value='karras',\n    validator=lambda x: x in modules.flags.scheduler_list\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_scheduler",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_scheduler = get_config_item_or_set_default(\n    key='default_scheduler',\n    default_value='karras',\n    validator=lambda x: x in modules.flags.scheduler_list\n)\ndefault_styles = get_config_item_or_set_default(\n    key='default_styles',\n    default_value=[\n        \"Fooocus V2\",\n        \"Fooocus Enhance\",",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_styles",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_styles = get_config_item_or_set_default(\n    key='default_styles',\n    default_value=[\n        \"Fooocus V2\",\n        \"Fooocus Enhance\",\n        \"Fooocus Sharp\"\n    ],\n    validator=lambda x: isinstance(x, list) and all(y in modules.sdxl_styles.legal_style_names for y in x)\n)\ndefault_prompt_negative = get_config_item_or_set_default(",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_prompt_negative",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_prompt_negative = get_config_item_or_set_default(\n    key='default_prompt_negative',\n    default_value='',\n    validator=lambda x: isinstance(x, str),\n    disable_empty_as_none=True\n)\ndefault_prompt = get_config_item_or_set_default(\n    key='default_prompt',\n    default_value='',\n    validator=lambda x: isinstance(x, str),",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_prompt",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_prompt = get_config_item_or_set_default(\n    key='default_prompt',\n    default_value='',\n    validator=lambda x: isinstance(x, str),\n    disable_empty_as_none=True\n)\ndefault_performance = get_config_item_or_set_default(\n    key='default_performance',\n    default_value='Speed',\n    validator=lambda x: x in modules.flags.performance_selections",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_performance",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_performance = get_config_item_or_set_default(\n    key='default_performance',\n    default_value='Speed',\n    validator=lambda x: x in modules.flags.performance_selections\n)\ndefault_advanced_checkbox = get_config_item_or_set_default(\n    key='default_advanced_checkbox',\n    default_value=False,\n    validator=lambda x: isinstance(x, bool)\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_advanced_checkbox",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_advanced_checkbox = get_config_item_or_set_default(\n    key='default_advanced_checkbox',\n    default_value=False,\n    validator=lambda x: isinstance(x, bool)\n)\ndefault_image_number = get_config_item_or_set_default(\n    key='default_image_number',\n    default_value=2,\n    validator=lambda x: isinstance(x, int) and 1 <= x <= 32\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_image_number",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_image_number = get_config_item_or_set_default(\n    key='default_image_number',\n    default_value=2,\n    validator=lambda x: isinstance(x, int) and 1 <= x <= 32\n)\ncheckpoint_downloads = get_config_item_or_set_default(\n    key='checkpoint_downloads',\n    default_value={\n        \"juggernautXL_version6Rundiffusion.safetensors\": \"https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/juggernautXL_version6Rundiffusion.safetensors\"\n    },",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "checkpoint_downloads",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "checkpoint_downloads = get_config_item_or_set_default(\n    key='checkpoint_downloads',\n    default_value={\n        \"juggernautXL_version6Rundiffusion.safetensors\": \"https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/juggernautXL_version6Rundiffusion.safetensors\"\n    },\n    validator=lambda x: isinstance(x, dict) and all(isinstance(k, str) and isinstance(v, str) for k, v in x.items())\n)\nlora_downloads = get_config_item_or_set_default(\n    key='lora_downloads',\n    default_value={",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "lora_downloads",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "lora_downloads = get_config_item_or_set_default(\n    key='lora_downloads',\n    default_value={\n        \"sd_xl_offset_example-lora_1.0.safetensors\": \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors\"\n    },\n    validator=lambda x: isinstance(x, dict) and all(isinstance(k, str) and isinstance(v, str) for k, v in x.items())\n)\nembeddings_downloads = get_config_item_or_set_default(\n    key='embeddings_downloads',\n    default_value={},",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "embeddings_downloads",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "embeddings_downloads = get_config_item_or_set_default(\n    key='embeddings_downloads',\n    default_value={},\n    validator=lambda x: isinstance(x, dict) and all(isinstance(k, str) and isinstance(v, str) for k, v in x.items())\n)\navailable_aspect_ratios = get_config_item_or_set_default(\n    key='available_aspect_ratios',\n    default_value=[\n        '704*1408', '704*1344', '768*1344', '768*1280', '832*1216', '832*1152',\n        '896*1152', '896*1088', '960*1088', '960*1024', '1024*1024', '1024*960',",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "available_aspect_ratios",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "available_aspect_ratios = get_config_item_or_set_default(\n    key='available_aspect_ratios',\n    default_value=[\n        '704*1408', '704*1344', '768*1344', '768*1280', '832*1216', '832*1152',\n        '896*1152', '896*1088', '960*1088', '960*1024', '1024*1024', '1024*960',\n        '1088*960', '1088*896', '1152*896', '1152*832', '1216*832', '1280*768',\n        '1344*768', '1344*704', '1408*704', '1472*704', '1536*640', '1600*640',\n        '1664*576', '1728*576'\n    ],\n    validator=lambda x: isinstance(x, list) and all('*' in v for v in x) and len(x) > 1",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_aspect_ratio",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_aspect_ratio = get_config_item_or_set_default(\n    key='default_aspect_ratio',\n    default_value='1152*896' if '1152*896' in available_aspect_ratios else available_aspect_ratios[0],\n    validator=lambda x: x in available_aspect_ratios\n)\ndefault_inpaint_engine_version = get_config_item_or_set_default(\n    key='default_inpaint_engine_version',\n    default_value='v2.6',\n    validator=lambda x: x in modules.flags.inpaint_engine_versions\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_inpaint_engine_version",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_inpaint_engine_version = get_config_item_or_set_default(\n    key='default_inpaint_engine_version',\n    default_value='v2.6',\n    validator=lambda x: x in modules.flags.inpaint_engine_versions\n)\ndefault_cfg_tsnr = get_config_item_or_set_default(\n    key='default_cfg_tsnr',\n    default_value=7.0,\n    validator=lambda x: isinstance(x, numbers.Number)\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_cfg_tsnr",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_cfg_tsnr = get_config_item_or_set_default(\n    key='default_cfg_tsnr',\n    default_value=7.0,\n    validator=lambda x: isinstance(x, numbers.Number)\n)\ndefault_overwrite_step = get_config_item_or_set_default(\n    key='default_overwrite_step',\n    default_value=-1,\n    validator=lambda x: isinstance(x, int)\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_overwrite_step",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_overwrite_step = get_config_item_or_set_default(\n    key='default_overwrite_step',\n    default_value=-1,\n    validator=lambda x: isinstance(x, int)\n)\ndefault_overwrite_switch = get_config_item_or_set_default(\n    key='default_overwrite_switch',\n    default_value=-1,\n    validator=lambda x: isinstance(x, int)\n)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_overwrite_switch",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_overwrite_switch = get_config_item_or_set_default(\n    key='default_overwrite_switch',\n    default_value=-1,\n    validator=lambda x: isinstance(x, int)\n)\nexample_inpaint_prompts = get_config_item_or_set_default(\n    key='example_inpaint_prompts',\n    default_value=[\n        'highly detailed face', 'detailed girl face', 'detailed man face', 'detailed hand', 'beautiful eyes'\n    ],",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "example_inpaint_prompts",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "example_inpaint_prompts = get_config_item_or_set_default(\n    key='example_inpaint_prompts',\n    default_value=[\n        'highly detailed face', 'detailed girl face', 'detailed man face', 'detailed hand', 'beautiful eyes'\n    ],\n    validator=lambda x: isinstance(x, list) and all(isinstance(v, str) for v in x)\n)\nexample_inpaint_prompts = [[x] for x in example_inpaint_prompts]\nconfig_dict[\"default_loras\"] = default_loras = default_loras[:5] + [['None', 1.0] for _ in range(5 - len(default_loras))]\npossible_preset_keys = [",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "example_inpaint_prompts",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "example_inpaint_prompts = [[x] for x in example_inpaint_prompts]\nconfig_dict[\"default_loras\"] = default_loras = default_loras[:5] + [['None', 1.0] for _ in range(5 - len(default_loras))]\npossible_preset_keys = [\n    \"default_model\",\n    \"default_refiner\",\n    \"default_refiner_switch\",\n    \"default_loras\",\n    \"default_cfg_scale\",\n    \"default_sample_sharpness\",\n    \"default_sampler\",",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "config_dict[\"default_loras\"]",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "config_dict[\"default_loras\"] = default_loras = default_loras[:5] + [['None', 1.0] for _ in range(5 - len(default_loras))]\npossible_preset_keys = [\n    \"default_model\",\n    \"default_refiner\",\n    \"default_refiner_switch\",\n    \"default_loras\",\n    \"default_cfg_scale\",\n    \"default_sample_sharpness\",\n    \"default_sampler\",\n    \"default_scheduler\",",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "possible_preset_keys",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "possible_preset_keys = [\n    \"default_model\",\n    \"default_refiner\",\n    \"default_refiner_switch\",\n    \"default_loras\",\n    \"default_cfg_scale\",\n    \"default_sample_sharpness\",\n    \"default_sampler\",\n    \"default_scheduler\",\n    \"default_performance\",",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "REWRITE_PRESET",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "REWRITE_PRESET = False\nif REWRITE_PRESET and isinstance(args_manager.args.preset, str):\n    save_path = 'presets/' + args_manager.args.preset + '.json'\n    with open(save_path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump({k: config_dict[k] for k in possible_preset_keys}, json_file, indent=4)\n    print(f'Preset saved to {save_path}. Exiting ...')\n    exit(0)\ndef add_ratio(x):\n    a, b = x.replace('*', ' ').split(' ')[:2]\n    a, b = int(a), int(b)",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "default_aspect_ratio",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "default_aspect_ratio = add_ratio(default_aspect_ratio)\navailable_aspect_ratios = [add_ratio(x) for x in available_aspect_ratios]\n# Only write config in the first launch.\nif not os.path.exists(config_path):\n    with open(config_path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump({k: config_dict[k] for k in always_save_keys}, json_file, indent=4)\n# Always write tutorials.\nwith open(config_example_path, \"w\", encoding=\"utf-8\") as json_file:\n    cpa = config_path.replace(\"\\\\\", \"\\\\\\\\\")\n    json_file.write(f'You can modify your \"{cpa}\" using the below keys, formats, and examples.\\n'",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "available_aspect_ratios",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "available_aspect_ratios = [add_ratio(x) for x in available_aspect_ratios]\n# Only write config in the first launch.\nif not os.path.exists(config_path):\n    with open(config_path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump({k: config_dict[k] for k in always_save_keys}, json_file, indent=4)\n# Always write tutorials.\nwith open(config_example_path, \"w\", encoding=\"utf-8\") as json_file:\n    cpa = config_path.replace(\"\\\\\", \"\\\\\\\\\")\n    json_file.write(f'You can modify your \"{cpa}\" using the below keys, formats, and examples.\\n'\n                    f'Do not modify this file. Modifications in this file will not take effect.\\n'",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "model_filenames",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "model_filenames = []\nlora_filenames = []\ndef get_model_filenames(folder_path, name_filter=None):\n    return get_files_from_folder(folder_path, ['.pth', '.ckpt', '.bin', '.safetensors', '.fooocus.patch'], name_filter)\ndef update_all_model_names():\n    global model_filenames, lora_filenames\n    model_filenames = get_model_filenames(path_checkpoints)\n    lora_filenames = get_model_filenames(path_loras)\n    return\ndef downloading_inpaint_models(v):",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "lora_filenames",
        "kind": 5,
        "importPath": "Fooocus.modules.config",
        "description": "Fooocus.modules.config",
        "peekOfCode": "lora_filenames = []\ndef get_model_filenames(folder_path, name_filter=None):\n    return get_files_from_folder(folder_path, ['.pth', '.ckpt', '.bin', '.safetensors', '.fooocus.patch'], name_filter)\ndef update_all_model_names():\n    global model_filenames, lora_filenames\n    model_filenames = get_model_filenames(path_checkpoints)\n    lora_filenames = get_model_filenames(path_loras)\n    return\ndef downloading_inpaint_models(v):\n    assert v in modules.flags.inpaint_engine_versions",
        "detail": "Fooocus.modules.config",
        "documentation": {}
    },
    {
        "label": "MIN_SEED",
        "kind": 5,
        "importPath": "Fooocus.modules.constants",
        "description": "Fooocus.modules.constants",
        "peekOfCode": "MIN_SEED = 0\nMAX_SEED = 2**63 - 1\nAUTH_FILENAME = 'auth.json'",
        "detail": "Fooocus.modules.constants",
        "documentation": {}
    },
    {
        "label": "MAX_SEED",
        "kind": 5,
        "importPath": "Fooocus.modules.constants",
        "description": "Fooocus.modules.constants",
        "peekOfCode": "MAX_SEED = 2**63 - 1\nAUTH_FILENAME = 'auth.json'",
        "detail": "Fooocus.modules.constants",
        "documentation": {}
    },
    {
        "label": "AUTH_FILENAME",
        "kind": 5,
        "importPath": "Fooocus.modules.constants",
        "description": "Fooocus.modules.constants",
        "peekOfCode": "AUTH_FILENAME = 'auth.json'",
        "detail": "Fooocus.modules.constants",
        "documentation": {}
    },
    {
        "label": "StableDiffusionModel",
        "kind": 6,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "class StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet\n        self.vae = vae\n        self.clip = clip\n        self.clip_vision = clip_vision\n        self.filename = filename\n        self.unet_with_lora = unet\n        self.clip_with_lora = clip\n        self.visited_loras = ''",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "VAEApprox",
        "kind": 6,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "class VAEApprox(torch.nn.Module):\n    def __init__(self):\n        super(VAEApprox, self).__init__()\n        self.conv1 = torch.nn.Conv2d(4, 8, (7, 7))\n        self.conv2 = torch.nn.Conv2d(8, 16, (5, 5))\n        self.conv3 = torch.nn.Conv2d(16, 32, (3, 3))\n        self.conv4 = torch.nn.Conv2d(32, 64, (3, 3))\n        self.conv5 = torch.nn.Conv2d(64, 32, (3, 3))\n        self.conv6 = torch.nn.Conv2d(32, 16, (3, 3))\n        self.conv7 = torch.nn.Conv2d(16, 8, (3, 3))",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "apply_freeu",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def apply_freeu(model, b1, b2, s1, s2):\n    return opFreeU.patch(model=model, b1=b1, b2=b2, s1=s1, s2=s2)[0]\n@torch.no_grad()\n@torch.inference_mode()\ndef load_controlnet(ckpt_filename):\n    return fcbh.controlnet.load_controlnet(ckpt_filename)\n@torch.no_grad()\n@torch.inference_mode()\ndef apply_controlnet(positive, negative, control_net, image, strength, start_percent, end_percent):\n    return opControlNetApplyAdvanced.apply_controlnet(positive=positive, negative=negative, control_net=control_net,",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "load_controlnet",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def load_controlnet(ckpt_filename):\n    return fcbh.controlnet.load_controlnet(ckpt_filename)\n@torch.no_grad()\n@torch.inference_mode()\ndef apply_controlnet(positive, negative, control_net, image, strength, start_percent, end_percent):\n    return opControlNetApplyAdvanced.apply_controlnet(positive=positive, negative=negative, control_net=control_net,\n        image=image, strength=strength, start_percent=start_percent, end_percent=end_percent)\n@torch.no_grad()\n@torch.inference_mode()\ndef load_model(ckpt_filename):",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "apply_controlnet",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def apply_controlnet(positive, negative, control_net, image, strength, start_percent, end_percent):\n    return opControlNetApplyAdvanced.apply_controlnet(positive=positive, negative=negative, control_net=control_net,\n        image=image, strength=strength, start_percent=start_percent, end_percent=end_percent)\n@torch.no_grad()\n@torch.inference_mode()\ndef load_model(ckpt_filename):\n    unet, clip, vae, clip_vision = load_checkpoint_guess_config(ckpt_filename, embedding_directory=path_embeddings)\n    unet.model_options['sampler_cfg_function'] = patched_sampler_cfg_function\n    return StableDiffusionModel(unet=unet, clip=clip, vae=vae, clip_vision=clip_vision, filename=ckpt_filename)\n@torch.no_grad()",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def load_model(ckpt_filename):\n    unet, clip, vae, clip_vision = load_checkpoint_guess_config(ckpt_filename, embedding_directory=path_embeddings)\n    unet.model_options['sampler_cfg_function'] = patched_sampler_cfg_function\n    return StableDiffusionModel(unet=unet, clip=clip, vae=vae, clip_vision=clip_vision, filename=ckpt_filename)\n@torch.no_grad()\n@torch.inference_mode()\ndef generate_empty_latent(width=1024, height=1024, batch_size=1):\n    return opEmptyLatentImage.generate(width=width, height=height, batch_size=batch_size)[0]\n@torch.no_grad()\n@torch.inference_mode()",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "generate_empty_latent",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def generate_empty_latent(width=1024, height=1024, batch_size=1):\n    return opEmptyLatentImage.generate(width=width, height=height, batch_size=batch_size)[0]\n@torch.no_grad()\n@torch.inference_mode()\ndef decode_vae(vae, latent_image, tiled=False):\n    if tiled:\n        return opVAEDecodeTiled.decode(samples=latent_image, vae=vae, tile_size=512)[0]\n    else:\n        return opVAEDecode.decode(samples=latent_image, vae=vae)[0]\n@torch.no_grad()",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "decode_vae",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def decode_vae(vae, latent_image, tiled=False):\n    if tiled:\n        return opVAEDecodeTiled.decode(samples=latent_image, vae=vae, tile_size=512)[0]\n    else:\n        return opVAEDecode.decode(samples=latent_image, vae=vae)[0]\n@torch.no_grad()\n@torch.inference_mode()\ndef encode_vae(vae, pixels, tiled=False):\n    if tiled:\n        return opVAEEncodeTiled.encode(pixels=pixels, vae=vae, tile_size=512)[0]",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "encode_vae",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def encode_vae(vae, pixels, tiled=False):\n    if tiled:\n        return opVAEEncodeTiled.encode(pixels=pixels, vae=vae, tile_size=512)[0]\n    else:\n        return opVAEEncode.encode(pixels=pixels, vae=vae)[0]\n@torch.no_grad()\n@torch.inference_mode()\ndef encode_vae_inpaint(vae, pixels, mask):\n    assert mask.ndim == 3 and pixels.ndim == 4\n    assert mask.shape[-1] == pixels.shape[-2]",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "encode_vae_inpaint",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def encode_vae_inpaint(vae, pixels, mask):\n    assert mask.ndim == 3 and pixels.ndim == 4\n    assert mask.shape[-1] == pixels.shape[-2]\n    assert mask.shape[-2] == pixels.shape[-3]\n    w = mask.round()[..., None]\n    pixels = pixels * (1 - w) + 0.5 * w\n    latent = vae.encode(pixels)\n    B, C, H, W = latent.shape\n    latent_mask = mask[:, None, :, :]\n    latent_mask = torch.nn.functional.interpolate(latent_mask, size=(H * 8, W * 8), mode=\"bilinear\").round()",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "get_previewer",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def get_previewer(model):\n    global VAE_approx_models\n    from modules.config import path_vae_approx\n    is_sdxl = isinstance(model.model.latent_format, fcbh.latent_formats.SDXL)\n    vae_approx_filename = os.path.join(path_vae_approx, 'xlvaeapp.pth' if is_sdxl else 'vaeapp_sd15.pth')\n    if vae_approx_filename in VAE_approx_models:\n        VAE_approx_model = VAE_approx_models[vae_approx_filename]\n    else:\n        sd = torch.load(vae_approx_filename, map_location='cpu')\n        VAE_approx_model = VAEApprox()",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "ksampler",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def ksampler(model, positive, negative, latent, seed=None, steps=30, cfg=7.0, sampler_name='dpmpp_2m_sde_gpu',\n             scheduler='karras', denoise=1.0, disable_noise=False, start_step=None, last_step=None,\n             force_full_denoise=False, callback_function=None, refiner=None, refiner_switch=-1,\n             previewer_start=None, previewer_end=None, sigmas=None, noise_mean=None):\n    if sigmas is not None:\n        sigmas = sigmas.clone().to(fcbh.model_management.get_torch_device())\n    latent_image = latent[\"samples\"]\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "pytorch_to_numpy",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def pytorch_to_numpy(x):\n    return [np.clip(255. * y.cpu().numpy(), 0, 255).astype(np.uint8) for y in x]\n@torch.no_grad()\n@torch.inference_mode()\ndef numpy_to_pytorch(x):\n    y = x.astype(np.float32) / 255.0\n    y = y[None]\n    y = np.ascontiguousarray(y.copy())\n    y = torch.from_numpy(y).float()\n    return y",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "numpy_to_pytorch",
        "kind": 2,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "def numpy_to_pytorch(x):\n    y = x.astype(np.float32) / 255.0\n    y = y[None]\n    y = np.ascontiguousarray(y.copy())\n    y = torch.from_numpy(y).float()\n    return y",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opEmptyLatentImage",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opEmptyLatentImage = EmptyLatentImage()\nopVAEDecode = VAEDecode()\nopVAEEncode = VAEEncode()\nopVAEDecodeTiled = VAEDecodeTiled()\nopVAEEncodeTiled = VAEEncodeTiled()\nopControlNetApplyAdvanced = ControlNetApplyAdvanced()\nopFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opVAEDecode",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opVAEDecode = VAEDecode()\nopVAEEncode = VAEEncode()\nopVAEDecodeTiled = VAEDecodeTiled()\nopVAEEncodeTiled = VAEEncodeTiled()\nopControlNetApplyAdvanced = ControlNetApplyAdvanced()\nopFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opVAEEncode",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opVAEEncode = VAEEncode()\nopVAEDecodeTiled = VAEDecodeTiled()\nopVAEEncodeTiled = VAEEncodeTiled()\nopControlNetApplyAdvanced = ControlNetApplyAdvanced()\nopFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet\n        self.vae = vae",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opVAEDecodeTiled",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opVAEDecodeTiled = VAEDecodeTiled()\nopVAEEncodeTiled = VAEEncodeTiled()\nopControlNetApplyAdvanced = ControlNetApplyAdvanced()\nopFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet\n        self.vae = vae\n        self.clip = clip",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opVAEEncodeTiled",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opVAEEncodeTiled = VAEEncodeTiled()\nopControlNetApplyAdvanced = ControlNetApplyAdvanced()\nopFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet\n        self.vae = vae\n        self.clip = clip\n        self.clip_vision = clip_vision",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opControlNetApplyAdvanced",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opControlNetApplyAdvanced = ControlNetApplyAdvanced()\nopFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet\n        self.vae = vae\n        self.clip = clip\n        self.clip_vision = clip_vision\n        self.filename = filename",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opFreeU",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opFreeU = FreeU_V2()\nopModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet\n        self.vae = vae\n        self.clip = clip\n        self.clip_vision = clip_vision\n        self.filename = filename\n        self.unet_with_lora = unet",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "opModelSamplingDiscrete",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "opModelSamplingDiscrete = ModelSamplingDiscrete()\nclass StableDiffusionModel:\n    def __init__(self, unet=None, vae=None, clip=None, clip_vision=None, filename=None):\n        self.unet = unet\n        self.vae = vae\n        self.clip = clip\n        self.clip_vision = clip_vision\n        self.filename = filename\n        self.unet_with_lora = unet\n        self.clip_with_lora = clip",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "VAE_approx_models",
        "kind": 5,
        "importPath": "Fooocus.modules.core",
        "description": "Fooocus.modules.core",
        "peekOfCode": "VAE_approx_models = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef get_previewer(model):\n    global VAE_approx_models\n    from modules.config import path_vae_approx\n    is_sdxl = isinstance(model.model.latent_format, fcbh.latent_formats.SDXL)\n    vae_approx_filename = os.path.join(path_vae_approx, 'xlvaeapp.pth' if is_sdxl else 'vaeapp_sd15.pth')\n    if vae_approx_filename in VAE_approx_models:\n        VAE_approx_model = VAE_approx_models[vae_approx_filename]",
        "detail": "Fooocus.modules.core",
        "documentation": {}
    },
    {
        "label": "refresh_controlnets",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def refresh_controlnets(model_paths):\n    global loaded_ControlNets\n    cache = {}\n    for p in model_paths:\n        if p is not None:\n            if p in loaded_ControlNets:\n                cache[p] = loaded_ControlNets[p]\n            else:\n                cache[p] = core.load_controlnet(p)\n    loaded_ControlNets = cache",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "assert_model_integrity",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def assert_model_integrity():\n    error_message = None\n    if not isinstance(model_base.unet_with_lora.model, SDXL):\n        error_message = 'You have selected base model other than SDXL. This is not supported yet.'\n    if error_message is not None:\n        raise NotImplementedError(error_message)\n    return True\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_base_model(name):",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "refresh_base_model",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def refresh_base_model(name):\n    global model_base\n    filename = os.path.abspath(os.path.realpath(os.path.join(modules.config.path_checkpoints, name)))\n    if model_base.filename == filename:\n        return\n    model_base = core.StableDiffusionModel()\n    model_base = core.load_model(filename)\n    print(f'Base model loaded: {model_base.filename}')\n    return\n@torch.no_grad()",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "refresh_refiner_model",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def refresh_refiner_model(name):\n    global model_refiner\n    filename = os.path.abspath(os.path.realpath(os.path.join(modules.config.path_checkpoints, name)))\n    if model_refiner.filename == filename:\n        return\n    model_refiner = core.StableDiffusionModel()\n    if name == 'None':\n        print(f'Refiner unloaded.')\n        return\n    model_refiner = core.load_model(filename)",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "synthesize_refiner_model",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def synthesize_refiner_model():\n    global model_base, model_refiner\n    print('Synthetic Refiner Activated')\n    model_refiner = core.StableDiffusionModel(\n        unet=model_base.unet,\n        vae=model_base.vae,\n        clip=model_base.clip,\n        clip_vision=model_base.clip_vision,\n        filename=model_base.filename\n    )",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "refresh_loras",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def refresh_loras(loras, base_model_additional_loras=None):\n    global model_base, model_refiner\n    if not isinstance(base_model_additional_loras, list):\n        base_model_additional_loras = []\n    model_base.refresh_loras(loras + base_model_additional_loras)\n    model_refiner.refresh_loras(loras)\n    return\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_encode_single(clip, text, verbose=False):",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "clip_encode_single",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def clip_encode_single(clip, text, verbose=False):\n    cached = clip.fcs_cond_cache.get(text, None)\n    if cached is not None:\n        if verbose:\n            print(f'[CLIP Cached] {text}')\n        return cached\n    tokens = clip.tokenize(text)\n    result = clip.encode_from_tokens(tokens, return_pooled=True)\n    clip.fcs_cond_cache[text] = result\n    if verbose:",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "clone_cond",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def clone_cond(conds):\n    results = []\n    for c, p in conds:\n        p = p[\"pooled_output\"]\n        if isinstance(c, torch.Tensor):\n            c = c.clone()\n        if isinstance(p, torch.Tensor):\n            p = p.clone()\n        results.append([c, {\"pooled_output\": p}])\n    return results",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "clip_encode",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def clip_encode(texts, pool_top_k=1):\n    global final_clip\n    if final_clip is None:\n        return None\n    if not isinstance(texts, list):\n        return None\n    if len(texts) == 0:\n        return None\n    cond_list = []\n    pooled_acc = 0",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "clear_all_caches",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def clear_all_caches():\n    final_clip.fcs_cond_cache = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef prepare_text_encoder(async_call=True):\n    if async_call:\n        # TODO: make sure that this is always called in an async way so that users cannot feel it.\n        pass\n    assert_model_integrity()\n    fcbh.model_management.load_models_gpu([final_clip.patcher, final_expansion.patcher])",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "prepare_text_encoder",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def prepare_text_encoder(async_call=True):\n    if async_call:\n        # TODO: make sure that this is always called in an async way so that users cannot feel it.\n        pass\n    assert_model_integrity()\n    fcbh.model_management.load_models_gpu([final_clip.patcher, final_expansion.patcher])\n    return\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_everything(refiner_model_name, base_model_name, loras,",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "refresh_everything",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def refresh_everything(refiner_model_name, base_model_name, loras,\n                       base_model_additional_loras=None, use_synthetic_refiner=False):\n    global final_unet, final_clip, final_vae, final_refiner_unet, final_refiner_vae, final_expansion\n    final_unet = None\n    final_clip = None\n    final_vae = None\n    final_refiner_unet = None\n    final_refiner_vae = None\n    if use_synthetic_refiner and refiner_model_name == 'None':\n        print('Synthetic Refiner Activated')",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "vae_parse",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def vae_parse(latent):\n    if final_refiner_vae is None:\n        return latent\n    result = vae_interpose.parse(latent[\"samples\"])\n    return {'samples': result}\n@torch.no_grad()\n@torch.inference_mode()\ndef calculate_sigmas_all(sampler, model, scheduler, steps):\n    from fcbh.samplers import calculate_sigmas_scheduler\n    discard_penultimate_sigma = False",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "calculate_sigmas_all",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def calculate_sigmas_all(sampler, model, scheduler, steps):\n    from fcbh.samplers import calculate_sigmas_scheduler\n    discard_penultimate_sigma = False\n    if sampler in ['dpm_2', 'dpm_2_ancestral']:\n        steps += 1\n        discard_penultimate_sigma = True\n    sigmas = calculate_sigmas_scheduler(model, scheduler, steps)\n    if discard_penultimate_sigma:\n        sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])\n    return sigmas",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "calculate_sigmas",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def calculate_sigmas(sampler, model, scheduler, steps, denoise):\n    if denoise is None or denoise > 0.9999:\n        sigmas = calculate_sigmas_all(sampler, model, scheduler, steps)\n    else:\n        new_steps = int(steps / denoise)\n        sigmas = calculate_sigmas_all(sampler, model, scheduler, new_steps)\n        sigmas = sigmas[-(steps + 1):]\n    return sigmas\n@torch.no_grad()\n@torch.inference_mode()",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "get_candidate_vae",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def get_candidate_vae(steps, switch, denoise=1.0, refiner_swap_method='joint'):\n    assert refiner_swap_method in ['joint', 'separate', 'vae']\n    if final_refiner_vae is not None and final_refiner_unet is not None:\n        if denoise > 0.9:\n            return final_vae, final_refiner_vae\n        else:\n            if denoise > (float(steps - switch) / float(steps)) ** 0.834:  # karras 0.834\n                return final_vae, None\n            else:\n                return final_refiner_vae, None",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "process_diffusion",
        "kind": 2,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "def process_diffusion(positive_cond, negative_cond, steps, switch, width, height, image_seed, callback, sampler_name, scheduler_name, latent=None, denoise=1.0, tiled=False, cfg_scale=7.0, refiner_swap_method='joint'):\n    target_unet, target_vae, target_refiner_unet, target_refiner_vae, target_clip \\\n        = final_unet, final_vae, final_refiner_unet, final_refiner_vae, final_clip\n    assert refiner_swap_method in ['joint', 'separate', 'vae']\n    if final_refiner_vae is not None and final_refiner_unet is not None:\n        # Refiner Use Different VAE (then it is SD15)\n        if denoise > 0.9:\n            refiner_swap_method = 'vae'\n        else:\n            refiner_swap_method = 'joint'",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "model_base",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "model_base = core.StableDiffusionModel()\nmodel_refiner = core.StableDiffusionModel()\nfinal_expansion = None\nfinal_unet = None\nfinal_clip = None\nfinal_vae = None\nfinal_refiner_unet = None\nfinal_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "model_refiner",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "model_refiner = core.StableDiffusionModel()\nfinal_expansion = None\nfinal_unet = None\nfinal_clip = None\nfinal_vae = None\nfinal_refiner_unet = None\nfinal_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "final_expansion",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "final_expansion = None\nfinal_unet = None\nfinal_clip = None\nfinal_vae = None\nfinal_refiner_unet = None\nfinal_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "final_unet",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "final_unet = None\nfinal_clip = None\nfinal_vae = None\nfinal_refiner_unet = None\nfinal_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):\n    global loaded_ControlNets",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "final_clip",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "final_clip = None\nfinal_vae = None\nfinal_refiner_unet = None\nfinal_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):\n    global loaded_ControlNets\n    cache = {}",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "final_vae",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "final_vae = None\nfinal_refiner_unet = None\nfinal_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):\n    global loaded_ControlNets\n    cache = {}\n    for p in model_paths:",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "final_refiner_unet",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "final_refiner_unet = None\nfinal_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):\n    global loaded_ControlNets\n    cache = {}\n    for p in model_paths:\n        if p is not None:",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "final_refiner_vae",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "final_refiner_vae = None\nloaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):\n    global loaded_ControlNets\n    cache = {}\n    for p in model_paths:\n        if p is not None:\n            if p in loaded_ControlNets:",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "loaded_ControlNets",
        "kind": 5,
        "importPath": "Fooocus.modules.default_pipeline",
        "description": "Fooocus.modules.default_pipeline",
        "peekOfCode": "loaded_ControlNets = {}\n@torch.no_grad()\n@torch.inference_mode()\ndef refresh_controlnets(model_paths):\n    global loaded_ControlNets\n    cache = {}\n    for p in model_paths:\n        if p is not None:\n            if p in loaded_ControlNets:\n                cache[p] = loaded_ControlNets[p]",
        "detail": "Fooocus.modules.default_pipeline",
        "documentation": {}
    },
    {
        "label": "FooocusExpansion",
        "kind": 6,
        "importPath": "Fooocus.modules.expansion",
        "description": "Fooocus.modules.expansion",
        "peekOfCode": "class FooocusExpansion:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(path_fooocus_expansion)\n        positive_words = open(os.path.join(path_fooocus_expansion, 'positive.txt'),\n                              encoding='utf-8').read().splitlines()\n        positive_words = ['' + x.lower() for x in positive_words if x != '']\n        self.logits_bias = torch.zeros((1, len(self.tokenizer.vocab)), dtype=torch.float32) + neg_inf\n        debug_list = []\n        for k, v in self.tokenizer.vocab.items():\n            if k in positive_words:",
        "detail": "Fooocus.modules.expansion",
        "documentation": {}
    },
    {
        "label": "safe_str",
        "kind": 2,
        "importPath": "Fooocus.modules.expansion",
        "description": "Fooocus.modules.expansion",
        "peekOfCode": "def safe_str(x):\n    x = str(x)\n    for _ in range(16):\n        x = x.replace('  ', ' ')\n    return x.strip(\",. \\r\\n\")\ndef remove_pattern(x, pattern):\n    for p in pattern:\n        x = x.replace(p, '')\n    return x\nclass FooocusExpansion:",
        "detail": "Fooocus.modules.expansion",
        "documentation": {}
    },
    {
        "label": "remove_pattern",
        "kind": 2,
        "importPath": "Fooocus.modules.expansion",
        "description": "Fooocus.modules.expansion",
        "peekOfCode": "def remove_pattern(x, pattern):\n    for p in pattern:\n        x = x.replace(p, '')\n    return x\nclass FooocusExpansion:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(path_fooocus_expansion)\n        positive_words = open(os.path.join(path_fooocus_expansion, 'positive.txt'),\n                              encoding='utf-8').read().splitlines()\n        positive_words = ['' + x.lower() for x in positive_words if x != '']",
        "detail": "Fooocus.modules.expansion",
        "documentation": {}
    },
    {
        "label": "SEED_LIMIT_NUMPY",
        "kind": 5,
        "importPath": "Fooocus.modules.expansion",
        "description": "Fooocus.modules.expansion",
        "peekOfCode": "SEED_LIMIT_NUMPY = 2**32\nneg_inf = - 8192.0\ndef safe_str(x):\n    x = str(x)\n    for _ in range(16):\n        x = x.replace('  ', ' ')\n    return x.strip(\",. \\r\\n\")\ndef remove_pattern(x, pattern):\n    for p in pattern:\n        x = x.replace(p, '')",
        "detail": "Fooocus.modules.expansion",
        "documentation": {}
    },
    {
        "label": "neg_inf",
        "kind": 5,
        "importPath": "Fooocus.modules.expansion",
        "description": "Fooocus.modules.expansion",
        "peekOfCode": "neg_inf = - 8192.0\ndef safe_str(x):\n    x = str(x)\n    for _ in range(16):\n        x = x.replace('  ', ' ')\n    return x.strip(\",. \\r\\n\")\ndef remove_pattern(x, pattern):\n    for p in pattern:\n        x = x.replace(p, '')\n    return x",
        "detail": "Fooocus.modules.expansion",
        "documentation": {}
    },
    {
        "label": "disabled",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "disabled = 'Disabled'\nenabled = 'Enabled'\nsubtle_variation = 'Vary (Subtle)'\nstrong_variation = 'Vary (Strong)'\nupscale_15 = 'Upscale (1.5x)'\nupscale_2 = 'Upscale (2x)'\nupscale_fast = 'Upscale (Fast 2x)'\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "enabled",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "enabled = 'Enabled'\nsubtle_variation = 'Vary (Subtle)'\nstrong_variation = 'Vary (Strong)'\nupscale_15 = 'Upscale (1.5x)'\nupscale_2 = 'Upscale (2x)'\nupscale_fast = 'Upscale (Fast 2x)'\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "subtle_variation",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "subtle_variation = 'Vary (Subtle)'\nstrong_variation = 'Vary (Strong)'\nupscale_15 = 'Upscale (1.5x)'\nupscale_2 = 'Upscale (2x)'\nupscale_fast = 'Upscale (Fast 2x)'\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "strong_variation",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "strong_variation = 'Vary (Strong)'\nupscale_15 = 'Upscale (1.5x)'\nupscale_2 = 'Upscale (2x)'\nupscale_fast = 'Upscale (Fast 2x)'\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "upscale_15",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "upscale_15 = 'Upscale (1.5x)'\nupscale_2 = 'Upscale (2x)'\nupscale_fast = 'Upscale (Fast 2x)'\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\", \"lcm\"]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "upscale_2",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "upscale_2 = 'Upscale (2x)'\nupscale_fast = 'Upscale (Fast 2x)'\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\", \"lcm\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "upscale_fast",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "upscale_fast = 'Upscale (Fast 2x)'\nuov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\", \"lcm\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\nsampler_list = SAMPLER_NAMES",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "uov_list",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "uov_list = [\n    disabled, subtle_variation, strong_variation, upscale_15, upscale_2, upscale_fast\n]\nKSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\", \"lcm\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\nsampler_list = SAMPLER_NAMES\nscheduler_list = SCHEDULER_NAMES",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "KSAMPLER_NAMES",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "KSAMPLER_NAMES = [\"euler\", \"euler_ancestral\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                  \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\"]\nSCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\", \"lcm\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\nsampler_list = SAMPLER_NAMES\nscheduler_list = SCHEDULER_NAMES\ncn_ip = \"ImagePrompt\"\ncn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "SCHEDULER_NAMES",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "SCHEDULER_NAMES = [\"normal\", \"karras\", \"exponential\", \"sgm_uniform\", \"simple\", \"ddim_uniform\", \"lcm\"]\nSAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\nsampler_list = SAMPLER_NAMES\nscheduler_list = SCHEDULER_NAMES\ncn_ip = \"ImagePrompt\"\ncn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "SAMPLER_NAMES",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "SAMPLER_NAMES = KSAMPLER_NAMES + [\"ddim\", \"uni_pc\", \"uni_pc_bh2\"]\nsampler_list = SAMPLER_NAMES\nscheduler_list = SCHEDULER_NAMES\ncn_ip = \"ImagePrompt\"\ncn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "sampler_list",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "sampler_list = SAMPLER_NAMES\nscheduler_list = SCHEDULER_NAMES\ncn_ip = \"ImagePrompt\"\ncn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "scheduler_list",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "scheduler_list = SCHEDULER_NAMES\ncn_ip = \"ImagePrompt\"\ncn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "cn_ip",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "cn_ip = \"ImagePrompt\"\ncn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "cn_ip_face",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "cn_ip_face = \"FaceSwap\"\ncn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\nperformance_selections = ['Speed', 'Quality', 'Extreme Speed']",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "cn_canny",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "cn_canny = \"PyraCanny\"\ncn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\nperformance_selections = ['Speed', 'Quality', 'Extreme Speed']\ninpaint_option_default = 'Inpaint or Outpaint (default)'",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "cn_cpds",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "cn_cpds = \"CPDS\"\nip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\nperformance_selections = ['Speed', 'Quality', 'Extreme Speed']\ninpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "ip_list",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "ip_list = [cn_ip, cn_canny, cn_cpds, cn_ip_face]\ndefault_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\nperformance_selections = ['Speed', 'Quality', 'Extreme Speed']\ninpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "default_ip",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "default_ip = cn_ip\ndefault_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\nperformance_selections = ['Speed', 'Quality', 'Extreme Speed']\ninpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "default_parameters",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "default_parameters = {\n    cn_ip: (0.5, 0.6), cn_ip_face: (0.9, 0.75), cn_canny: (0.5, 1.0), cn_cpds: (0.5, 1.0)\n}  # stop, weight\ninpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\nperformance_selections = ['Speed', 'Quality', 'Extreme Speed']\ninpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "inpaint_engine_versions",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "inpaint_engine_versions = ['None', 'v1', 'v2.5', 'v2.6']\nperformance_selections = ['Speed', 'Quality', 'Extreme Speed']\ninpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "performance_selections",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "performance_selections = ['Speed', 'Quality', 'Extreme Speed']\ninpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "inpaint_option_default",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "inpaint_option_default = 'Inpaint or Outpaint (default)'\ninpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "inpaint_option_detail",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "inpaint_option_detail = 'Improve Detail (face, hand, eyes, etc.)'\ninpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "inpaint_option_modify",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "inpaint_option_modify = 'Modify Content (add objects, change background, etc.)'\ninpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "inpaint_options",
        "kind": 5,
        "importPath": "Fooocus.modules.flags",
        "description": "Fooocus.modules.flags",
        "peekOfCode": "inpaint_options = [inpaint_option_default, inpaint_option_detail, inpaint_option_modify]",
        "detail": "Fooocus.modules.flags",
        "documentation": {}
    },
    {
        "label": "Image",
        "kind": 6,
        "importPath": "Fooocus.modules.gradio_hijack",
        "description": "Fooocus.modules.gradio_hijack",
        "peekOfCode": "class Image(\n    Editable,\n    Clearable,\n    Changeable,\n    Streamable,\n    Selectable,\n    Uploadable,\n    IOComponent,\n    ImgSerializable,\n    TokenInterpretable,",
        "detail": "Fooocus.modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "blk_ini",
        "kind": 2,
        "importPath": "Fooocus.modules.gradio_hijack",
        "description": "Fooocus.modules.gradio_hijack",
        "peekOfCode": "def blk_ini(self, *args, **kwargs):\n    all_components.append(self)\n    return Block.original_init(self, *args, **kwargs)\nBlock.__init__ = blk_ini\ngradio.routes.asyncio = importlib.reload(gradio.routes.asyncio)\nif not hasattr(gradio.routes.asyncio, 'original_wait_for'):\n    gradio.routes.asyncio.original_wait_for = gradio.routes.asyncio.wait_for\ndef patched_wait_for(fut, timeout):\n    del timeout\n    return gradio.routes.asyncio.original_wait_for(fut, timeout=65535)",
        "detail": "Fooocus.modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "patched_wait_for",
        "kind": 2,
        "importPath": "Fooocus.modules.gradio_hijack",
        "description": "Fooocus.modules.gradio_hijack",
        "peekOfCode": "def patched_wait_for(fut, timeout):\n    del timeout\n    return gradio.routes.asyncio.original_wait_for(fut, timeout=65535)\ngradio.routes.asyncio.wait_for = patched_wait_for",
        "detail": "Fooocus.modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "all_components",
        "kind": 5,
        "importPath": "Fooocus.modules.gradio_hijack",
        "description": "Fooocus.modules.gradio_hijack",
        "peekOfCode": "all_components = []\nif not hasattr(Block, 'original__init__'):\n    Block.original_init = Block.__init__\ndef blk_ini(self, *args, **kwargs):\n    all_components.append(self)\n    return Block.original_init(self, *args, **kwargs)\nBlock.__init__ = blk_ini\ngradio.routes.asyncio = importlib.reload(gradio.routes.asyncio)\nif not hasattr(gradio.routes.asyncio, 'original_wait_for'):\n    gradio.routes.asyncio.original_wait_for = gradio.routes.asyncio.wait_for",
        "detail": "Fooocus.modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "Block.__init__",
        "kind": 5,
        "importPath": "Fooocus.modules.gradio_hijack",
        "description": "Fooocus.modules.gradio_hijack",
        "peekOfCode": "Block.__init__ = blk_ini\ngradio.routes.asyncio = importlib.reload(gradio.routes.asyncio)\nif not hasattr(gradio.routes.asyncio, 'original_wait_for'):\n    gradio.routes.asyncio.original_wait_for = gradio.routes.asyncio.wait_for\ndef patched_wait_for(fut, timeout):\n    del timeout\n    return gradio.routes.asyncio.original_wait_for(fut, timeout=65535)\ngradio.routes.asyncio.wait_for = patched_wait_for",
        "detail": "Fooocus.modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "gradio.routes.asyncio",
        "kind": 5,
        "importPath": "Fooocus.modules.gradio_hijack",
        "description": "Fooocus.modules.gradio_hijack",
        "peekOfCode": "gradio.routes.asyncio = importlib.reload(gradio.routes.asyncio)\nif not hasattr(gradio.routes.asyncio, 'original_wait_for'):\n    gradio.routes.asyncio.original_wait_for = gradio.routes.asyncio.wait_for\ndef patched_wait_for(fut, timeout):\n    del timeout\n    return gradio.routes.asyncio.original_wait_for(fut, timeout=65535)\ngradio.routes.asyncio.wait_for = patched_wait_for",
        "detail": "Fooocus.modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "gradio.routes.asyncio.wait_for",
        "kind": 5,
        "importPath": "Fooocus.modules.gradio_hijack",
        "description": "Fooocus.modules.gradio_hijack",
        "peekOfCode": "gradio.routes.asyncio.wait_for = patched_wait_for",
        "detail": "Fooocus.modules.gradio_hijack",
        "documentation": {}
    },
    {
        "label": "make_progress_html",
        "kind": 2,
        "importPath": "Fooocus.modules.html",
        "description": "Fooocus.modules.html",
        "peekOfCode": "def make_progress_html(number, text):\n    return progress_html.replace('*number*', str(number)).replace('*text*', text)",
        "detail": "Fooocus.modules.html",
        "documentation": {}
    },
    {
        "label": "css",
        "kind": 5,
        "importPath": "Fooocus.modules.html",
        "description": "Fooocus.modules.html",
        "peekOfCode": "css = '''\n.loader-container {\n  display: flex; /* Use flex to align items horizontally */\n  align-items: center; /* Center items vertically within the container */\n  white-space: nowrap; /* Prevent line breaks within the container */\n}\n.loader {\n  border: 8px solid #f3f3f3; /* Light grey */\n  border-top: 8px solid #3498db; /* Blue */\n  border-radius: 50%;",
        "detail": "Fooocus.modules.html",
        "documentation": {}
    },
    {
        "label": "progress_html",
        "kind": 5,
        "importPath": "Fooocus.modules.html",
        "description": "Fooocus.modules.html",
        "peekOfCode": "progress_html = '''\n<div class=\"loader-container\">\n  <div class=\"loader\"></div>\n  <div class=\"progress-container\">\n    <progress value=\"*number*\" max=\"100\"></progress>\n  </div>\n  <span>*text*</span>\n</div>\n'''\ndef make_progress_html(number, text):",
        "detail": "Fooocus.modules.html",
        "documentation": {}
    },
    {
        "label": "InpaintHead",
        "kind": 6,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "class InpaintHead(torch.nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.head = torch.nn.Parameter(torch.empty(size=(320, 5, 3, 3), device='cpu'))\n    def __call__(self, x):\n        x = torch.nn.functional.pad(x, (1, 1, 1, 1), \"replicate\")\n        return torch.nn.functional.conv2d(input=x, weight=self.head)\ncurrent_task = None\ndef box_blur(x, k):\n    x = Image.fromarray(x)",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "InpaintWorker",
        "kind": 6,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "class InpaintWorker:\n    def __init__(self, image, mask, use_fill=True, k=0.618):\n        a, b, c, d = compute_initial_abcd(mask > 0)\n        a, b, c, d = solve_abcd(mask, a, b, c, d, k=k)\n        # interested area\n        self.interested_area = (a, b, c, d)\n        self.interested_mask = mask[a:b, c:d]\n        self.interested_image = image[a:b, c:d]\n        # super resolution\n        if get_image_shape_ceil(self.interested_image) < 1024:",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "box_blur",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def box_blur(x, k):\n    x = Image.fromarray(x)\n    x = x.filter(ImageFilter.BoxBlur(k))\n    return np.array(x)\ndef max33(x):\n    x = Image.fromarray(x)\n    x = x.filter(ImageFilter.MaxFilter(3))\n    return np.array(x)\ndef morphological_open(x):\n    x_int32 = np.zeros_like(x).astype(np.int32)",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "max33",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def max33(x):\n    x = Image.fromarray(x)\n    x = x.filter(ImageFilter.MaxFilter(3))\n    return np.array(x)\ndef morphological_open(x):\n    x_int32 = np.zeros_like(x).astype(np.int32)\n    x_int32[x > 127] = 256\n    for _ in range(32):\n        maxed = max33(x_int32) - 8\n        x_int32 = np.maximum(maxed, x_int32)",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "morphological_open",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def morphological_open(x):\n    x_int32 = np.zeros_like(x).astype(np.int32)\n    x_int32[x > 127] = 256\n    for _ in range(32):\n        maxed = max33(x_int32) - 8\n        x_int32 = np.maximum(maxed, x_int32)\n    return x_int32.clip(0, 255).astype(np.uint8)\ndef up255(x, t=0):\n    y = np.zeros_like(x).astype(np.uint8)\n    y[x > t] = 255",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "up255",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def up255(x, t=0):\n    y = np.zeros_like(x).astype(np.uint8)\n    y[x > t] = 255\n    return y\ndef imsave(x, path):\n    x = Image.fromarray(x)\n    x.save(path)\ndef regulate_abcd(x, a, b, c, d):\n    H, W = x.shape[:2]\n    if a < 0:",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "imsave",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def imsave(x, path):\n    x = Image.fromarray(x)\n    x.save(path)\ndef regulate_abcd(x, a, b, c, d):\n    H, W = x.shape[:2]\n    if a < 0:\n        a = 0\n    if a > H:\n        a = H\n    if b < 0:",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "regulate_abcd",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def regulate_abcd(x, a, b, c, d):\n    H, W = x.shape[:2]\n    if a < 0:\n        a = 0\n    if a > H:\n        a = H\n    if b < 0:\n        b = 0\n    if b > H:\n        b = H",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "compute_initial_abcd",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def compute_initial_abcd(x):\n    indices = np.where(x)\n    a = np.min(indices[0])\n    b = np.max(indices[0])\n    c = np.min(indices[1])\n    d = np.max(indices[1])\n    abp = (b + a) // 2\n    abm = (b - a) // 2\n    cdp = (d + c) // 2\n    cdm = (d - c) // 2",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "solve_abcd",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def solve_abcd(x, a, b, c, d, k):\n    k = float(k)\n    assert 0.0 <= k <= 1.0\n    H, W = x.shape[:2]\n    if k == 1.0:\n        return 0, H, 0, W\n    while True:\n        if b - a >= H * k and d - c >= W * k:\n            break\n        add_h = (b - a) < (d - c)",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "fooocus_fill",
        "kind": 2,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "def fooocus_fill(image, mask):\n    current_image = image.copy()\n    raw_image = image.copy()\n    area = np.where(mask < 127)\n    store = raw_image[area]\n    for k, repeats in [(512, 2), (256, 2), (128, 4), (64, 4), (33, 8), (15, 8), (5, 16), (3, 16)]:\n        for _ in range(repeats):\n            current_image = box_blur(current_image, k)\n            current_image[area] = store\n    return current_image",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "inpaint_head_model",
        "kind": 5,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "inpaint_head_model = None\nclass InpaintHead(torch.nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.head = torch.nn.Parameter(torch.empty(size=(320, 5, 3, 3), device='cpu'))\n    def __call__(self, x):\n        x = torch.nn.functional.pad(x, (1, 1, 1, 1), \"replicate\")\n        return torch.nn.functional.conv2d(input=x, weight=self.head)\ncurrent_task = None\ndef box_blur(x, k):",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "current_task",
        "kind": 5,
        "importPath": "Fooocus.modules.inpaint_worker",
        "description": "Fooocus.modules.inpaint_worker",
        "peekOfCode": "current_task = None\ndef box_blur(x, k):\n    x = Image.fromarray(x)\n    x = x.filter(ImageFilter.BoxBlur(k))\n    return np.array(x)\ndef max33(x):\n    x = Image.fromarray(x)\n    x = x.filter(ImageFilter.MaxFilter(3))\n    return np.array(x)\ndef morphological_open(x):",
        "detail": "Fooocus.modules.inpaint_worker",
        "documentation": {}
    },
    {
        "label": "is_installed",
        "kind": 2,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "def is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n    return spec is not None\ndef run(command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live) -> str:\n    if desc is not None:\n        print(desc)\n    run_kwargs = {",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "def run(command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live) -> str:\n    if desc is not None:\n        print(desc)\n    run_kwargs = {\n        \"args\": command,\n        \"shell\": True,\n        \"env\": os.environ if custom_env is None else custom_env,\n        \"encoding\": 'utf8',\n        \"errors\": 'ignore',\n    }",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "run_pip",
        "kind": 2,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "def run_pip(command, desc=None, live=default_command_live):\n    try:\n        index_url_line = f' --index-url {index_url}' if index_url != '' else ''\n        return run(f'\"{python}\" -m pip {command} --prefer-binary{index_url_line}', desc=f\"Installing {desc}\",\n                   errdesc=f\"Couldn't install {desc}\", live=live)\n    except Exception as e:\n        print(e)\n        print(f'CMD Failed {desc}: {command}')\n        return None\ndef requirements_met(requirements_file):",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "requirements_met",
        "kind": 2,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "def requirements_met(requirements_file):\n    \"\"\"\n    Does a simple parse of a requirements.txt file to determine if all rerqirements in it\n    are already installed. Returns True if so, False if not installed or parsing fails.\n    \"\"\"\n    import importlib.metadata\n    import packaging.version\n    with open(requirements_file, \"r\", encoding=\"utf8\") as file:\n        for line in file:\n            if line.strip() == \"\":",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "re_requirement",
        "kind": 5,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "re_requirement = re.compile(r\"\\s*([-_a-zA-Z0-9]+)\\s*(?:==\\s*([-+_.a-zA-Z0-9]+))?\\s*\")\npython = sys.executable\ndefault_command_live = (os.environ.get('LAUNCH_LIVE_OUTPUT') == \"1\")\nindex_url = os.environ.get('INDEX_URL', \"\")\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "python",
        "kind": 5,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "python = sys.executable\ndefault_command_live = (os.environ.get('LAUNCH_LIVE_OUTPUT') == \"1\")\nindex_url = os.environ.get('INDEX_URL', \"\")\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "default_command_live",
        "kind": 5,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "default_command_live = (os.environ.get('LAUNCH_LIVE_OUTPUT') == \"1\")\nindex_url = os.environ.get('INDEX_URL', \"\")\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n    return spec is not None",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "index_url",
        "kind": 5,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "index_url = os.environ.get('INDEX_URL', \"\")\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n    return spec is not None\ndef run(command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live) -> str:",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "modules_path",
        "kind": 5,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "modules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n    return spec is not None\ndef run(command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live) -> str:\n    if desc is not None:",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "script_path",
        "kind": 5,
        "importPath": "Fooocus.modules.launch_util",
        "description": "Fooocus.modules.launch_util",
        "peekOfCode": "script_path = os.path.dirname(modules_path)\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n    return spec is not None\ndef run(command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live) -> str:\n    if desc is not None:\n        print(desc)",
        "detail": "Fooocus.modules.launch_util",
        "documentation": {}
    },
    {
        "label": "localization_js",
        "kind": 2,
        "importPath": "Fooocus.modules.localization",
        "description": "Fooocus.modules.localization",
        "peekOfCode": "def localization_js(filename):\n    global current_translation\n    if isinstance(filename, str):\n        full_name = os.path.abspath(os.path.join(localization_root, filename + '.json'))\n        if os.path.exists(full_name):\n            try:\n                with open(full_name, encoding='utf-8') as f:\n                    current_translation = json.load(f)\n                    assert isinstance(current_translation, dict)\n                    for k, v in current_translation.items():",
        "detail": "Fooocus.modules.localization",
        "documentation": {}
    },
    {
        "label": "dump_english_config",
        "kind": 2,
        "importPath": "Fooocus.modules.localization",
        "description": "Fooocus.modules.localization",
        "peekOfCode": "def dump_english_config(components):\n    all_texts = []\n    for c in components:\n        label = getattr(c, 'label', None)\n        value = getattr(c, 'value', None)\n        choices = getattr(c, 'choices', None)\n        info = getattr(c, 'info', None)\n        if isinstance(label, str):\n            all_texts.append(label)\n        if isinstance(value, str):",
        "detail": "Fooocus.modules.localization",
        "documentation": {}
    },
    {
        "label": "current_translation",
        "kind": 5,
        "importPath": "Fooocus.modules.localization",
        "description": "Fooocus.modules.localization",
        "peekOfCode": "current_translation = {}\nlocalization_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'language')\ndef localization_js(filename):\n    global current_translation\n    if isinstance(filename, str):\n        full_name = os.path.abspath(os.path.join(localization_root, filename + '.json'))\n        if os.path.exists(full_name):\n            try:\n                with open(full_name, encoding='utf-8') as f:\n                    current_translation = json.load(f)",
        "detail": "Fooocus.modules.localization",
        "documentation": {}
    },
    {
        "label": "localization_root",
        "kind": 5,
        "importPath": "Fooocus.modules.localization",
        "description": "Fooocus.modules.localization",
        "peekOfCode": "localization_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'language')\ndef localization_js(filename):\n    global current_translation\n    if isinstance(filename, str):\n        full_name = os.path.abspath(os.path.join(localization_root, filename + '.json'))\n        if os.path.exists(full_name):\n            try:\n                with open(full_name, encoding='utf-8') as f:\n                    current_translation = json.load(f)\n                    assert isinstance(current_translation, dict)",
        "detail": "Fooocus.modules.localization",
        "documentation": {}
    },
    {
        "label": "match_lora",
        "kind": 2,
        "importPath": "Fooocus.modules.lora",
        "description": "Fooocus.modules.lora",
        "peekOfCode": "def match_lora(lora, to_load):\n    patch_dict = {}\n    loaded_keys = set()\n    for x in to_load:\n        real_load_key = to_load[x]\n        if real_load_key in lora:\n            patch_dict[real_load_key] = lora[real_load_key]\n            loaded_keys.add(real_load_key)\n            continue\n        alpha_name = \"{}.alpha\".format(x)",
        "detail": "Fooocus.modules.lora",
        "documentation": {}
    },
    {
        "label": "load_file_from_url",
        "kind": 2,
        "importPath": "Fooocus.modules.model_loader",
        "description": "Fooocus.modules.model_loader",
        "peekOfCode": "def load_file_from_url(\n        url: str,\n        *,\n        model_dir: str,\n        progress: bool = True,\n        file_name: Optional[str] = None,\n) -> str:\n    \"\"\"Download a file from `url` into `model_dir`, using the file present if possible.\n    Returns the path to the downloaded file.\n    \"\"\"",
        "detail": "Fooocus.modules.model_loader",
        "documentation": {}
    },
    {
        "label": "BrownianTreeNoiseSamplerPatched",
        "kind": 6,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "class BrownianTreeNoiseSamplerPatched:\n    transform = None\n    tree = None\n    global_sigma_min = 1.0\n    global_sigma_max = 1.0\n    @staticmethod\n    def global_init(x, sigma_min, sigma_max, seed=None, transform=lambda x: x, cpu=False):\n        t0, t1 = transform(torch.as_tensor(sigma_min)), transform(torch.as_tensor(sigma_max))\n        BrownianTreeNoiseSamplerPatched.transform = transform\n        BrownianTreeNoiseSamplerPatched.tree = BatchedBrownianTree(x, t0, t1, seed, cpu=cpu)",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "calculate_weight_patched",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]\n        strength_model = p[2]\n        if strength_model != 1.0:\n            weight *= strength_model\n        if isinstance(v, list):\n            v = (self.calculate_weight(v[1:], v[0].clone(), key),)\n        if len(v) == 1:",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "compute_cfg",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def compute_cfg(uncond, cond, cfg_scale, t):\n    global adaptive_cfg\n    mimic_cfg = float(adaptive_cfg)\n    real_cfg = float(cfg_scale)\n    real_eps = uncond + real_cfg * (cond - uncond)\n    if cfg_scale > adaptive_cfg:\n        mimicked_eps = uncond + mimic_cfg * (cond - uncond)\n        return real_eps * t + mimicked_eps * (1 - t)\n    else:\n        return real_eps",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_sampler_cfg_function",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patched_sampler_cfg_function(args):\n    global eps_record\n    positive_eps = args['cond']\n    negative_eps = args['uncond']\n    cfg_scale = args['cond_scale']\n    positive_x0 = args['input'] - positive_eps\n    sigma = args['sigma']\n    alpha = 0.001 * sharpness * global_diffusion_progress\n    positive_eps_degraded = anisotropic.adaptive_anisotropic_filter(x=positive_eps, g=positive_x0)\n    positive_eps_degraded_weighted = positive_eps_degraded * alpha + positive_eps * (1.0 - alpha)",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "sdxl_encode_adm_patched",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def sdxl_encode_adm_patched(self, **kwargs):\n    global positive_adm_scale, negative_adm_scale\n    clip_pooled = fcbh.model_base.sdxl_pooled(kwargs, self.noise_augmentor)\n    width = kwargs.get(\"width\", 768)\n    height = kwargs.get(\"height\", 768)\n    target_width = width\n    target_height = height\n    if kwargs.get(\"prompt_type\", \"\") == \"negative\":\n        width = float(width) * negative_adm_scale\n        height = float(height) * negative_adm_scale",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "encode_token_weights_patched_with_a1111_method",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def encode_token_weights_patched_with_a1111_method(self, token_weight_pairs):\n    to_encode = list()\n    max_token_len = 0\n    has_weights = False\n    for x in token_weight_pairs:\n        tokens = list(map(lambda a: a[0], x))\n        max_token_len = max(len(tokens), max_token_len)\n        has_weights = has_weights or not all(map(lambda a: a[1] == 1.0, x))\n        to_encode.append(tokens)\n    sections = len(to_encode)",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_KSamplerX0Inpaint_forward",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patched_KSamplerX0Inpaint_forward(self, x, sigma, uncond, cond, cond_scale, denoise_mask, model_options={}, seed=None):\n    if inpaint_worker.current_task is not None:\n        latent_processor = self.inner_model.inner_model.process_latent_in\n        inpaint_latent = latent_processor(inpaint_worker.current_task.latent).to(x)\n        inpaint_mask = inpaint_worker.current_task.latent_mask.to(x)\n        if getattr(self, 'energy_generator', None) is None:\n            # avoid bad results by using different seeds.\n            self.energy_generator = torch.Generator(device='cpu').manual_seed((seed + 1) % constants.MAX_SEED)\n        energy_sigma = sigma.reshape([sigma.shape[0]] + [1] * (len(x.shape) - 1))\n        current_energy = torch.randn(",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "timed_adm",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def timed_adm(y, timesteps):\n    if isinstance(y, torch.Tensor) and int(y.dim()) == 2 and int(y.shape[1]) == 5632:\n        y_mask = (timesteps > 999.0 * (1.0 - float(adm_scaler_end))).to(y)[..., None]\n        y_with_adm = y[..., :2816].clone()\n        y_without_adm = y[..., 2816:].clone()\n        return y_with_adm * y_mask + y_without_adm * (1.0 - y_mask)\n    return y\ndef patched_timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    # Consistent with Kohya to reduce differences between model training and inference.\n    if not repeat_only:",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_timestep_embedding",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patched_timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    # Consistent with Kohya to reduce differences between model training and inference.\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=timesteps.device)\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_cldm_forward",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patched_cldm_forward(self, x, hint, timesteps, context, y=None, **kwargs):\n    t_emb = fcbh.ldm.modules.diffusionmodules.openaimodel.timestep_embedding(\n        timesteps, self.model_channels, repeat_only=False).to(self.dtype)\n    emb = self.time_embed(t_emb)\n    guided_hint = self.input_hint_block(hint, emb, context)\n    y = timed_adm(y, timesteps)\n    outs = []\n    hs = []\n    if self.num_classes is not None:\n        assert y.shape[0] == x.shape[0]",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_unet_forward",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patched_unet_forward(self, x, timesteps=None, context=None, y=None, control=None, transformer_options={}, **kwargs):\n    global global_diffusion_progress\n    self.current_step = 1.0 - timesteps.to(x) / 999.0\n    global_diffusion_progress = float(self.current_step.detach().cpu().numpy().tolist()[0])\n    transformer_options[\"original_shape\"] = list(x.shape)\n    transformer_options[\"current_index\"] = 0\n    transformer_patches = transformer_options.get(\"patches\", {})\n    y = timed_adm(y, timesteps)\n    hs = []\n    t_emb = fcbh.ldm.modules.diffusionmodules.openaimodel.timestep_embedding(",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_register_schedule",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patched_register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n    # Consistent with Kohya to reduce differences between model training and inference.\n    if given_betas is not None:\n        betas = given_betas\n    else:\n        betas = make_beta_schedule(\n            beta_schedule,\n            timesteps,\n            linear_start=linear_start,",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patched_load_models_gpu",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patched_load_models_gpu(*args, **kwargs):\n    execution_start_time = time.perf_counter()\n    y = fcbh.model_management.load_models_gpu_origin(*args, **kwargs)\n    moving_time = time.perf_counter() - execution_start_time\n    if moving_time > 0.1:\n        print(f'[Fooocus Model Management] Moving model(s) has taken {moving_time:.2f} seconds')\n    return y\ndef build_loaded(module, loader_name):\n    original_loader_name = loader_name + '_origin'\n    if not hasattr(module, original_loader_name):",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "build_loaded",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def build_loaded(module, loader_name):\n    original_loader_name = loader_name + '_origin'\n    if not hasattr(module, original_loader_name):\n        setattr(module, original_loader_name, getattr(module, loader_name))\n    original_loader = getattr(module, original_loader_name)\n    def loader(*args, **kwargs):\n        result = None\n        try:\n            result = original_loader(*args, **kwargs)\n        except Exception as e:",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "patch_all",
        "kind": 2,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "def patch_all():\n    if not hasattr(fcbh.model_management, 'load_models_gpu_origin'):\n        fcbh.model_management.load_models_gpu_origin = fcbh.model_management.load_models_gpu\n    fcbh.model_management.load_models_gpu = patched_load_models_gpu\n    fcbh.model_patcher.ModelPatcher.calculate_weight = calculate_weight_patched\n    fcbh.cldm.cldm.ControlNet.forward = patched_cldm_forward\n    fcbh.ldm.modules.diffusionmodules.openaimodel.UNetModel.forward = patched_unet_forward\n    fcbh.model_base.SDXL.encode_adm = sdxl_encode_adm_patched\n    fcbh.sd1_clip.ClipTokenWeightEncoder.encode_token_weights = encode_token_weights_patched_with_a1111_method\n    fcbh.samplers.KSamplerX0Inpaint.forward = patched_KSamplerX0Inpaint_forward",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "sharpness",
        "kind": 5,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "sharpness = 2.0\nadm_scaler_end = 0.3\npositive_adm_scale = 1.5\nnegative_adm_scale = 0.8\nadaptive_cfg = 7.0\nglobal_diffusion_progress = 0\neps_record = None\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "adm_scaler_end",
        "kind": 5,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "adm_scaler_end = 0.3\npositive_adm_scale = 1.5\nnegative_adm_scale = 0.8\nadaptive_cfg = 7.0\nglobal_diffusion_progress = 0\neps_record = None\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "positive_adm_scale",
        "kind": 5,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "positive_adm_scale = 1.5\nnegative_adm_scale = 0.8\nadaptive_cfg = 7.0\nglobal_diffusion_progress = 0\neps_record = None\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]\n        strength_model = p[2]",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "negative_adm_scale",
        "kind": 5,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "negative_adm_scale = 0.8\nadaptive_cfg = 7.0\nglobal_diffusion_progress = 0\neps_record = None\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]\n        strength_model = p[2]\n        if strength_model != 1.0:",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "adaptive_cfg",
        "kind": 5,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "adaptive_cfg = 7.0\nglobal_diffusion_progress = 0\neps_record = None\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]\n        strength_model = p[2]\n        if strength_model != 1.0:\n            weight *= strength_model",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "global_diffusion_progress",
        "kind": 5,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "global_diffusion_progress = 0\neps_record = None\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]\n        strength_model = p[2]\n        if strength_model != 1.0:\n            weight *= strength_model\n        if isinstance(v, list):",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "eps_record",
        "kind": 5,
        "importPath": "Fooocus.modules.patch",
        "description": "Fooocus.modules.patch",
        "peekOfCode": "eps_record = None\ndef calculate_weight_patched(self, patches, weight, key):\n    for p in patches:\n        alpha = p[0]\n        v = p[1]\n        strength_model = p[2]\n        if strength_model != 1.0:\n            weight *= strength_model\n        if isinstance(v, list):\n            v = (self.calculate_weight(v[1:], v[0].clone(), key),)",
        "detail": "Fooocus.modules.patch",
        "documentation": {}
    },
    {
        "label": "get_current_html_path",
        "kind": 2,
        "importPath": "Fooocus.modules.private_logger",
        "description": "Fooocus.modules.private_logger",
        "peekOfCode": "def get_current_html_path():\n    date_string, local_temp_filename, only_name = generate_temp_filename(folder=modules.config.path_outputs,\n                                                                         extension='png')\n    html_name = os.path.join(os.path.dirname(local_temp_filename), 'log.html')\n    return html_name\ndef log(img, dic, single_line_number=3):\n    if args_manager.args.disable_image_log:\n        return\n    date_string, local_temp_filename, only_name = generate_temp_filename(folder=modules.config.path_outputs, extension='png')\n    os.makedirs(os.path.dirname(local_temp_filename), exist_ok=True)",
        "detail": "Fooocus.modules.private_logger",
        "documentation": {}
    },
    {
        "label": "log",
        "kind": 2,
        "importPath": "Fooocus.modules.private_logger",
        "description": "Fooocus.modules.private_logger",
        "peekOfCode": "def log(img, dic, single_line_number=3):\n    if args_manager.args.disable_image_log:\n        return\n    date_string, local_temp_filename, only_name = generate_temp_filename(folder=modules.config.path_outputs, extension='png')\n    os.makedirs(os.path.dirname(local_temp_filename), exist_ok=True)\n    Image.fromarray(img).save(local_temp_filename)\n    html_name = os.path.join(os.path.dirname(local_temp_filename), 'log.html')\n    existing_log = log_cache.get(html_name, None)\n    if existing_log is None:\n        if os.path.exists(html_name):",
        "detail": "Fooocus.modules.private_logger",
        "documentation": {}
    },
    {
        "label": "log_cache",
        "kind": 5,
        "importPath": "Fooocus.modules.private_logger",
        "description": "Fooocus.modules.private_logger",
        "peekOfCode": "log_cache = {}\ndef get_current_html_path():\n    date_string, local_temp_filename, only_name = generate_temp_filename(folder=modules.config.path_outputs,\n                                                                         extension='png')\n    html_name = os.path.join(os.path.dirname(local_temp_filename), 'log.html')\n    return html_name\ndef log(img, dic, single_line_number=3):\n    if args_manager.args.disable_image_log:\n        return\n    date_string, local_temp_filename, only_name = generate_temp_filename(folder=modules.config.path_outputs, extension='png')",
        "detail": "Fooocus.modules.private_logger",
        "documentation": {}
    },
    {
        "label": "clip_separate_inner",
        "kind": 2,
        "importPath": "Fooocus.modules.sample_hijack",
        "description": "Fooocus.modules.sample_hijack",
        "peekOfCode": "def clip_separate_inner(c, p, target_model=None, target_clip=None):\n    if target_model is None or isinstance(target_model, SDXLRefiner):\n        c = c[..., -1280:].clone()\n    elif isinstance(target_model, SDXL):\n        c = c.clone()\n    else:\n        p = None\n        c = c[..., :768].clone()\n        final_layer_norm = target_clip.cond_stage_model.clip_l.transformer.text_model.final_layer_norm\n        final_layer_norm_origin_device = final_layer_norm.weight.device",
        "detail": "Fooocus.modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "clip_separate",
        "kind": 2,
        "importPath": "Fooocus.modules.sample_hijack",
        "description": "Fooocus.modules.sample_hijack",
        "peekOfCode": "def clip_separate(cond, target_model=None, target_clip=None):\n    results = []\n    for c, px in cond:\n        p = px.get('pooled_output', None)\n        c, p = clip_separate_inner(c, p, target_model=target_model, target_clip=target_clip)\n        p = {} if p is None else {'pooled_output': p.clone()}\n        results.append([c, p])\n    return results\n@torch.no_grad()\n@torch.inference_mode()",
        "detail": "Fooocus.modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "clip_separate_after_preparation",
        "kind": 2,
        "importPath": "Fooocus.modules.sample_hijack",
        "description": "Fooocus.modules.sample_hijack",
        "peekOfCode": "def clip_separate_after_preparation(cond, target_model=None, target_clip=None):\n    results = []\n    for x in cond:\n        p = x.get('pooled_output', None)\n        c = x['model_conds']['c_crossattn'].cond\n        c, p = clip_separate_inner(c, p, target_model=target_model, target_clip=target_clip)\n        result = {'model_conds': {'c_crossattn': CONDRegular(c)}}\n        if p is not None:\n            result['pooled_output'] = p.clone()\n        results.append(result)",
        "detail": "Fooocus.modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "sample_hacked",
        "kind": 2,
        "importPath": "Fooocus.modules.sample_hijack",
        "description": "Fooocus.modules.sample_hijack",
        "peekOfCode": "def sample_hacked(model, noise, positive, negative, cfg, device, sampler, sigmas, model_options={}, latent_image=None, denoise_mask=None, callback=None, disable_pbar=False, seed=None):\n    global current_refiner\n    positive = positive[:]\n    negative = negative[:]\n    resolve_areas_and_cond_masks(positive, noise.shape[2], noise.shape[3], device)\n    resolve_areas_and_cond_masks(negative, noise.shape[2], noise.shape[3], device)\n    model_wrap = wrap_model(model)\n    calculate_start_end_timesteps(model, negative)\n    calculate_start_end_timesteps(model, positive)\n    #make sure each cond area has an opposite one with the same area",
        "detail": "Fooocus.modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "current_refiner",
        "kind": 5,
        "importPath": "Fooocus.modules.sample_hijack",
        "description": "Fooocus.modules.sample_hijack",
        "peekOfCode": "current_refiner = None\nrefiner_switch_step = -1\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_separate_inner(c, p, target_model=None, target_clip=None):\n    if target_model is None or isinstance(target_model, SDXLRefiner):\n        c = c[..., -1280:].clone()\n    elif isinstance(target_model, SDXL):\n        c = c.clone()\n    else:",
        "detail": "Fooocus.modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "refiner_switch_step",
        "kind": 5,
        "importPath": "Fooocus.modules.sample_hijack",
        "description": "Fooocus.modules.sample_hijack",
        "peekOfCode": "refiner_switch_step = -1\n@torch.no_grad()\n@torch.inference_mode()\ndef clip_separate_inner(c, p, target_model=None, target_clip=None):\n    if target_model is None or isinstance(target_model, SDXLRefiner):\n        c = c[..., -1280:].clone()\n    elif isinstance(target_model, SDXL):\n        c = c.clone()\n    else:\n        p = None",
        "detail": "Fooocus.modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "fcbh.samplers.sample",
        "kind": 5,
        "importPath": "Fooocus.modules.sample_hijack",
        "description": "Fooocus.modules.sample_hijack",
        "peekOfCode": "fcbh.samplers.sample = sample_hacked",
        "detail": "Fooocus.modules.sample_hijack",
        "documentation": {}
    },
    {
        "label": "normalize_key",
        "kind": 2,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "def normalize_key(k):\n    k = k.replace('-', ' ')\n    words = k.split(' ')\n    words = [w[:1].upper() + w[1:].lower() for w in words]\n    k = ' '.join(words)\n    k = k.replace('3d', '3D')\n    k = k.replace('Sai', 'SAI')\n    k = k.replace('Mre', 'MRE')\n    k = k.replace('(s', '(S')\n    return k",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "apply_style",
        "kind": 2,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "def apply_style(style, positive):\n    p, n = styles[style]\n    return p.replace('{prompt}', positive).splitlines(), n.splitlines()\ndef apply_wildcards(wildcard_text, rng, directory=wildcards_path):\n    for _ in range(wildcards_max_bfs_depth):\n        placeholders = re.findall(r'__([\\w-]+)__', wildcard_text)\n        if len(placeholders) == 0:\n            return wildcard_text\n        print(f'[Wildcards] processing: {wildcard_text}')\n        for placeholder in placeholders:",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "apply_wildcards",
        "kind": 2,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "def apply_wildcards(wildcard_text, rng, directory=wildcards_path):\n    for _ in range(wildcards_max_bfs_depth):\n        placeholders = re.findall(r'__([\\w-]+)__', wildcard_text)\n        if len(placeholders) == 0:\n            return wildcard_text\n        print(f'[Wildcards] processing: {wildcard_text}')\n        for placeholder in placeholders:\n            try:\n                words = open(os.path.join(directory, f'{placeholder}.txt'), encoding='utf-8').read().splitlines()\n                words = [x for x in words if x != '']",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "styles_path",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "styles_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../sdxl_styles/'))\nwildcards_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../wildcards/'))\nwildcards_max_bfs_depth = 64\ndef normalize_key(k):\n    k = k.replace('-', ' ')\n    words = k.split(' ')\n    words = [w[:1].upper() + w[1:].lower() for w in words]\n    k = ' '.join(words)\n    k = k.replace('3d', '3D')\n    k = k.replace('Sai', 'SAI')",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "wildcards_path",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "wildcards_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../wildcards/'))\nwildcards_max_bfs_depth = 64\ndef normalize_key(k):\n    k = k.replace('-', ' ')\n    words = k.split(' ')\n    words = [w[:1].upper() + w[1:].lower() for w in words]\n    k = ' '.join(words)\n    k = k.replace('3d', '3D')\n    k = k.replace('Sai', 'SAI')\n    k = k.replace('Mre', 'MRE')",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "wildcards_max_bfs_depth",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "wildcards_max_bfs_depth = 64\ndef normalize_key(k):\n    k = k.replace('-', ' ')\n    words = k.split(' ')\n    words = [w[:1].upper() + w[1:].lower() for w in words]\n    k = ' '.join(words)\n    k = k.replace('3d', '3D')\n    k = k.replace('Sai', 'SAI')\n    k = k.replace('Mre', 'MRE')\n    k = k.replace('(s', '(S')",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "styles",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "styles = {}\nstyles_files = get_files_from_folder(styles_path, ['.json'])\nfor x in ['sdxl_styles_fooocus.json',\n          'sdxl_styles_sai.json',\n          'sdxl_styles_mre.json',\n          'sdxl_styles_twri.json',\n          'sdxl_styles_diva.json']:\n    if x in styles_files:\n        styles_files.remove(x)\n        styles_files.append(x)",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "styles_files",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "styles_files = get_files_from_folder(styles_path, ['.json'])\nfor x in ['sdxl_styles_fooocus.json',\n          'sdxl_styles_sai.json',\n          'sdxl_styles_mre.json',\n          'sdxl_styles_twri.json',\n          'sdxl_styles_diva.json']:\n    if x in styles_files:\n        styles_files.remove(x)\n        styles_files.append(x)\nfor styles_file in styles_files:",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "style_keys",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "style_keys = list(styles.keys())\nfooocus_expansion = \"Fooocus V2\"\nlegal_style_names = [fooocus_expansion] + style_keys\ndef apply_style(style, positive):\n    p, n = styles[style]\n    return p.replace('{prompt}', positive).splitlines(), n.splitlines()\ndef apply_wildcards(wildcard_text, rng, directory=wildcards_path):\n    for _ in range(wildcards_max_bfs_depth):\n        placeholders = re.findall(r'__([\\w-]+)__', wildcard_text)\n        if len(placeholders) == 0:",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "fooocus_expansion",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "fooocus_expansion = \"Fooocus V2\"\nlegal_style_names = [fooocus_expansion] + style_keys\ndef apply_style(style, positive):\n    p, n = styles[style]\n    return p.replace('{prompt}', positive).splitlines(), n.splitlines()\ndef apply_wildcards(wildcard_text, rng, directory=wildcards_path):\n    for _ in range(wildcards_max_bfs_depth):\n        placeholders = re.findall(r'__([\\w-]+)__', wildcard_text)\n        if len(placeholders) == 0:\n            return wildcard_text",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "legal_style_names",
        "kind": 5,
        "importPath": "Fooocus.modules.sdxl_styles",
        "description": "Fooocus.modules.sdxl_styles",
        "peekOfCode": "legal_style_names = [fooocus_expansion] + style_keys\ndef apply_style(style, positive):\n    p, n = styles[style]\n    return p.replace('{prompt}', positive).splitlines(), n.splitlines()\ndef apply_wildcards(wildcard_text, rng, directory=wildcards_path):\n    for _ in range(wildcards_max_bfs_depth):\n        placeholders = re.findall(r'__([\\w-]+)__', wildcard_text)\n        if len(placeholders) == 0:\n            return wildcard_text\n        print(f'[Wildcards] processing: {wildcard_text}')",
        "detail": "Fooocus.modules.sdxl_styles",
        "documentation": {}
    },
    {
        "label": "try_load_sorted_styles",
        "kind": 2,
        "importPath": "Fooocus.modules.style_sorter",
        "description": "Fooocus.modules.style_sorter",
        "peekOfCode": "def try_load_sorted_styles(style_names, default_selected):\n    global all_styles\n    all_styles = style_names\n    try:\n        if os.path.exists('sorted_styles.json'):\n            with open('sorted_styles.json', 'rt', encoding='utf-8') as fp:\n                sorted_styles = json.load(fp)\n                if len(sorted_styles) == len(all_styles):\n                    if all(x in all_styles for x in sorted_styles):\n                        if all(x in sorted_styles for x in all_styles):",
        "detail": "Fooocus.modules.style_sorter",
        "documentation": {}
    },
    {
        "label": "sort_styles",
        "kind": 2,
        "importPath": "Fooocus.modules.style_sorter",
        "description": "Fooocus.modules.style_sorter",
        "peekOfCode": "def sort_styles(selected):\n    global all_styles\n    unselected = [y for y in all_styles if y not in selected]\n    sorted_styles = selected + unselected\n    try:\n        with open('sorted_styles.json', 'wt', encoding='utf-8') as fp:\n            json.dump(sorted_styles, fp, indent=4)\n    except Exception as e:\n        print('Write style sorting failed.')\n        print(e)",
        "detail": "Fooocus.modules.style_sorter",
        "documentation": {}
    },
    {
        "label": "localization_key",
        "kind": 2,
        "importPath": "Fooocus.modules.style_sorter",
        "description": "Fooocus.modules.style_sorter",
        "peekOfCode": "def localization_key(x):\n    return x + localization.current_translation.get(x, '')\ndef search_styles(selected, query):\n    unselected = [y for y in all_styles if y not in selected]\n    matched = [y for y in unselected if query.lower() in localization_key(y).lower()] if len(query.replace(' ', '')) > 0 else []\n    unmatched = [y for y in unselected if y not in matched]\n    sorted_styles = matched + selected + unmatched\n    return gr.CheckboxGroup.update(choices=sorted_styles)",
        "detail": "Fooocus.modules.style_sorter",
        "documentation": {}
    },
    {
        "label": "search_styles",
        "kind": 2,
        "importPath": "Fooocus.modules.style_sorter",
        "description": "Fooocus.modules.style_sorter",
        "peekOfCode": "def search_styles(selected, query):\n    unselected = [y for y in all_styles if y not in selected]\n    matched = [y for y in unselected if query.lower() in localization_key(y).lower()] if len(query.replace(' ', '')) > 0 else []\n    unmatched = [y for y in unselected if y not in matched]\n    sorted_styles = matched + selected + unmatched\n    return gr.CheckboxGroup.update(choices=sorted_styles)",
        "detail": "Fooocus.modules.style_sorter",
        "documentation": {}
    },
    {
        "label": "all_styles",
        "kind": 5,
        "importPath": "Fooocus.modules.style_sorter",
        "description": "Fooocus.modules.style_sorter",
        "peekOfCode": "all_styles = []\ndef try_load_sorted_styles(style_names, default_selected):\n    global all_styles\n    all_styles = style_names\n    try:\n        if os.path.exists('sorted_styles.json'):\n            with open('sorted_styles.json', 'rt', encoding='utf-8') as fp:\n                sorted_styles = json.load(fp)\n                if len(sorted_styles) == len(all_styles):\n                    if all(x in all_styles for x in sorted_styles):",
        "detail": "Fooocus.modules.style_sorter",
        "documentation": {}
    },
    {
        "label": "webpath",
        "kind": 2,
        "importPath": "Fooocus.modules.ui_gradio_extensions",
        "description": "Fooocus.modules.ui_gradio_extensions",
        "peekOfCode": "def webpath(fn):\n    if fn.startswith(script_path):\n        web_path = os.path.relpath(fn, script_path).replace('\\\\', '/')\n    else:\n        web_path = os.path.abspath(fn)\n    return f'file={web_path}?{os.path.getmtime(fn)}'\ndef javascript_html():\n    script_js_path = webpath('javascript/script.js')\n    context_menus_js_path = webpath('javascript/contextMenus.js')\n    localization_js_path = webpath('javascript/localization.js')",
        "detail": "Fooocus.modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "javascript_html",
        "kind": 2,
        "importPath": "Fooocus.modules.ui_gradio_extensions",
        "description": "Fooocus.modules.ui_gradio_extensions",
        "peekOfCode": "def javascript_html():\n    script_js_path = webpath('javascript/script.js')\n    context_menus_js_path = webpath('javascript/contextMenus.js')\n    localization_js_path = webpath('javascript/localization.js')\n    zoom_js_path = webpath('javascript/zoom.js')\n    edit_attention_js_path = webpath('javascript/edit-attention.js')\n    viewer_js_path = webpath('javascript/viewer.js')\n    image_viewer_js_path = webpath('javascript/imageviewer.js')\n    head = f'<script type=\"text/javascript\">{localization_js(args_manager.args.language)}</script>\\n'\n    head += f'<script type=\"text/javascript\" src=\"{script_js_path}\"></script>\\n'",
        "detail": "Fooocus.modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "css_html",
        "kind": 2,
        "importPath": "Fooocus.modules.ui_gradio_extensions",
        "description": "Fooocus.modules.ui_gradio_extensions",
        "peekOfCode": "def css_html():\n    style_css_path = webpath('css/style.css')\n    head = f'<link rel=\"stylesheet\" property=\"stylesheet\" href=\"{style_css_path}\">'\n    return head\ndef reload_javascript():\n    js = javascript_html()\n    css = css_html()\n    def template_response(*args, **kwargs):\n        res = GradioTemplateResponseOriginal(*args, **kwargs)\n        res.body = res.body.replace(b'</head>', f'{js}</head>'.encode(\"utf8\"))",
        "detail": "Fooocus.modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "reload_javascript",
        "kind": 2,
        "importPath": "Fooocus.modules.ui_gradio_extensions",
        "description": "Fooocus.modules.ui_gradio_extensions",
        "peekOfCode": "def reload_javascript():\n    js = javascript_html()\n    css = css_html()\n    def template_response(*args, **kwargs):\n        res = GradioTemplateResponseOriginal(*args, **kwargs)\n        res.body = res.body.replace(b'</head>', f'{js}</head>'.encode(\"utf8\"))\n        res.body = res.body.replace(b'</body>', f'{css}</body>'.encode(\"utf8\"))\n        res.init_headers()\n        return res\n    gr.routes.templates.TemplateResponse = template_response",
        "detail": "Fooocus.modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "GradioTemplateResponseOriginal",
        "kind": 5,
        "importPath": "Fooocus.modules.ui_gradio_extensions",
        "description": "Fooocus.modules.ui_gradio_extensions",
        "peekOfCode": "GradioTemplateResponseOriginal = gr.routes.templates.TemplateResponse\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\ndef webpath(fn):\n    if fn.startswith(script_path):\n        web_path = os.path.relpath(fn, script_path).replace('\\\\', '/')\n    else:\n        web_path = os.path.abspath(fn)\n    return f'file={web_path}?{os.path.getmtime(fn)}'\ndef javascript_html():",
        "detail": "Fooocus.modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "modules_path",
        "kind": 5,
        "importPath": "Fooocus.modules.ui_gradio_extensions",
        "description": "Fooocus.modules.ui_gradio_extensions",
        "peekOfCode": "modules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\ndef webpath(fn):\n    if fn.startswith(script_path):\n        web_path = os.path.relpath(fn, script_path).replace('\\\\', '/')\n    else:\n        web_path = os.path.abspath(fn)\n    return f'file={web_path}?{os.path.getmtime(fn)}'\ndef javascript_html():\n    script_js_path = webpath('javascript/script.js')",
        "detail": "Fooocus.modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "script_path",
        "kind": 5,
        "importPath": "Fooocus.modules.ui_gradio_extensions",
        "description": "Fooocus.modules.ui_gradio_extensions",
        "peekOfCode": "script_path = os.path.dirname(modules_path)\ndef webpath(fn):\n    if fn.startswith(script_path):\n        web_path = os.path.relpath(fn, script_path).replace('\\\\', '/')\n    else:\n        web_path = os.path.abspath(fn)\n    return f'file={web_path}?{os.path.getmtime(fn)}'\ndef javascript_html():\n    script_js_path = webpath('javascript/script.js')\n    context_menus_js_path = webpath('javascript/contextMenus.js')",
        "detail": "Fooocus.modules.ui_gradio_extensions",
        "documentation": {}
    },
    {
        "label": "perform_upscale",
        "kind": 2,
        "importPath": "Fooocus.modules.upscaler",
        "description": "Fooocus.modules.upscaler",
        "peekOfCode": "def perform_upscale(img):\n    global model\n    print(f'Upscaling image with shape {str(img.shape)} ...')\n    if model is None:\n        sd = torch.load(model_filename)\n        sdo = OrderedDict()\n        for k, v in sd.items():\n            sdo[k.replace('residual_block_', 'RDB')] = v\n        del sd\n        model = ESRGAN(sdo)",
        "detail": "Fooocus.modules.upscaler",
        "documentation": {}
    },
    {
        "label": "model_filename",
        "kind": 5,
        "importPath": "Fooocus.modules.upscaler",
        "description": "Fooocus.modules.upscaler",
        "peekOfCode": "model_filename = os.path.join(path_upscale_models, 'fooocus_upscaler_s409985e5.bin')\nopImageUpscaleWithModel = ImageUpscaleWithModel()\nmodel = None\ndef perform_upscale(img):\n    global model\n    print(f'Upscaling image with shape {str(img.shape)} ...')\n    if model is None:\n        sd = torch.load(model_filename)\n        sdo = OrderedDict()\n        for k, v in sd.items():",
        "detail": "Fooocus.modules.upscaler",
        "documentation": {}
    },
    {
        "label": "opImageUpscaleWithModel",
        "kind": 5,
        "importPath": "Fooocus.modules.upscaler",
        "description": "Fooocus.modules.upscaler",
        "peekOfCode": "opImageUpscaleWithModel = ImageUpscaleWithModel()\nmodel = None\ndef perform_upscale(img):\n    global model\n    print(f'Upscaling image with shape {str(img.shape)} ...')\n    if model is None:\n        sd = torch.load(model_filename)\n        sdo = OrderedDict()\n        for k, v in sd.items():\n            sdo[k.replace('residual_block_', 'RDB')] = v",
        "detail": "Fooocus.modules.upscaler",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Fooocus.modules.upscaler",
        "description": "Fooocus.modules.upscaler",
        "peekOfCode": "model = None\ndef perform_upscale(img):\n    global model\n    print(f'Upscaling image with shape {str(img.shape)} ...')\n    if model is None:\n        sd = torch.load(model_filename)\n        sdo = OrderedDict()\n        for k, v in sd.items():\n            sdo[k.replace('residual_block_', 'RDB')] = v\n        del sd",
        "detail": "Fooocus.modules.upscaler",
        "documentation": {}
    },
    {
        "label": "resample_image",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def resample_image(im, width, height):\n    im = Image.fromarray(im)\n    im = im.resize((int(width), int(height)), resample=LANCZOS)\n    return np.array(im)\ndef resize_image(im, width, height, resize_mode=1):\n    \"\"\"\n    Resizes an image with the specified resize_mode, width, and height.\n    Args:\n        resize_mode: The mode to use when resizing the image.\n            0: Resize the image to the specified width and height.",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "resize_image",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def resize_image(im, width, height, resize_mode=1):\n    \"\"\"\n    Resizes an image with the specified resize_mode, width, and height.\n    Args:\n        resize_mode: The mode to use when resizing the image.\n            0: Resize the image to the specified width and height.\n            1: Resize the image to fill the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, cropping the excess.\n            2: Resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, filling empty with data from image.\n        im: The image to resize.\n        width: The width to resize the image to.",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "get_shape_ceil",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def get_shape_ceil(h, w):\n    return math.ceil(((h * w) ** 0.5) / 64.0) * 64.0\ndef get_image_shape_ceil(im):\n    H, W = im.shape[:2]\n    return get_shape_ceil(H, W)\ndef set_image_shape_ceil(im, shape_ceil):\n    shape_ceil = float(shape_ceil)\n    H_origin, W_origin, _ = im.shape\n    H, W = H_origin, W_origin\n    for _ in range(256):",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "get_image_shape_ceil",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def get_image_shape_ceil(im):\n    H, W = im.shape[:2]\n    return get_shape_ceil(H, W)\ndef set_image_shape_ceil(im, shape_ceil):\n    shape_ceil = float(shape_ceil)\n    H_origin, W_origin, _ = im.shape\n    H, W = H_origin, W_origin\n    for _ in range(256):\n        current_shape_ceil = get_shape_ceil(H, W)\n        if abs(current_shape_ceil - shape_ceil) < 0.1:",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "set_image_shape_ceil",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def set_image_shape_ceil(im, shape_ceil):\n    shape_ceil = float(shape_ceil)\n    H_origin, W_origin, _ = im.shape\n    H, W = H_origin, W_origin\n    for _ in range(256):\n        current_shape_ceil = get_shape_ceil(H, W)\n        if abs(current_shape_ceil - shape_ceil) < 0.1:\n            break\n        k = shape_ceil / current_shape_ceil\n        H = int(round(float(H) * k / 64.0) * 64)",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "HWC3",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def HWC3(x):\n    assert x.dtype == np.uint8\n    if x.ndim == 2:\n        x = x[:, :, None]\n    assert x.ndim == 3\n    H, W, C = x.shape\n    assert C == 1 or C == 3 or C == 4\n    if C == 3:\n        return x\n    if C == 1:",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "remove_empty_str",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def remove_empty_str(items, default=None):\n    items = [x for x in items if x != \"\"]\n    if len(items) == 0 and default is not None:\n        return [default]\n    return items\ndef join_prompts(*args, **kwargs):\n    prompts = [str(x) for x in args if str(x) != \"\"]\n    if len(prompts) == 0:\n        return \"\"\n    if len(prompts) == 1:",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "join_prompts",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def join_prompts(*args, **kwargs):\n    prompts = [str(x) for x in args if str(x) != \"\"]\n    if len(prompts) == 0:\n        return \"\"\n    if len(prompts) == 1:\n        return prompts[0]\n    return ', '.join(prompts)\ndef generate_temp_filename(folder='./outputs/', extension='png'):\n    current_time = datetime.datetime.now()\n    date_string = current_time.strftime(\"%Y-%m-%d\")",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "generate_temp_filename",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def generate_temp_filename(folder='./outputs/', extension='png'):\n    current_time = datetime.datetime.now()\n    date_string = current_time.strftime(\"%Y-%m-%d\")\n    time_string = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    random_number = random.randint(1000, 9999)\n    filename = f\"{time_string}_{random_number}.{extension}\"\n    result = os.path.join(folder, date_string, filename)\n    return date_string, os.path.abspath(os.path.realpath(result)), filename\ndef get_files_from_folder(folder_path, exensions=None, name_filter=None):\n    if not os.path.isdir(folder_path):",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "get_files_from_folder",
        "kind": 2,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "def get_files_from_folder(folder_path, exensions=None, name_filter=None):\n    if not os.path.isdir(folder_path):\n        raise ValueError(\"Folder path is not a valid directory.\")\n    filenames = []\n    for root, dirs, files in os.walk(folder_path):\n        relative_path = os.path.relpath(root, folder_path)\n        if relative_path == \".\":\n            relative_path = \"\"\n        for filename in files:\n            _, file_extension = os.path.splitext(filename)",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "LANCZOS",
        "kind": 5,
        "importPath": "Fooocus.modules.util",
        "description": "Fooocus.modules.util",
        "peekOfCode": "LANCZOS = (Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\ndef resample_image(im, width, height):\n    im = Image.fromarray(im)\n    im = im.resize((int(width), int(height)), resample=LANCZOS)\n    return np.array(im)\ndef resize_image(im, width, height, resize_mode=1):\n    \"\"\"\n    Resizes an image with the specified resize_mode, width, and height.\n    Args:\n        resize_mode: The mode to use when resizing the image.",
        "detail": "Fooocus.modules.util",
        "documentation": {}
    },
    {
        "label": "fcbh_cli.args",
        "kind": 5,
        "importPath": "Fooocus.args_manager",
        "description": "Fooocus.args_manager",
        "peekOfCode": "fcbh_cli.args = fcbh_cli.parser.parse_args()\n# (Disable by default because of issues like https://github.com/lllyasviel/Fooocus/issues/724)\nfcbh_cli.args.disable_smart_memory = not fcbh_cli.args.enable_smart_memory\nargs = fcbh_cli.args",
        "detail": "Fooocus.args_manager",
        "documentation": {}
    },
    {
        "label": "fcbh_cli.args.disable_smart_memory",
        "kind": 5,
        "importPath": "Fooocus.args_manager",
        "description": "Fooocus.args_manager",
        "peekOfCode": "fcbh_cli.args.disable_smart_memory = not fcbh_cli.args.enable_smart_memory\nargs = fcbh_cli.args",
        "detail": "Fooocus.args_manager",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Fooocus.args_manager",
        "description": "Fooocus.args_manager",
        "peekOfCode": "args = fcbh_cli.args",
        "detail": "Fooocus.args_manager",
        "documentation": {}
    },
    {
        "label": "build_launcher",
        "kind": 2,
        "importPath": "Fooocus.build_launcher",
        "description": "Fooocus.build_launcher",
        "peekOfCode": "def build_launcher():\n    if not is_win32_standalone_build:\n        return\n    presets = [None, 'anime', 'realistic']\n    for preset in presets:\n        win32_cmd_preset = win32_cmd.replace('{cmds}', '' if preset is None else f'--preset {preset}')\n        bat_path = os.path.join(win32_root, 'run.bat' if preset is None else f'run_{preset}.bat')\n        if not os.path.exists(bat_path):\n            with open(bat_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(win32_cmd_preset)",
        "detail": "Fooocus.build_launcher",
        "documentation": {}
    },
    {
        "label": "win32_root",
        "kind": 5,
        "importPath": "Fooocus.build_launcher",
        "description": "Fooocus.build_launcher",
        "peekOfCode": "win32_root = os.path.dirname(os.path.dirname(__file__))\npython_embeded_path = os.path.join(win32_root, 'python_embeded')\nis_win32_standalone_build = os.path.exists(python_embeded_path) and os.path.isdir(python_embeded_path)\nwin32_cmd = '''\n.\\python_embeded\\python.exe -s Fooocus\\entry_with_update.py {cmds} %*\npause\n'''\ndef build_launcher():\n    if not is_win32_standalone_build:\n        return",
        "detail": "Fooocus.build_launcher",
        "documentation": {}
    },
    {
        "label": "python_embeded_path",
        "kind": 5,
        "importPath": "Fooocus.build_launcher",
        "description": "Fooocus.build_launcher",
        "peekOfCode": "python_embeded_path = os.path.join(win32_root, 'python_embeded')\nis_win32_standalone_build = os.path.exists(python_embeded_path) and os.path.isdir(python_embeded_path)\nwin32_cmd = '''\n.\\python_embeded\\python.exe -s Fooocus\\entry_with_update.py {cmds} %*\npause\n'''\ndef build_launcher():\n    if not is_win32_standalone_build:\n        return\n    presets = [None, 'anime', 'realistic']",
        "detail": "Fooocus.build_launcher",
        "documentation": {}
    },
    {
        "label": "is_win32_standalone_build",
        "kind": 5,
        "importPath": "Fooocus.build_launcher",
        "description": "Fooocus.build_launcher",
        "peekOfCode": "is_win32_standalone_build = os.path.exists(python_embeded_path) and os.path.isdir(python_embeded_path)\nwin32_cmd = '''\n.\\python_embeded\\python.exe -s Fooocus\\entry_with_update.py {cmds} %*\npause\n'''\ndef build_launcher():\n    if not is_win32_standalone_build:\n        return\n    presets = [None, 'anime', 'realistic']\n    for preset in presets:",
        "detail": "Fooocus.build_launcher",
        "documentation": {}
    },
    {
        "label": "win32_cmd",
        "kind": 5,
        "importPath": "Fooocus.build_launcher",
        "description": "Fooocus.build_launcher",
        "peekOfCode": "win32_cmd = '''\n.\\python_embeded\\python.exe -s Fooocus\\entry_with_update.py {cmds} %*\npause\n'''\ndef build_launcher():\n    if not is_win32_standalone_build:\n        return\n    presets = [None, 'anime', 'realistic']\n    for preset in presets:\n        win32_cmd_preset = win32_cmd.replace('{cmds}', '' if preset is None else f'--preset {preset}')",
        "detail": "Fooocus.build_launcher",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 5,
        "importPath": "Fooocus.entry_with_update",
        "description": "Fooocus.entry_with_update",
        "peekOfCode": "root = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(root)\nos.chdir(root)\ntry:\n    import pygit2\n    pygit2.option(pygit2.GIT_OPT_SET_OWNER_VALIDATION, 0)\n    repo = pygit2.Repository(os.path.abspath(os.path.dirname(__file__)))\n    branch_name = repo.head.shorthand\n    remote_name = 'origin'\n    remote = repo.remotes[remote_name]",
        "detail": "Fooocus.entry_with_update",
        "documentation": {}
    },
    {
        "label": "expansion",
        "kind": 5,
        "importPath": "Fooocus.expansion_experiments",
        "description": "Fooocus.expansion_experiments",
        "peekOfCode": "expansion = FooocusExpansion()\ntext = 'a handsome man'\nfor i in range(64):\n    print(expansion(text, seed=i))",
        "detail": "Fooocus.expansion_experiments",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "Fooocus.expansion_experiments",
        "description": "Fooocus.expansion_experiments",
        "peekOfCode": "text = 'a handsome man'\nfor i in range(64):\n    print(expansion(text, seed=i))",
        "detail": "Fooocus.expansion_experiments",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "Fooocus.face_experiments",
        "description": "Fooocus.face_experiments",
        "peekOfCode": "img = cv2.imread('lena.png')\nresult = cropper.crop_image(img)\ncv2.imwrite('lena_result.png', result)",
        "detail": "Fooocus.face_experiments",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "Fooocus.face_experiments",
        "description": "Fooocus.face_experiments",
        "peekOfCode": "result = cropper.crop_image(img)\ncv2.imwrite('lena_result.png', result)",
        "detail": "Fooocus.face_experiments",
        "documentation": {}
    },
    {
        "label": "version",
        "kind": 5,
        "importPath": "Fooocus.fooocus_version",
        "description": "Fooocus.fooocus_version",
        "peekOfCode": "version = '2.1.824'",
        "detail": "Fooocus.fooocus_version",
        "documentation": {}
    },
    {
        "label": "prepare_environment",
        "kind": 2,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "def prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND',\n                                   f\"pip install torch==2.1.0 torchvision==0.16.0 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n    print(f\"Python {sys.version}\")\n    print(f\"Fooocus version: {fooocus_version.version}\")\n    if REINSTALL_ALL or not is_installed(\"torch\") or not is_installed(\"torchvision\"):\n        run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\", live=True)\n    if TRY_INSTALL_XFORMERS:",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "download_models",
        "kind": 2,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "def download_models():\n    for file_name, url in checkpoint_downloads.items():\n        load_file_from_url(url=url, model_dir=path_checkpoints, file_name=file_name)\n    for file_name, url in embeddings_downloads.items():\n        load_file_from_url(url=url, model_dir=path_embeddings, file_name=file_name)\n    for file_name, url in lora_downloads.items():\n        load_file_from_url(url=url, model_dir=path_loras, file_name=file_name)\n    for file_name, url in vae_approx_filenames:\n        load_file_from_url(url=url, model_dir=path_vae_approx, file_name=file_name)\n    load_file_from_url(",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "ini_fcbh_args",
        "kind": 2,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "def ini_fcbh_args():\n    from args_manager import args\n    return args\nprepare_environment()\nbuild_launcher()\nargs = ini_fcbh_args()\nif args.cuda_device is not None:\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)\n    print(\"Set device to:\", args.cuda_device)\ndownload_models()",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "root = os.path.dirname(os.path.abspath(__file__))\nbackend_path = os.path.join(root, 'backend', 'headless')\nsys.path += [root, backend_path]\nos.chdir(root)\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "backend_path",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "backend_path = os.path.join(root, 'backend', 'headless')\nsys.path += [root, backend_path]\nos.chdir(root)\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met\nfrom modules.model_loader import load_file_from_url",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met\nfrom modules.model_loader import load_file_from_url\nfrom modules.config import path_checkpoints, path_loras, path_vae_approx, path_fooocus_expansion, \\\n    checkpoint_downloads, path_embeddings, embeddings_downloads, lora_downloads\nREINSTALL_ALL = False",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "os.environ[\"GRADIO_SERVER_PORT\"]",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "os.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met\nfrom modules.model_loader import load_file_from_url\nfrom modules.config import path_checkpoints, path_loras, path_vae_approx, path_fooocus_expansion, \\\n    checkpoint_downloads, path_embeddings, embeddings_downloads, lora_downloads\nREINSTALL_ALL = False\nTRY_INSTALL_XFORMERS = False",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "REINSTALL_ALL",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "REINSTALL_ALL = False\nTRY_INSTALL_XFORMERS = False\ndef prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND',\n                                   f\"pip install torch==2.1.0 torchvision==0.16.0 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n    print(f\"Python {sys.version}\")\n    print(f\"Fooocus version: {fooocus_version.version}\")\n    if REINSTALL_ALL or not is_installed(\"torch\") or not is_installed(\"torchvision\"):",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "TRY_INSTALL_XFORMERS",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "TRY_INSTALL_XFORMERS = False\ndef prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND',\n                                   f\"pip install torch==2.1.0 torchvision==0.16.0 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n    print(f\"Python {sys.version}\")\n    print(f\"Fooocus version: {fooocus_version.version}\")\n    if REINSTALL_ALL or not is_installed(\"torch\") or not is_installed(\"torchvision\"):\n        run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\", live=True)",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "vae_approx_filenames",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "vae_approx_filenames = [\n    ('xlvaeapp.pth', 'https://huggingface.co/lllyasviel/misc/resolve/main/xlvaeapp.pth'),\n    ('vaeapp_sd15.pth', 'https://huggingface.co/lllyasviel/misc/resolve/main/vaeapp_sd15.pt'),\n    ('xl-to-v1_interposer-v3.1.safetensors',\n     'https://huggingface.co/lllyasviel/misc/resolve/main/xl-to-v1_interposer-v3.1.safetensors')\n]\ndef download_models():\n    for file_name, url in checkpoint_downloads.items():\n        load_file_from_url(url=url, model_dir=path_checkpoints, file_name=file_name)\n    for file_name, url in embeddings_downloads.items():",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "Fooocus.launch",
        "description": "Fooocus.launch",
        "peekOfCode": "args = ini_fcbh_args()\nif args.cuda_device is not None:\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)\n    print(\"Set device to:\", args.cuda_device)\ndownload_models()\nfrom webui import *",
        "detail": "Fooocus.launch",
        "documentation": {}
    },
    {
        "label": "gradio_root",
        "kind": 5,
        "importPath": "Fooocus.shared",
        "description": "Fooocus.shared",
        "peekOfCode": "gradio_root = None\nlast_stop = None",
        "detail": "Fooocus.shared",
        "documentation": {}
    },
    {
        "label": "last_stop",
        "kind": 5,
        "importPath": "Fooocus.shared",
        "description": "Fooocus.shared",
        "peekOfCode": "last_stop = None",
        "detail": "Fooocus.shared",
        "documentation": {}
    },
    {
        "label": "generate_clicked",
        "kind": 2,
        "importPath": "Fooocus.webui",
        "description": "Fooocus.webui",
        "peekOfCode": "def generate_clicked(*args):\n    import fcbh.model_management as model_management\n    with model_management.interrupt_processing_mutex:\n        model_management.interrupt_processing = False\n    # outputs=[progress_html, progress_window, progress_gallery, gallery]\n    execution_start_time = time.perf_counter()\n    task = worker.AsyncTask(args=list(args))\n    finished = False\n    yield gr.update(visible=True, value=modules.html.make_progress_html(1, 'Waiting for task to start ...')), \\\n        gr.update(visible=True, value=None), \\",
        "detail": "Fooocus.webui",
        "documentation": {}
    },
    {
        "label": "dump_default_english_config",
        "kind": 2,
        "importPath": "Fooocus.webui",
        "description": "Fooocus.webui",
        "peekOfCode": "def dump_default_english_config():\n    from modules.localization import dump_english_config\n    dump_english_config(grh.all_components)\n# dump_default_english_config()\nshared.gradio_root.launch(\n    inbrowser=args_manager.args.auto_launch,\n    server_name=args_manager.args.listen,\n    server_port=args_manager.args.port,\n    share=args_manager.args.share,\n    auth=check_auth if args_manager.args.share and auth_enabled else None,",
        "detail": "Fooocus.webui",
        "documentation": {}
    },
    {
        "label": "title",
        "kind": 5,
        "importPath": "Fooocus.webui",
        "description": "Fooocus.webui",
        "peekOfCode": "title = f'Fooocus {fooocus_version.version}'\nif isinstance(args_manager.args.preset, str):\n    title += ' ' + args_manager.args.preset\nshared.gradio_root = gr.Blocks(\n    title=title,\n    css=modules.html.css).queue()\nwith shared.gradio_root:\n    with gr.Row():\n        with gr.Column(scale=2):\n            with gr.Row():",
        "detail": "Fooocus.webui",
        "documentation": {}
    },
    {
        "label": "shared.gradio_root",
        "kind": 5,
        "importPath": "Fooocus.webui",
        "description": "Fooocus.webui",
        "peekOfCode": "shared.gradio_root = gr.Blocks(\n    title=title,\n    css=modules.html.css).queue()\nwith shared.gradio_root:\n    with gr.Row():\n        with gr.Column(scale=2):\n            with gr.Row():\n                progress_window = grh.Image(label='Preview', show_label=True, visible=False, height=768,\n                                            elem_classes=['main_view'])\n                progress_gallery = gr.Gallery(label='Finished Images', show_label=True, object_fit='contain',",
        "detail": "Fooocus.webui",
        "documentation": {}
    },
    {
        "label": "prepare_environment",
        "kind": 2,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "def prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND',\n                                   f\"pip install torch==2.1.0 torchvision==0.16.0 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n    print(f\"Python {sys.version}\")\n    print(f\"Fooocus version: {fooocus_version.version}\")\n    if REINSTALL_ALL or not is_installed(\"torch\") or not is_installed(\"torchvision\"):\n        run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\", live=True)\n    if TRY_INSTALL_XFORMERS:",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "download_models",
        "kind": 2,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "def download_models():\n    for file_name, url in checkpoint_downloads.items():\n        load_file_from_url(url=url, model_dir=path_checkpoints, file_name=file_name)\n    for file_name, url in embeddings_downloads.items():\n        load_file_from_url(url=url, model_dir=path_embeddings, file_name=file_name)\n    for file_name, url in lora_downloads.items():\n        load_file_from_url(url=url, model_dir=path_loras, file_name=file_name)\n    for file_name, url in vae_approx_filenames:\n        load_file_from_url(url=url, model_dir=path_vae_approx, file_name=file_name)\n    load_file_from_url(",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "ini_fcbh_args",
        "kind": 2,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "def ini_fcbh_args():\n    from args_manager import args\n    return args\nprepare_environment()\nbuild_launcher()\nargs = ini_fcbh_args()\nif args.cuda_device is not None:\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)\n    print(\"Set device to:\", args.cuda_device)\ndownload_models()",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "root = os.path.dirname(os.path.abspath(__file__))\nbackend_path = os.path.join(root, 'backend', 'headless')\nsys.path += [root, backend_path]\nos.chdir(root)\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "backend_path",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "backend_path = os.path.join(root, 'backend', 'headless')\nsys.path += [root, backend_path]\nos.chdir(root)\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met\nfrom modules.model_loader import load_file_from_url",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met\nfrom modules.model_loader import load_file_from_url\nfrom modules.config import path_checkpoints, path_loras, path_vae_approx, path_fooocus_expansion, \\\n    checkpoint_downloads, path_embeddings, embeddings_downloads, lora_downloads\nREINSTALL_ALL = False",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "os.environ[\"GRADIO_SERVER_PORT\"]",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "os.environ[\"GRADIO_SERVER_PORT\"] = \"7865\"\nimport platform\nimport fooocus_version\nfrom build_launcher import build_launcher\nfrom modules.launch_util import is_installed, run, python, run_pip, requirements_met\nfrom modules.model_loader import load_file_from_url\nfrom modules.config import path_checkpoints, path_loras, path_vae_approx, path_fooocus_expansion, \\\n    checkpoint_downloads, path_embeddings, embeddings_downloads, lora_downloads\nREINSTALL_ALL = False\nTRY_INSTALL_XFORMERS = False",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "REINSTALL_ALL",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "REINSTALL_ALL = False\nTRY_INSTALL_XFORMERS = False\ndef prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND',\n                                   f\"pip install torch==2.1.0 torchvision==0.16.0 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n    print(f\"Python {sys.version}\")\n    print(f\"Fooocus version: {fooocus_version.version}\")\n    if REINSTALL_ALL or not is_installed(\"torch\") or not is_installed(\"torchvision\"):",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "TRY_INSTALL_XFORMERS",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "TRY_INSTALL_XFORMERS = False\ndef prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND',\n                                   f\"pip install torch==2.1.0 torchvision==0.16.0 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n    print(f\"Python {sys.version}\")\n    print(f\"Fooocus version: {fooocus_version.version}\")\n    if REINSTALL_ALL or not is_installed(\"torch\") or not is_installed(\"torchvision\"):\n        run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\", live=True)",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "vae_approx_filenames",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "vae_approx_filenames = [\n    ('xlvaeapp.pth', 'https://huggingface.co/lllyasviel/misc/resolve/main/xlvaeapp.pth'),\n    ('vaeapp_sd15.pth', 'https://huggingface.co/lllyasviel/misc/resolve/main/vaeapp_sd15.pt'),\n    ('xl-to-v1_interposer-v3.1.safetensors',\n     'https://huggingface.co/lllyasviel/misc/resolve/main/xl-to-v1_interposer-v3.1.safetensors')\n]\ndef download_models():\n    for file_name, url in checkpoint_downloads.items():\n        load_file_from_url(url=url, model_dir=path_checkpoints, file_name=file_name)\n    for file_name, url in embeddings_downloads.items():",
        "detail": "overridePrepare",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "overridePrepare",
        "description": "overridePrepare",
        "peekOfCode": "args = ini_fcbh_args()\nif args.cuda_device is not None:\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)\n    print(\"Set device to:\", args.cuda_device)\ndownload_models()",
        "detail": "overridePrepare",
        "documentation": {}
    }
]